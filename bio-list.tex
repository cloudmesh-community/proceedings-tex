\part{Preface}

\section{About}

The document is hosted at 

\URL{http://cyberaide.org/papers/vonLaszewski-cloud-vol-7.pdf}

and is updated on a weekly basis once Mondays between 9am and 5pm.

\section{Bibtex Labels}

In contrast to other classes we have provided you in this document
with a convenient way to add \verb|chktex| to the document creation
and notified yo in your abstracts what chktex returns.


This is an extraordinary service and I do not think any other
professor at the iniversity does this. However, we noticed that some
students did not take advantage of this service and others did not
follow our required nameing scheme for bibtex labels.

I like you to reflect upon why we do this. This is a cloud class that
uses cloud services in which users in the cloud contribute to a
document. In order to avoid duplication of the refernces labels we
need to have a convention that we communicated to you in multiple
posts to you. 

As the attempt to have you fix this failed, we have now added a
section that lists the abstracts that do not follow this convention
and prevents us from compiling the document with bibliography
included.

\section{Word count}

A good length of an abstract is between 150 to 300 words.

\section{Other checks}

We also \textit{require} you to run \verb|chktex| locally on your
Latex abstracts. This is in order to identify if you have installed a
virtual machine on your computre, installed \LaTeX\ and understood how
to create and check the PDF output with the Makefile. Certainly you
can use sharelatex, but you need to check the document more carefully
as share latex does not allow you to use all the features that you
have to your exposure locally and make writing your final report much
easier. 

To see the checks you also need to visit your Bio in this document, as
well as all abstracts. As mentioned before, we will only look at this
document.

Please make sure that you are not flagged by our checks. 

\section{Automated exclusion}

If your abstract is in the following list, it will be automatically
excluded from the proceedings as it does not fulfill our trivial
guidelines.

\section{Missing hid prefix in label}
\begin{verbatim}
../hid-sp18-411/technology/abstract-apachekylin.tex
../hid-sp18-411/technology/abstract-googlefirebase.tex
../hid-sp18-501/technology/abstract-apache-ignite.tex
../hid-sp18-501/technology/abstract-azure-blob-storage.tex
../hid-sp18-501/technology/abstract-azure-cosmosdb.tex
../hid-sp18-501/technology/abstract-google-bigquery.tex
../hid-sp18-501/technology/abstract-oracle-cloud-machine.tex
../hid-sp18-508/technology/abstract-amazonec2.tex
../hid-sp18-508/technology/abstract-azureblas.tex
../hid-sp18-508/technology/abstract-futuregrid.tex
../hid-sp18-508/technology/abstract-openvz.tex
../hid-sp18-508/technology/abstract-sqlite.tex
../hid-sp18-519/technology/abstract-alluxio.tex
../hid-sp18-519/technology/abstract-connectthedots.tex
../hid-sp18-519/technology/abstract-denodo.tex
../hid-sp18-519/technology/abstract-spagobi.tex
../hid-sp18-519/technology/abstract-systemml.tex
../hid-sp18-522/technology/abstract-aws-deeplens.tex
../hid-sp18-522/technology/abstract-aws-fargate.tex
../hid-sp18-522/technology/abstract-aws-lightshell.tex
../hid-sp18-522/technology/abstract-the-go-language.tex
../hid-sp18-524/technology/abstract-CenOS.tex
../hid-sp18-524/technology/abstract-GCP-BD.tex
../hid-sp18-524/technology/abstract-GCP-CD.tex
../hid-sp18-524/technology/abstract-HPCC.tex
../hid-sp18-524/technology/abstract-QDS.tex

\end{verbatim}
\section{Missing citation}
\begin{verbatim}
../hid-sp18-411/technology/abstract-apachekylin.tex
../hid-sp18-411/technology/abstract-googlefirebase.tex

\end{verbatim}
\part{Technologioes}
\chapter{New Technologies}
\section{Apache Ambari}  

Ambari is a software  to manage Hadoop environment
efficiently by providing services like managing, monitoring and provisioning to
the hadoop clusters~\cite{hid-sp18-401-wiki-Ambari}.

When Apache Hadoop started developing with aim of increasing its scalability,
several application layers started to cover its architecture like Pig, Hive,
HBase etc.\ making the management of Hadoop architecture bulky and unmanageable,
and several problems were faced by the developers in handling large hadoop
clusters. Ambari is developed aiming to be the solution to the above problems


Apache architecture includes two main components - Ambari Server and Ambari
Agent. Ambari supports 64 bit OS like RHEL 5 (Redhat enterprise Linux), RHEL 6,
CentOS 5, CentOS 6 etc.~\cite{hid-sp18-401-Ambari}. Ambari provided monitoring
services through tools like Dashboard views - which shows cluster health and
cluster status, also by collecting different metrics like Job status, Maps slots
utilization, garbage collection.



\begin{IU}

hid-sp18-401

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-401/blob/master//technology/abstract-apache-ambari.tex}{abstract-apache-ambari.tex}

 

Wordcount: 130


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

ERROR: Illegal quotes in the file skipping inclusion. Please fix the folllowing file:

\begin{IU}

hid-sp18-401

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-401/blob/master//technology/abstract-xgboost.tex}{abstract-xgboost.tex}

 

Wordcount: 142


Quote ratio: 36.64 \%

WARNING: Quote ratio very high
 
Max Line length: 87
\end{IU}

\section{Apatar}
Apatar~\cite{hid-sp18-402-www-apatar} is a data integration tool which
provides the capability to work with data across different systems and
helps to move data between those systems. It also provides ETL
capability for the data extraction and transformation. Application
point of view it can be used in data warehousing, data migration,
synchronization and integration between applications. It can be used
across heterogeneous systems like databases, files, FTP, Queue, and
applications like ERP, CRM. Since it is an open source tool developed
in Java, it provides platform independence and can be used on any
operating system. It provides flexible deployment options as desktop,
server or embedded into a 3rd party software. The desktop deployment
comes with a GUI client installation along with command line support
on the local machine. Server deployment allows Apatar to be deployed
as server engine over the network. The embedded option allows other
software providers to embed Apatar into their software to provide data
integration capabilities. Apatar has GUI for mapping and design which
can be used by technical as well as the non-technical person. Apatar
is based on modular open application architecture which allows
customization and flexibility to modify the source code for customized
business logic or integration with new systems. As per the Apatar
website, it currently supports connectivity and works with Oracle, MS
SQL, MySQL, Sybase, DB2, MS Access, PostgreSQL, XML, InstantDB,
Paradox, BorlandJDataStore, CSV, MS Excel, Qed, HSQL, Compiere ERP,
SalesForce.Com, SugarCRM, Goldmine, any JDBC data sources and
more. Apatar also has data quality tool which helps with the data
cleansing. It provides support to multiple languages as it is Unicode
compliant. The Apatar architecture consists of 3 major component as
presentation/GUI, ETL, and data source. GUI is used to perform various
data integration task like data mapping, data source configuration etc
in a user-friendly way. Data source provides various connectors to
connect with different data sources like databases, files,
application (SAP, Siebel, etc), real-time feeds like queuing
services. Extract, Transformation and Load (ETL) component provides
functionality like data transformation, real-time in-memory data
processing, data cleansing and validation, data exception/rejection
management, data loading, post data load processing like archival,
indexing, aggregation and scheduling and event management.


\begin{IU}

hid-sp18-402

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-402/blob/master//technology/abstract-apatar.tex}{abstract-apatar.tex}

 

Wordcount: 325


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{IBM BlueMix}
BlueMix is a cloud developed by IBM to provide platform as a service
s (PaaS) to build enterprise level application. In 2017, IBM merged
bluemix brand with the IBM cloud brand and now it is known as IBM
Cloud instead of IBM Bluexix~\cite{hid-sp18-402-www-ibm-blog}. All
services offered under IBM Bluemix is now available under IBM Cloud
and provides over 170+ services. These services are published as
infrastructure and platform services. Infrastructure services are
consists of Compute, Storage, Network, Security, Containers and
VMware. Platform services are consists of Boilerplates, APIs,
Application Services, Blockchain, Cloud Foundry Apps, Data and
Analytics, DevOps, Finance, Functions, Integrate, IoT, Mobile and
Watson. These wide arrays of infrastructure and platform services help
create enterprise level of applications. IBM cloud also provides
industry-wide solutions in Banking and Finance, Gaming, As Tech,
Retail, Healthcare, Telecommunications, Media and Entertainment which
can be readily used by the business. IBM Cloud provides pricing
options to use its cloud service as free, pay as you go and
subscription. It provides various deployment options as on-premises,
dedicated private cloud or public cloud~\cite{hid-sp18-402-www-ibm}.


\begin{IU}

hid-sp18-402

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-402/blob/master//technology/abstract-bluemix.tex}{abstract-bluemix.tex}

 

Wordcount: 160


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{BMC Multi-Cloud}
Cloud service introduced a new concept in how to manage IT application
and the infrastructure cost and as it matured more, many businesses
started adopting cloud solution for their business needs. This
resulted sometimes in multiple cloud implementation depending on the
business needs. This cloud implementation single or multiple poses
challenges in terms of managing their cost, performance, security,
automation, visibility, and migration. BMC Multi-Cloud Management
solution is specifically built to handle all those challenges and
help overall cloud management easy. It provides cost control by
providing cloud cost forecast and analysis. Performance monitor
provides real-time performance tracking across multiple clouds and
provides predictive analytics to keep cloud performance in-check. It
provides all assets and dependencies visibility across the clouds which help in
inventory and change management. Multi-Cloud security ensures security
policy compliance across clouds also it embeds compliance security testing
during software development phase. Automation helps automate workload across multiple
clouds. It has migration service which helps with the migration to the
cloud as well as provides migration plan and simulates forecast annual
cost~\cite{hid-sp18-402-bmc}.


\begin{IU}

hid-sp18-402

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-402/blob/master//technology/abstract-bmc.tex}{abstract-bmc.tex}

 

Wordcount: 158


Quote ratio: 0.00 \%
 
Max Line length: 85
\end{IU}

\section{Dokku}
Dokku~\cite{hid-sp18-402-www-dokku} is a Platform as a Service (PaaS)
that runs on a single server which helps build and manage the
lifecycle of applications. It is powered by Docker and can be
installed on any hardware. Dokku requires minimum of 1GB memory and
Ubuntu 16.04 x64, Ubuntu 14.04 x64, Debian 8.2 x64 or CentOS 7 x64 for
the installation. It supports application deployment through
git. Technically Dokku is a set of scripts which combined as build
pipeline. It takes input as code and generates the running application. It
mostly is written in shell script and provides various features as
plugins, for example, config, storage etc.\ Dokku helps in easy code
deployment to the cloud so that developers can concentrate more on
application logic~\cite{hid-sp18-402-www-dokkuG}.


\begin{IU}

hid-sp18-402

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-402/blob/master//technology/abstract-dokku.tex}{abstract-dokku.tex}

 

Wordcount: 111

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 74
\end{IU}

\section{Gephi}
Gephi~\cite{hid-sp18-402-gephi} is an open source software for
visualization and exploration for all kind of graphs and network. It
is a useful tool for data analyst and scientist to understand network
and relationship. This tool is developed in Java and needs Java 1.7 or
higher. It provides the capability to generate various graphs, interact
with those graphs, manipulate the graph to discover the pattern. These
graphs mostly consist of nodes and edges. Edges ate nothing but the
relationship between various nodes. Gephi has various layouts which
provide graph in a different layout for the analysis purpose. Real-time
visualization capability provides analysis by changing graph in real
time through data filtering. Data filtering capability help reduce
nodes and edges in the graph to do drill down analysis or keeping
graph in human readable format.  It has statistics and matrix framework
which provides social network analysis and help community detection
which is called as modularity. Gephi has Data Laboratory which allows us data manipulation as
well as data transformation for analysis. It provides data import
capability through various graph file format as well as CSV format. The export
capability provided by Gephi exports graph in pdf and image format for
analysis and presentation. It supports big data to some extent by
processing capability of around 100k data points. It can be extended
using built-in plugin center. It is supported on Windows, Mac OS X,
Linux platforms~\cite{hid-sp18-402-gephiF}.


\begin{IU}

hid-sp18-402

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-402/blob/master//technology/abstract-gephi.tex}{abstract-gephi.tex}

 

Wordcount: 213


Quote ratio: 0.00 \%
 
Max Line length: 93
\end{IU}

\section{DBI}
\index{DBI}
\index{R-DBI}

DBI is a package for R that provides a common interface to databases
for R programmers to use~\cite{hid-sp18-403-R-dbi}. This allows
R to access data that is too big to fit into local memory, or even
onto local disk. Key components are classes for database connections,
and database results, which can be treated differently, to minimize
local computation. Connections to particular database systems, such as
MySQL, or PostgreSQL are handled through connectivity packages, such
as \texttt{odbc}~\cite{hid-sp18-403-rstudio-odbc}.


\begin{IU}

hid-sp18-403

\href{https://github.com/cloudmesh-community/hid-sp18-403/blob/master//technology/abstract-dbi.tex}{abstract-dbi.tex}

 

Wordcount: 68

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{DBplyr}
\index{DBplyr}
\index{R-DBplyr}

DBPlyr is the bridge between R’s immensly popular tidyverse, and the
DBI data connection family~\cite{hid-sp18-403-tidy-dbplyr}. The package allows
tables on remote or local databases, regardless of backend, to be
treated as first-class data structures in R. It does this by
procedurally generating (ususally SQL) queries for the databases on
the fly~\cite{hid-sp18-403-R-dbplyr}. While the data semantics are
agnostic (all data structures are treated the same, regardless of
provenance), dbplyr is aware of the limitations of different systems,
and will adjust its queries accordingly. Further, dbplyr will evaluate
queries lazily, meaning that almost no data is transferred into local
memory until it is explicitly asked for.


\begin{IU}

hid-sp18-403

\href{https://github.com/cloudmesh-community/hid-sp18-403/blob/master//technology/abstract-dbplyr.tex}{abstract-dbplyr.tex}

 

Wordcount: 94

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 78
\end{IU}

\section{Drake}
\index{Drake}
\index{R!Drake}

Drake is an R package focused on reproducible research and
high-performance computing~\cite{hid-sp18-403-R-drake}. It is an
R-centric version of Make. The core functionality of Drake is based on
the idea that space is cheaper than time.  Therefore, it stores local
caches of target objects when they are built, along witht the commands
that were used to build them. From this, it can build a dependency
network, and automatically determine which objects are outdated before
the next run, and only build the required objects. Because it is
R-focused, it has an advantage over \textit{make} in that it allows
for easy plan expansion, rather than make’s requirement for explicit
commands and targets.  Drake also enables higher performance
computing, by allowing users to build multiple targets at once,
elevating R past its single threaded default.


\begin{IU}

hid-sp18-403

\href{https://github.com/cloudmesh-community/hid-sp18-403/blob/master//technology/abstract-drake.tex}{abstract-drake.tex}

 

Wordcount: 121

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{ODBC}
\index{ODBC}
\index{R-ODBC}

Odbc is an R package which allows connectivity to commercial
databases, such as Oracle, and MS SQL
Server~\cite{hid-sp18-403-R-odbc}. It also permits connection to other
databases with odbc  (Open Database Connectivity) hooks, however other
packages simplify these connections~\cite{hid-sp18-403-R-dbi}.
Because \textit{odbc} is actaully a thin wrapper around the c++ ODBC
bindings, it is faster than any other common database
connecter~\cite{hid-sp18-403-rstudio-odbc}.


\begin{IU}

hid-sp18-403

\href{https://github.com/cloudmesh-community/hid-sp18-403/blob/master//technology/abstract-odbc.tex}{abstract-odbc.tex}

 

Wordcount: 52

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Pool}
\index{Pool}
\index{R!Pool}

Pool is a connection manager for R, which interfaces with the DBI
family of connections~\cite{hid-sp18-403-R-dbi}. The advantage of
using pool as a connection manager is that it automatically maintains
a connection as open, or re-opens closed ones if needed. This helps
ensure that for long-running, interactive contexts, such as
data-visualization dashboards, access is maintained to
data~\cite{hid-sp18-403-R-pool}. Importantly, pool also closes
connections at the end of session, ensuring that there are no dangling
operations.


\begin{IU}

hid-sp18-403

\href{https://github.com/cloudmesh-community/hid-sp18-403/blob/master//technology/abstract-pool.tex}{abstract-pool.tex}

 

Wordcount: 65

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Apache Drill}
\index{Google Dremel}
\index{Microsoft Azure}
\index{Mongodb}
\index{Amazon S3}
\index{Google Cloud Storage}

Apache Drill is an open-source framework for distributing 
computing on applications handling data-intensive analysis. 
It is the open-source parallel to Google Dremel for querying 
very large datasets. Drill is an Apache Top-Level~\cite{hid-sp18-404-BlogsApache2014} 
project which enables queries to process on many servers at once 
over multiple datastores. Drill supports many database systems 
including MongoDB, Amazon S3, Azure Blob Storage, and Google 
Cloud Storage~\cite{hid-sp18-404-Drill2015} and storage file 
formats including Parquet, JSON, CSV, and TSV in MapR-XD~\cite{hid-sp18-404-Drill2017}.

\begin{IU}

hid-sp18-404

\href{https://github.com/cloudmesh-community/hid-sp18-404/blob/master//technology/abstract-apachedrill.tex}{abstract-apachedrill.tex}

 

Wordcount: 80

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 87
\end{IU}

\section{Apache Mesos}
\index{Containers}
\index{Two-level Architecture}
\index{Linux Kernel}

Apache Mesos is the distributed systems kernel 
built similarly to the Linux kernel, but runs 
on a different level~\cite{hid-sp18-404-Apache2018}. 
Apache Mesos performs container and support and 
massively scalable data support by splitting 
scheduling into a two-level architecture. 
Applications running on Mesos are containerized separately 
from the framework handling infrastructure scheduling 
operations~\cite{hid-sp18-404-Mesos2018}.  

\begin{IU}

hid-sp18-404

\href{https://github.com/cloudmesh-community/hid-sp18-404/blob/master//technology/abstract-apachemesos.tex}{abstract-apachemesos.tex}

 

Wordcount: 56

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 59
\end{IU}

\section{Caffe}
\index{Berkely AI Research}
\index{C++}
\index{CUDA}

Caffe is a deep learning framework developed by Berkeley AI Research. 
Caffe is optimized for research experiments and industrial applications. 
Caffe is built in C++ and CUDA with interfaces available in Python and 
MATLAB~\cite{hid-sp18-404-Evan}. The open source collection 
of deep learning models is a valuable bundle of tools for research with 
models including picture pattern recognition, text parsing, and speech 
composition~\cite{hid-sp18-404-jia2014caffe}. 


\begin{IU}

hid-sp18-404

\href{https://github.com/cloudmesh-community/hid-sp18-404/blob/master//technology/abstract-caffe.tex}{abstract-caffe.tex}

 

Wordcount: 64

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 73
\end{IU}

\section{Mesosphere}
\index{Apache Mesos}
\index{Amazon AWS}
\index{Microsoft Azure}

Mesosphere is an Datacenter Operating Platform for 
data-intensive applications. It is based on the Apache Mesos 
kernel~\cite{hid-sp18-404-Concepts2018}. It is a top-level 
cluster, manager, container platform and operating system~\cite{hid-sp18-404-Features2018}. 
Mesosphere performs resource consolidation, resource isolation, 
and storage capabilities in a scalable system as it runs distributed 
containerized software. It is agnostic to the infrastructure level, 
and so can be run on either physical or virtual machines~\cite{hid-sp18-404-Architecture2018}. 
Mesosphere incorporates\footnote{Please can you elaborate?} with Amazon AWS and Microsoft Azure.


\begin{IU}

hid-sp18-404

ERROR: entry contains a footnote that has not yet been addressed

\href{https://github.com/cloudmesh-community/hid-sp18-404/blob/master//technology/abstract-mesosphere.tex}{abstract-mesosphere.tex}

 

Wordcount: 79

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 96
\end{IU}

\section{Pivotal}
\index{Pivotal}
\index{DevOps}
\index{Cloud Foundry}

Pivotal is a developer of cloud-native applications, containers, and
tools for DevOps. The primary cloud computing tool is the Pivotal
Cloud Foundry (PCF) platform~\cite{hid-sp18-404-Pivotal2017}. PCF is a
commercial platform built the built on the open-source Cloud Foundry
platform. The architecture is container-based and offers an option to
web developers in the shift to cloud-native software
development~\cite{hid-sp18-404-Darrow2016}.


\begin{IU}

hid-sp18-404

\href{https://github.com/cloudmesh-community/hid-sp18-404/blob/master//technology/abstract-pivotal.tex}{abstract-pivotal.tex}

 

Wordcount: 51

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{LinkedIn WhereHows}
\index{LinkedIn!WhereHows}
\index{WhereHows}

LinkedIn WhereHows is a open source project carried out by the
LinkedIn Data team. The project works by creating a central repository
and portal for several important elements of big data systems: the
processes, people, and knowledge around the
data~\cite{hid-sp18-405-www-wherehows}. The repository has captured
the status of 50 thousand datasets, 14 thousand comments, 35 million
job executions and related lineage
information~\cite{hid-sp18-405-www-wherehows}. WhereHows integrates
with all LinkedIn data processing environments and extracts metadata
before offering this piece of information through two interfaces: one
is a web application which facilitates functionalities such as navigation, 
search, lineage visualization, annotation, discussion, and community 
participation; the other is an API endpoint that empowers automation of 
other data processes and applications~\cite{hid-sp18-405-www-wherehows}. 
The name WhereHows comes from the two fundamental questions related to 
the data: \textit{where} is the data, and \textit{how} is it produced and 
consumed~\cite{hid-sp18-405-githubwiki-wherehows}.


\begin{IU}

hid-sp18-405

\href{https://github.com/cloudmesh-community/hid-sp18-405/blob/master//technology/abstract-linkedinwherehows.tex}{abstract-linkedinwherehows.tex}

 

Wordcount: 130


Quote ratio: 0.00 \%
 
Max Line length: 75
\end{IU}

\section{Lumify}
\index{Lumify}

Lumify is an open source project developed at US national security
contractor Altamira, with key features indlucing big data fusion, 
analysis, and visualization platform. The web-based interface provides users 
with the ability to discover connections and explore relationships in their
data via various analytic options. These options include 2D and 3D
graph visualizations, full-text faceted search, dynamic histograms,
interactive geographic maps, and collaborative workspaces shared in
real-time~\cite{hid-sp18-405-www-lumify}. Lumify has an Open
Layers-compatible mapping system which can be utilized by tools like
Google Maps to display an interactive geospatial analysis of the data
set. Further, Lumify was integrated with SAP's high speed HANA
in-memory database and computation engine, which enables faster data
retrieval and calculation speed compared to the use of conventional
database system~\cite{hid-sp18-405-linkedinblog-lumify}. By August
2017, Altamira’s Lumify is available through both the Microsoft Azure
Marketplace and Amazon AWS Marketplace. The tool can be immediately
run on the Azure and AWS cloud platforms, where customers have the
option to purchase a license from 
Altamira~\cite{hid-sp18-405-wwwaws-lumify}\cite{hid-sp18-405-wwwazure-lumify}.
 These cloud technologies allow for greater flexibility and usability of Lumify.


\begin{IU}

hid-sp18-405

\href{https://github.com/cloudmesh-community/hid-sp18-405/blob/master//technology/abstract-lumify.tex}{abstract-lumify.tex}

 

Wordcount: 160


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Apache NiFi}
\index{Apache!NiFi}
\index{NiFi}

Apache NiFi, which is short for NiagaraFiles, is a open source
software project from the Apache Software Foundation designed to
automate the flow of data between software
systems~\cite{hid-sp18-405-wiki-nifi}. Based on the \emph{NiagaraFiles}
software previously developed by the NSA, Apache NiFi is part of its
technology transfer program in 2014~\cite{hid-sp18-405-wiki-nifi}.
NiFi executes within a Java Virtual Machine with the following primary
components: Web Server, Flow Controller, Extensions, FlowFile
Repository, Content Repository and Provenance Repository. Since NiFi's
fundamental design concepts are closely related to Flow Based
Programming (FBP), some of the above components can be mapped closely
to FBP terms. For example, Flow Controller and FlowFile can be related
to Scheduler and Information Packet in FBP terms
respectively~\cite{hid-sp18-405-wwwoverview-nifi}~\cite{hid-sp18-405-wikifbp-nifi}.
Apache NiFi supports scalable directed graphs of data routing,
transformation, and system mediation logic, aiming at leveraging the
capabilities of the underlying host system on which it is operating,
especially with regard to CPU and
disk~\cite{hid-sp18-405-wwwoverview-nifi}. Some of the high-level
capabilities and objectives of Apache NiFi include: Web-based user
interface, Highly configurable, Data Provenance, Designed for
extension and Secure~\cite{hid-sp18-405-www-nifi}.


\begin{IU}

hid-sp18-405

\href{https://github.com/cloudmesh-community/hid-sp18-405/blob/master//technology/abstract-nifi.tex}{abstract-nifi.tex}

 

Wordcount: 155


Quote ratio: 0.00 \%
 
Max Line length: 83
\end{IU}

\section{Apache Samoa}
\index{Apache!Samoa}
\index{Samoa}

Apache Samoa, which stands for Scalable Advanced Massive Online
Analysis, is a distributed streaming machine learning framework that
contains a programming abstraction for distributed streaming machine
learning algorithms~\cite{hid-sp18-405-www-samoa}. \color{blue}``\emph{It features a
pluggable architecture that allows it to run on several distributed
stream processing engines such as Storm, S4, and
Samza}''\color{black}~\cite{hid-sp18-405-www-samoa}. Real time analytics can be
utilized by tools like Samoa and allow organizations to react in a
timely manner when problems appear or to detect new trends helping to
improve their performance by obtain useful knowledge from what is
happening now~\cite{hid-sp18-405-bif2015mining-samoa}. Apache Samoa
users can develop distributed streaming ML algorithms once and execute
them on multiple DSPEs (distributed stream processing
engine)~\cite{hid-sp18-405-mor2015samoa-samoa}. In addition, users
could also add new platforms by using the API provided, therefore, the
Samoa project is divided into two different parts, namely: Samoa-API and 
Samoa-Platform. By using Samoa-API, developers could develop for Samoa 
without worrying about which DSPE is going to be 
used~\cite{hid-sp18-405-blog-samoa}. Samoa, written in Java, is open 
source under the Apache Software License version 2.0.



\begin{IU}

hid-sp18-405

\href{https://github.com/cloudmesh-community/hid-sp18-405/blob/master//technology/abstract-samoa.tex}{abstract-samoa.tex}

 

Wordcount: 156


Quote ratio: 10.66 \%
 
Max Line length: 84
\end{IU}

\section{Stardog}
\index{Stardog}

Stardog is a graph database from US-software company
Complexible. Stardog has a particular focus on OWL and RDF-based
systems, with the latest release Stardog 5.2 (9 January 2018) supports
SPARQL query language; property graph model and Gremlin graph
traversal language; OWL 2 and user-defined rules for inference and
data analytics; virtual graphs; geospatial query answering; and
programmatic interaction via several languages and network
interfaces~\cite{hid-sp18-405-wwwdocs-stardog}. Further, the
developers of StarDog OWL/RDF DBMS have pioneered a new use of OWL as
a schema language for RDF databases. This is achieved by adding
integrity constraints (IC), also expressed in OWL syntax, to the
traditional \textit{open-world} OWL
axioms~\cite{hid-sp18-405-cer2012graphical-stardog}. Other key
features of Stardog include Machine Learning and Logical Inference,
Semantic Search, Geospatial Search etc.\ As a commercial software,
Stardog is priced for community, developer and enterprise tiers. The
enterprise version has a free 30-day trial and the community version
is free to download and use for up to four users and ten graph
databases~\cite{hid-sp18-405-www-stardog}.





\begin{IU}

hid-sp18-405

\href{https://github.com/cloudmesh-community/hid-sp18-405/blob/master//technology/abstract-stardog.tex}{abstract-stardog.tex}

 

Wordcount: 142


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{AlibabaCloud}

Alibaba Cloud is a tech giant which provides cloud computing services to support
both international customers and their own internal business partners who are
using Alibaba Group's e-commerce ecosystem.


The service provided by Alibaba Cloud are efficient as they include
high-performance and great computing power in the cloud system. Every service
offered by them are available as pay-as-you-go along with Anti-DDoS protection
and also includes the luxury of Content Delivery Networkss (CDN). On the other
hand, Alibaba Cloud is doing a great impact towards research and development of
large database systems and advanced big data technologies. Alibaba Cloud's
research and development includes Internet of Things technology, virtual
reality, smart homes, networking and also cloud-based mobile-device
operating systems~\cite{hid-sp18-406-AlibabaCloud}.



\begin{IU}

hid-sp18-406

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-406/blob/master//technology/abstract-alibabacloud.tex}{abstract-alibabacloud.tex}

 

Wordcount: 106

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{mLab}

{\bf citation is place wrong. check how to cite}

mLab\ is an efficient service to host MongoDB databases with fully
managed cloud database services. mLab\ has partnered with
platform-as-a-service providers and it also runs on cloud providers
such as Amazon, Google, and Microsoft Azure.

The main goal of mLab\ is to make software developers more productive.
This is achieved by providing a total package of mLab\ which includes
managed cloud database service featuring along with automated
provisioning and scaling of MongoDB\ Databases, backup, recovery,
monitoring, web-based management tools, and expert
support~\cite{hid-sp18-406-mLab}.


\begin{IU}

hid-sp18-406

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-406/blob/master//technology/abstract-mlab.tex}{abstract-mlab.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 12 in ../hid-sp18-406/technology/abstract-mlab.tex line 3: Interword
spacing (`\ ') should perhaps be used. {\bf citation is place wrong. check how
to cite}                                ^ Warning 12 in ../hid-
sp18-406/technology/abstract-mlab.tex line 6: Interword spacing (`\ ') should
perhaps be used. managed cloud database services. mLab\ has partnered with
^
\end{verbatim}
\end{tiny}

Wordcount: 81

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Owncloud}

OwnCloud has made its significant impact in providing client-server software
services for creating file hosting services and also to use them. Even though
most of the functionalities are comparable to Dropbox, OwnCloud distinguishes
itself by presenting as an open-source and free server edition. OwnCloud is
easily available which makes any user easy to install and operate it.

OwnCloud is putting its best efforts to make it work like Google Drive,
providing features such as online document editing, and contact
synchronization~\cite{hid-sp18-406-Owncloud}.


\begin{IU}

hid-sp18-406

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-406/blob/master//technology/abstract-owncloud.tex}{abstract-owncloud.tex}

 

Wordcount: 73

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 77
\end{IU}

\section{Rackspace}

Rackspace is a cloud computing company which administrates cloud system for their
business partners and further helping them to concentrate on managing the
business growth of their partners. Rackspace differentiates itself from other
companies by stating that the cloud system alone which is provided by other
companies are not sufficient to operate the infrastructure efficiently.

Rackspace proudly states that it takes several innovative and cognitive
engineering skills to develop and manage the infrastructure and also
concentrating on tools and applications which are necessary to administrate. BY
providing all such toolwith updated data engines and e-commerce platforms,
Rackspace claims that it will manage cloud and infrastructure of their business
partners in a much different and efficient way~\cite{hid-sp18-406-Rackspace}.



 


\begin{IU}

hid-sp18-406

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-406/blob/master//technology/abstract-rackspace.tex}{abstract-rackspace.tex}

 

Wordcount: 107

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 81
\end{IU}

\section{Twilio}

Twilio is one among the famous companies which provide cloud communications
platform as a services (Paas). Through the APIs provided by Twilio, the software
developers will be able to programmatically manage phone calls and also the text
messages. It deploys its technology on the most successful HTTP protocol and
also provides the flexibility of billing according to the usage.

In order to protect against unexpected outages, Twilio follows strict
architectural design methodologies. For such efforts, Twilio has been applauded.
Twilio also makes efforts in the development of open-source software and is
consistently making contributions to the open-source community~\cite{hid-sp18-406-Twilio}.





\begin{IU}

hid-sp18-406

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-406/blob/master//technology/abstract-twilio.tex}{abstract-twilio.tex}

 

Wordcount: 90

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 90
\end{IU}

\section{IBM Big Replicate}

To make Hadoop deployment enterprise-class, easy data replication is required 
to support critical business applications that depend on Hadoop. Keeping this 
in mind, IBM created IBM Big Replicate which does class replication for Hadoop
and object store.

The main features of the product include
continuous availability, high performance with guaranteed data 
consistency. Also, it replicates large amounts of data from lab to production 
environment, from production to disaster recovery sites. These replications are
governed by the business rules set up. This technology replicates data as the 
data streams in. Thus, it reduces dependency on completion of file operation 
i.e., closing of file before data can be transferred. It offers replication in
a flexible way by handling various Hadoop ditributions and verisons. 
Additionally, for each cluster, multiple IBM Big Replicate can be deployed as 
proxy servers to add resilience. Users can access Hadoop Distributed File 
System using Big Replicate via the standard HDFS URI\cite{hid-sp18-408-IBMBigReplicate}.

\begin{IU}

hid-sp18-408

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-408/blob/master//technology/abstract-IBMBigReplicate.tex}{abstract-IBMBigReplicate.tex}

 

Wordcount: 150


Quote ratio: 0.00 \%
 
Max Line length: 88
\end{IU}

\section{IBM Db2 Big Sql}

{\bf citation labels do not have spaces}

IBM Db2 Big Sql fecilitates operations like accessing data, querying data and
analsying data across data warehouses and also Hadoop.

It is a well formed hybrid engine that lets you get data by querying 
Hadoop using SQL. It gives you the flexibilty of having a single database
connection or make queries to different data sources such as \color{blue}``\emph{HDFS, RDBMS,
NoSql databases, object stores and WebHDFS.}''\color{black}~\cite{IBM DB2 Bi Sql}
One of the most important feature of this Big Sql is that it provides
low latency. This makes data retrival easier in complex business systems.
It also provides high performance, security, SQL compatibility and federation
capabilities to your data warehouses. 

It enables short, rapid queries that facilitates searching by key word or key 
ranges. It uses HBase for operations such as point queries and rapid insert.
Workloads can be updated and deleted via this Hbase. To make use of easier and
faster data processing in Apache Spark, it can be integrated with Spark\cite{hid-sp18-408-IBMDB2BigSql}.
 


\begin{IU}

hid-sp18-408

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-408/blob/master//technology/abstract-IBMDb2BigSql.tex}{abstract-IBMDb2BigSql.tex}

 

Wordcount: 161


Quote ratio: 6.12 \%
 
Max Line length: 104
\end{IU}

\section{Jelastic}
Jelastic is a cloud service provider which combines
platform as a service and container as a service in a single
package. 

The main features include buitl-in metering, monitoring 
and troubleshooting tools. It is available as a public, private, 
hybrid and multi-cloud application. It can manage multi tenant 
Docker containerst to native ecosystem. It facilitates live migration
of workloads across various regions and various clouds with 
zero downtime. This makes the system highly reliable during 
migration. All the resources from different cloud environment
can be accessed using a single panel. It also supports 
microservices and legacy application with absolutely no code
changes. It provides integration with Git, SVN and CI/CD tools
and services. It enables scripting to automate processes and events
in the cloud.

In terms of languages, it supports various languages such as Java, PHP, Ruby,
Node.js, Python, .NET and Go. Additionally, it supports virtualization 
technologies like Docker and Virtuozzo. It also supports a wide
range of databases such as MySQL, MariaDB, Percona, PostgreSQL, 
Redis, Neo4j, MongoDB, Cassandra, CouchDB and OrientDB\cite{hid-sp18-408-JelasticWiki}.



\begin{IU}

hid-sp18-408

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-408/blob/master//technology/abstract-Jelastic.tex}{abstract-Jelastic.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 26 in ../hid-sp18-408/technology/abstract-Jelastic.tex line 20: You
ought to remove spaces in front of punctuation. Node.js, Python, .NET and Go.
Additionally, it supports virtualization                    ^
\end{verbatim}
\end{tiny}

Wordcount: 161


Quote ratio: 0.00 \%
 
Max Line length: 87
\end{IU}

\section{Teradata Intellibase}

Teradata Intellibase provides a compact environment to perform
data warehousing, data exploration in an iterative way and advanced 
analytics using the stored data. Storage of data come at a low cost 
in Intellibase.

The platform enables a combination of Teradata and Hadoop nodes
to make up for the varied workload requirements. It does this by installing
everything into a single cabinet to preserve the floor space in the data
center.

The features include Teradata Database, Hadoop, Terdata Aster Analytics and
Teradata Unified Data Architecture. With these features, it enables application
deployment in a single cabinet. It also provides advanced in-memory computing 
and also provides data protection for all the data sources. The hardware could 
be re-deployed elsewhere thereby reducing infrastructure costs. Additionally,
it also provides software re-imaging for quick replication.

The technology specifications include 18 nodes, 375 TB of uncompresed data from
user and 18TB of memory in a single cabinet. The processors are dual multi-core 
Intel Xeon Processors\cite{hid-sp18-408-TeradataIntellibase}.

\begin{IU}

hid-sp18-408

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-408/blob/master//technology/abstract-TeradataIntelliBase.tex}{abstract-TeradataIntelliBase.tex}

 

Wordcount: 149


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Teradata Kylo}

Kylo is a data lake management software management platform. It is open source
and provides features like data ingestion with data clensing and validation, 
metadata management, governance and security. 

It can connect to many data sources and infer the schema from the common
available data formats. Kylo's data inegestion workflow transfers
data from source to Hive tables with various configuration options
which are built around validation of data fields, protection of data,
data profiling, data security and overall governance.

Kylo includes a metadata repository and provides key capabilities for data 
exploration. Using this feature, users can search in data and metadata
to explore their entities of interest to gain insights. 


By utilizing Kylo's capabilities, designers can develop new
pipeline templates in Apache Nifi. Kylo and Nifi can communicate between 
each other to handle tasks between the cluster and the data center.
The combination of Kylo and Nifi enables data owners to create new
data feeds\cite{hid-sp18-408-TeradataKylo}.



\begin{IU}

hid-sp18-408

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-408/blob/master//technology/abstract-TeradataKylo.tex}{abstract-TeradataKylo.tex}

 

Wordcount: 146


Quote ratio: 0.00 \%
 
Max Line length: 78
\end{IU}

\section{Databricks}

Azure Databricks is founded as an open source project by Microsoft in
collaboration with and the creators of Apache Spark and Databricks
aiming to help clients with cloud-based big data processing using
Apache Spark \cite{hid-sp18-409-www-databricks}. Databricks is closely
coupled with Azure to provide easy integration, streamlined workflows,
and an interactive workspace which satisfied the requirements of data
scientists and data engineers
\cite{hid-sp18-409-www-databrick-doc}. Azure Databricks is packaged
with the complete open-source Apache Spark cluster technologies such
as Spark SQL and DataFrames, Streaming, MLib, GraphX and Spark Core
API \cite{hid-sp18-409-www-databrick-doc}. The main advantage of Azure
Databrick platform is that it is a zero-management cloud platform that
includes fully managed Spark clusters, an interactive workspace for an
exploration and visualization and a platform for powering Spark-based
applications \cite{hid-sp18-409-www-databricks}. As Databricks website
showcases, Viacom, Shell Energy, HP Inc and Hotels.com are few
successful applications which utilizes Databricks
services~\cite{hid-sp18-409-www-databricks}.  Databricks also provides
enterprise level Azure security to protect the data using Azure Active
Directory integration, role-based controls, SLAs,
etc.~\cite{hid-sp18-409-www-databricks}.


\begin{IU}

hid-sp18-409

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-409/blob/master//technology/abstract-databricks.tex}{abstract-databricks.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 2 in ../hid-sp18-409/technology/abstract-databricks.tex line 6: Non-
breaking space (`~') should have been used. Apache Spark \cite{hid-sp18-409-www-
databricks}. Databricks is closely               ^ Warning 2 in ../hid-
sp18-409/technology/abstract-databricks.tex line 13: Non-breaking space (`~')
should have been used. API \cite{hid-sp18-409-www-databrick-doc}. The main
advantage of Azure      ^ Warning 2 in ../hid-sp18-409/technology/abstract-
databricks.tex line 17: Non-breaking space (`~') should have been used.
applications \cite{hid-sp18-409-www-databricks}. As Databricks website
^
\end{verbatim}
\end{tiny}

Wordcount: 145


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Firebase}

Firebase is an open source project found by James Tamplin and Andrew
Lee in 2011 and later acquired by Google in
2014~\cite{hid-sp18-409-www-firebase}. Firebase cloud services started
as an online chat message service and soon expanded to provide cloud
services such as Firebase cloud messaging, Firebase auth, realtime
database, Firebase storage, Firebase hosting, Firebase test lab for
Android and iOS and Firebase crash
reporting~\cite{hid-sp18-409-www-firebase-official}. A new version of
Firebase has released after merging with Google and it provides an
unified cloud platform to build Android, iOS, and web Apps
\cite{hid-sp18-409-www-firebase-official}. After the acquisition,
Google has stopped supporting their cloud messaging services and
merged it with firebase cloud messaging
services~\cite{hid-sp18-409-www-firebase-merged}. Admob, Analytics,
Authentication, Indexing, Test Lab, and Push Notifications are few
important features introduced in the latest release of Firebase
\cite{hid-sp18-409-www-firebase-official}. As James
\cite{hid-sp18-409-www-firebase} stated, push notification support for
Android and iOS mobile application is recently identified as the most
famous feature of firebase cloud services.


\begin{IU}

hid-sp18-409

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-409/blob/master//technology/abstract-firebase.tex}{abstract-firebase.tex}

 

Wordcount: 135


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Firepad}

Firepad is an open source real-time collaborative code and text
editing cloud platform found by Google in 2016 and licensed under
MIT~\cite{hid-sp18-409-www-firepad}.  It is mostly used for rich text
editing and code editing as it is empowered with true collaborative
editing and intelligent operational transform-based merging and
conflict resolution~\cite{hid-sp18-409-www-firepad-wikipedia}. Some
important features included in the Firepad are cursor position
synchronization, undo and redo, text highlighting, user attribution,
presence detection and version check-pointing. As Michael
Lehenbauer~\cite{hid-sp18-409-www-firepad}, the founder of Firepad
claims that it has no server dependencies and yet provide real-time
data synchronization using the Firebase realtime database technology.
It is easy to integrate Firepad to any application since inclusion of
few JavaScript files would enable the Firepad in all modern browsers
such as Chrome, Safari, Opera 11+, IE8+ and Firefox 3.6+
\cite{hid-sp18-409-www-firepad}. As Firepad website showcases,
Socrates.io, Nitrous.IO, LiveMinutes, Koding, CoderPad.io and
ShiftEdit are few successful applications which utilizes
Firepad~\cite{hid-sp18-409-www-databricks}


\begin{IU}

hid-sp18-409

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-409/blob/master//technology/abstract-firepad.tex}{abstract-firepad.tex}

 

Wordcount: 133


Quote ratio: 0.00 \%
 
Max Line length: 69
\end{IU}

\section{Instabug}

Instabug is a cloud service provider which provides in-app feedback,
user surveys, bug reporting, and crash reporting for mobile
applications. The platform was founded in 2012 and as of 2017,
Instabug has been plugged in over 800 million devices including most
of the top 100 apps in Android, iOS, Cordova, Ionic, Xamarin, and web
application markets\cite{hid-sp18-409-www-instabug}. As the
Instabug~\cite{hid-sp18-409-www-instabug} claims most of the top apps
in the world rely on Instabug for beta testing, user engagement and
crash reporting because of it is reliable, and easy to integrate (it
comes with customizable SDK). Instabug is well known for the
customizable Shake to Send feature on the mobile app to invoke the bug
reporting, annotated screenshots, voice note or a screen recording to
better describe the bug to provide a descriptive report on the
developer side without interrupting the user experience
\cite{hid-sp18-409-www-instabug-wikipedia}. Yahoo, soundCloud, paypal,
Lyft, Buzzfeed, Kik and Nextdoor are few famous applications which
uses Instabug for bug and crash
reporting\cite{hid-sp18-409-www-instabug}.



\begin{IU}

hid-sp18-409

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-409/blob/master//technology/abstract-instabug.tex}{abstract-instabug.tex}

 

Wordcount: 144


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{PubNub}

PubNub is globally recognized as a cloud Data Stream Network and a
real-time infrastructure as a service platform founded by Stephen Blum
and Todd Greene in 2010~\cite{ hid-sp18-409-www-pubnub}. PubNub
provides cloud-based services and products to build real-time web,
mobile, and Internet of Things (IoT) applications~\cite{
  hid-sp18-409-www-pubnub-wikipedia}. PubNub's main product is PubNub
push messaging API which is currently being utilized by iOS, Android,
Nodejs, and many other applications.  This push messaging API is built
on PubNub replicated global data streaming network at 14 data centers
distributed among the entire
world~\cite{hid-sp18-409-www-pubnub-wikipedia}.  PubNub is also being
use as IoT device control platform to manage bidirectional
communication, cross-device and platform messaging, monitor device
metadata, act on data instantly, intelligent data routing, device
provisioning and remote firmware upgrades, enterprise grade security,
and minimal battery and bandwidth drain in home automation, wearables,
connected car, sensor deployments, delivery and fulfillment,
manufacturing and industrial, smart cities, and beacons and eTail
\cite{hid-sp18-409-www-pubnub}.


\begin{IU}

hid-sp18-409

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-409/blob/master//technology/abstract-pubnub.tex}{abstract-pubnub.tex}

 

Wordcount: 139


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Amazon Redshift}

Amazon Redshift is a product of amazon mainly designed as
datawarehouse service center that fully manages data warehousing and
makes it a very simple and cost-effective application that could be
used to analyze all the data using standard SQL interface and existing
Business Intelligence tools.  Their website gives more details saying
\color{blue}``\emph{It allows you to run complex analytic queries against petabytes of
structured data, using sophisticated query optimization, columnar
storage on high-performance local disks, and massively parallel query
execution}''\color{black}~\cite{hid-sp18-410-Amazon-Redshift}.

It is an Internet hosting service and data warehouse product that was
announced in 2012 and it became a part of the bigger cloud services
project namely Amazon Web Services. Few stats about the technology and
its adoption - \color{blue}``\emph{It is built on top of technology from the massive
parallel processing (MPP) data-warehouse company ParAccel (later
acquired by Actian),to handle large scale data sets and database
migrations.Redshift differs from Amazon's other hosted database
offering, Amazon RDS, in its ability to handle analytics workloads on
big data data sets stored by a column-oriented DBMS
principle}''\color{black}~\cite{hid-sp18-410-AmazonWiki}.



\begin{IU}

hid-sp18-410

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-410/blob/master//technology/abstract-AmazonRedshift.tex}{abstract-AmazonRedshift.tex}

 

Wordcount: 157


Quote ratio: 46.47 \%

WARNING: Quote ratio very high
 
Max Line length: 86
\end{IU}

\section{Cloudlet}


A cloudlet is technique or mechanism by which the cloud capabilities
and its wonderful storage,data processing and data analysis power is
brought at the edge of the cellular network.  The main idea behind
cloudlet is to bring the cloud and its services closer to the
client (IOT devices,smart phones, smart watches) which cannot
independently complete the high load of computation and would require
offloadingto meet the computational requirements. This offloading to
the main cloud serverwould take relatively longer time in cases where
the action has to be taken as soonas possible in real time. Thus in
scenarios where latency must be minimum and offloading becomes
compulsory we would then be compelled to use cloudlets, where the
computation now happens at the edge of cellular network and latency is
reduced significantly.\color{blue}``\emph{It is a new architectural element that extends
today’s cloud computing infrastructure.  It represents the middle tier
of a 3-tier hierarchy: mobile device - cloudlet -
cloud.}''\color{black}~\cite{hid-sp18-410-wikiCloudlet}

Thus a cloudlet can be viewed as a mini data center whose aim is to
bring the cloud closer to the Non powerful devices. \color{blue}``\emph{The cloudlet
term was first coined by Satyanarayanan and a prototype implementation
is developed by Carnegie Mellon University as a research project.The
concept of cloudlet is also known as follow me cloud,and mobile
micro-cloud}''\color{black}~\cite{hid-sp18-410-wikiCloudlet}


\begin{IU}

hid-sp18-410

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-410/blob/master//technology/abstract-Cloudlet.tex}{abstract-Cloudlet.tex}

 

Wordcount: 195


Quote ratio: 28.24 \%
 
Max Line length: 88
\end{IU}

\section{ELK Stack}

ELK is one of most powerful and scalable BigData solutions in the current
market and is indeed doing pretty good. It can solve many challenging 
problems with respect to indexing, logging, searching, troubleshooting,
storage and reporting.

ELK acronyms three open source projects: Elasticsearch, Logstash, 
and Kibana. \color{blue}``\emph{Elasticsearch is a search and analytics engine. Logstash is a 
server‑side data processing pipeline that ingests data from multiple sources 
simultaneously, transforms it, and then sends it to a stash like 
Elasticsearch. Kibana lets users visualize data with charts and graphs in 
Elasticsearch}''\color{black}~\cite{hid-sp18-410-ELKBlog}. ELK is one of the most scalable solutions in
field of reporting and indexing where Elastic search is an indexing and
database kind of service and Logstash works more like a tool for logging
everything feeding it to Elastic search for indexing and storing in 
the database, while Kibana is a nice GUI that helps in data visualization
and also allows users to build their own reporting requirements in the
Kibana framwork which also provides flexibility and scalability.
Thus Elasticsearch, logstash and Kibana is a wonderful open source that
has collaborated solution for most of problems dealing with BigData 
and cloud.



\begin{IU}

hid-sp18-410

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-410/blob/master//technology/abstract-ELKStack.tex}{abstract-ELKStack.tex}

 

Wordcount: 181


Quote ratio: 22.96 \%
 
Max Line length: 103
\end{IU}

\section{Edge Computing}

Edge computing is a network architecture concept where in the cloud 
computing capabilities are carried out at the edge of cellular network
where the end device or requester is located.

The main idea behind edge computing is to reduce the network latency and
radio network resource consumption by bringing the cloud services closer
to the device so that latency is reduced significantly.

This mechanism requires leveraging or using resources that may not be 
connected to a network with devices such as laptops, smartphones, 
tablets and sensors.

\color{blue}``\emph{Edge computing covers a wide range of technologies including wireless 
sensor networks, mobile data acquisition, mobile signature analysis, 
cooperative distributed peer-to-peer ad hoc networking and processing 
also classifiable as local cloud/fog computing and grid/mesh computing, 
dew computing, mobile edge computing,cloudlet, distributed data storage
and retrieval, autonomic self-healing networks,
remote cloud services}''\color{black}~\cite{hid-sp18-410-edge}

Majority of its application are realized in IOT and other smart connected
ecosystem where emergency is the highest priority and data processing
is scarce. Naive example would be a baby crossing a road and an autonous
vehicle running over the same road, needs to decide as soon as possible
to stop motion in order to save baby's life. It cannot send the data to
main cloud server and wait for response which would be time consuming and
baby's life would be at jeopardy.
Hence edge computing would be really useful and saviour for scenarios where
offloading to cloud is considered costly.


\begin{IU}

hid-sp18-410

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-410/blob/master//technology/abstract-EdgeComputing.tex}{abstract-EdgeComputing.tex}

 

Wordcount: 222


Quote ratio: 26.33 \%
 
Max Line length: 90
\end{IU}

\section{Intel Cloud Finder}



Intel Cloud finder is an enterprise level solution for choosing cloud
service provider. It helps the customers seeking help for cloud
service providers. It also provides a very good resource for people
looking for a very good performance with decent amount of security in
the cloud. They state that \color{blue}``\emph{In the domain of Intel Cloud Technology
we have Intel Advanced vector expansions, Intel Turbo boost technology
and Intel Xeon processor but on the security side we have Intel
trusted execution technology hardware-based protection for the cloud,
ensuring a secure foundation and protecting applications against
malware, malicious software, and other attacks}''\color{black}~\cite{hid-sp18-410-Intel}

looking forward to using this tool for selecting cloud
services that would satisfy personal cloud requirements. Obviosly we need to
wait until Intel makes this tool open to public as this looks to be
proprietary tool.


\begin{IU}

hid-sp18-410

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-410/blob/master//technology/abstract-IntelCloudFinder.tex}{abstract-IntelCloudFinder.tex}

 

Wordcount: 125

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 38.08 \%

WARNING: Quote ratio very high
 
Max Line length: 88
\end{IU}

\section{Amazon Elastic Beanstalk}
Elastic Beanstalk is an Amazon Web Services platform that enable for quick
and easy deployment of scaling webapplications.\color{blue}``\emph{You can simply upload your
code and Elastic Beanstalk automatically handles the deployment, from
capacity provisioning, load balancing, auto-scaling to application health
monitoring.}''\color{black}\cite{hid-sp18-411-amazonelasticbeanstalk}. The platform has
Docker support in addition to facilitating application in wide array of
technologies. The platform manages all the hassle of deployment while the
users have to just worry about uploading their applications.


\begin{IU}

hid-sp18-411

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-411/blob/master//technology/abstract-amazonelasticbeanstalk.tex}{abstract-amazonelasticbeanstalk.tex}

 

Wordcount: 69

ERROR: This abstract is too short.


Quote ratio: 30.33 \%

WARNING: Quote ratio very high
 
Max Line length: 93
\end{IU}

\section{Apache Mahout}
Apache Mahout is commericial platform built for scalable implementation of
machine learning algorithms. It has support for Apache Spark implementation.
The platform built primarily for distributed analytics provides
functionalities for clustering, classification, collaborative fileteric etc.
It provides a linear algebra framework that lets users implement their own
algorithm for data analytics. The platform though available commercially is
still in its development stage.\cite{hid-sp18-411-apachemahout}.

\begin{IU}

hid-sp18-411

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-411/blob/master//technology/abstract-apachemahout.tex}{abstract-apachemahout.tex}

 

Wordcount: 57

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 76
\end{IU}

\section{Skytap}

Skytap is a cloud platform that provides Environment-as-a-Service (Eaas). The
company enables businesses to implement their IT without having to
trouble themselves about the infrastructure needs of their products/services.
One of the high lights of Skytap is that in addition to providing the
cutting edge technologies in their environment, they cater to businesses that
require traditional application or technologies. Beyond which they enable the
customer businesses to modernize. \color{blue}``\emph{True self-service, on-demand resources
enable you to create your own software-defined datacenter and networks with
environments on demand that work in the cloud just like in your 
datacenter.}''\color{black}~\cite{hid-sp18-411-skytap}


\begin{IU}

hid-sp18-411

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-411/blob/master//technology/abstract-skytap.tex}{abstract-skytap.tex}

 

Wordcount: 88

ERROR: This abstract is too short.


Quote ratio: 26.85 \%
 
Max Line length: 92
\end{IU}

\section{Apache Atlas}

Apache atlas is the novel adaptable platform which incorporates the
center set of the functional administration services. The Apache atlas
empowers the ventures to effectively meet the prerequisites inside the
Hadoop.  Additionally, it delivers the integration of the entire data
environment.  The database researchers, data analysts, and the data
administration group can take advantage of the open metadata
management and the administration capabilities can be utilized for the
organizations to create and make the catalog of their information
resources. These resources can be classified and collaborated inside
the venture effortlessly~\cite{hid-sp18-412-Apache_Atlas_by_Maven}.

There are three main core components of the Apache Atlas, Type System,
Graph Engine and Ingest/Export. The type system enables the modeling
of the metadata for the objects that are intended to be administered.
The metadata objects are represented by the \textit{entities} which
are the instances of the \textit{Types}. Inside the Apache Atlas, the
metadata objects are managed with the help of the graph model. The
rich relationships between the metadata objects are taken care by this
approach by providing the good adaptability and effective handling of
the relationships. Additionally, the graph engine also provides the
effective indexing by creating the relevant indices for the metadata
objects with the goal of providing the efficient search results. The
next component called ingest helps the users to post the metadata to
the Atlas. In contrast, the export component will help the users to
expose the metadata of the Atlas and creates an event specific to each
change. The end users will able to respond to these alterations in the
real time by consuming these change
events~\cite{hid-sp18-412-Apache_Atlas_architecture}.



\begin{IU}

hid-sp18-412

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-ApacheAtlas.tex}{abstract-ApacheAtlas.tex}

 

Wordcount: 241


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

ERROR: Illegal quotes in the file skipping inclusion. Please fix the folllowing file:

\begin{IU}

hid-sp18-412

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-appfog.tex}{abstract-appfog.tex}

 

Wordcount: 178


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Appscale}
Appscale developed with the objective of releasing, sending and scaling 
the Google App Engine applications over the public and private systems 
of the cloud, provides the scalable open-source cloud computing. 
In addition, appscale also provides the clusters on the same environment 
and comprehensive bolster for programming languages such as Go, Java, PHP, 
and Python applications. This is enabled with the effective modeling
of the AppScale with the App Engine APIs~\cite{hid-sp18-412-wiki_appscale}.

To enable running applications on any cloud infrastructure Appscale 
provides the API-based development environment and quick responsive 
functionalities to the designers. The application rationale and the 
service system are decoupled from each other in order to effectively 
control the application release, data storage, resource utilization, 
backup and migration~\cite{hid-sp18-412-wiki_appscale}.

Appscale provides a simplified serverless platform for the wide 
variety of the web and mobile applications. The enterprises that use 
this platform will achieve the goals to quickly manage the time, 
cut out the functional costs, improve application stability, and 
the compatibility to combine the existing platform with the other 
novel technologies~\cite{hid-sp18-412-git_appscale}.





\begin{IU}

hid-sp18-412

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-appscale.tex}{abstract-appscale.tex}

 

Wordcount: 167


Quote ratio: 0.00 \%
 
Max Line length: 75
\end{IU}

\section{OrientDB}
The orientDB  is a distributed Multi-Model NoSQL database having an additional
Graph Database Engine~\cite{hid-sp18-412-orientDB_by_CallidusCloud}. In order
to support the efficient control of the data and
discover the true potential of the data orientDB provides the support for the
multi-model in contrast to the primitive single-model database. The novel
multi-model approach of the orientDB has enabled the users to work
simultaneously with the Graph, Document, key-value, geo-Spatial and
reactive models. The multi-model database drives to reduce the
complications with respect to the operations and maintain the consistency
in the data~\cite{hid-sp18-412-orientDB_multimodel}.

The orientDB is the NoSQL graph database which utilizes the graph data model. 
The graph is modeled with the vertices and the edges. The vertices can be a 
person, place, object or a chunk of the data. The relation between the two 
nodes can be represented by an edge~\cite{hid-sp18-412-orientDB_graph}. 
The orientDB has the custom JDBC driver that helps the user to connect 
to the remote servers. This enables the users to interact with the 
database by utilizing the standard set of interactions in the java. 
The orientDB is compatible with the Tinkerpop API. This API provides 
the effective functionalities to abstract the graph, vertex, edge 
and the other properties~\cite{hid-sp18-412-orientDB_graph}.

The database provides the support for the searching
the data of any type with the several indexing mechanisms based on the
B-tree and the extensible hashing. The orientDB also supports the three 
types of the schema models, namely schema-less, schema-full and the 
schema-mixed modes. Each of the record in the database has the 
surrogate key that points to the record in the ArrayList. In the DB-Engine 
graph database standings orientDB stands 
in the 3rd position~\cite{hid-sp18-412-orientDB_wiki}.








\begin{IU}

hid-sp18-412

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-orientDB.tex}{abstract-orientDB.tex}

 

Wordcount: 261


Quote ratio: 0.00 \%
 
Max Line length: 78
\end{IU}

\section{Talend}
Talend provides the vigorous, open source integration support with the various
software tools at no cost. The users can easily start creating and developing
applications effortlessly using the drag-and-drop interface, locally generated
code, globally enriched library of connectors, and a flourishing client
community~\cite{hid-sp18-412-talend_products}.

Talend provides the open studio tool for the effective integration of the data.
 The java code can be easily implanted in the existing libraries and in
addition the new components can be created or the existing components
contributed by the other users can be utilized to effectively code and
inherit the project~\cite{hid-sp18-412-talend_open_studio}.

Talend provides the ability to create the local code without any much effort
and cutting down the work of writing the code.
The users can increase the efficiency, agility and
reduce the time to release the application using the hundreds of the
inbuilt components and connectors. Additionally, talend provides the
ability to move to any of the other integration environment using the
Talend Open Studio with the free download of batch ETL, big data, real-time,
ESB, data profiling, and
the data assessment~\cite{hid-sp18-412-talend_products}.

The integration of the data into the cloud and consolidation of the
conventional enormous amount of the data can be effectively performed
with the help of the Talend Open Studio. The users will be able to share
the connectors and components that are created by them freely,
in order to reach the usability of these components to the
other users with the same objective~\cite{hid-sp18-412-talend_products}.




\begin{IU}

hid-sp18-412

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-talend.tex}{abstract-talend.tex}

 

Wordcount: 218


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Zmanda}
Zmanda is an open source platform which offers open source cloud
backup.  The recovery software and the services provided by Zmanda are
used by many of the small and the mid-size ventures. In order to
effectively protect the Linux, Solaris, Windows, Mac OS X environments
and enable the backup and recovery in these operating systems, Zmanda
offers the Amanda Enterprise. The Zmanda Recovery Manager (ZRM) is
targeted to achieve the functionalities for scheduling the full and
incremental backups~\cite{hid-sp18-412-zmanda_crunchbase}.

There has been a huge growth in the data size in the recent years and
numerous organizations lack the budget and don't have the ability to
perform the complex tasks and manage the costly backups. In order to
effectively address this, Zmanda provides the Amanda Enterprise which
offers the backup and recovery services that integrates and provides
the quick setup, disentangled administration to tasks, and less
cost. Amand Enterprise liberates us from being bolted into a vendor by
providing the standard formats and
tools~\cite{hid-sp18-412-zmanda_amanda}.


Amanda Enterprise is one of the toll compelling and predominant
commercial open source backup and recovery software. It provides the
less time consuming solution with the goal to implement the backup
tasks in a simplified manner for the various systems, databases and
other applications. Apart, from these it also establishes the secure
environment that puts a barrier for intruders to avoid breaching the
critical data and the engineers can quickly restore the backups in a
chaotic situation~\cite{hid-sp18-412-zmanda_webinar}.



\begin{IU}

hid-sp18-412

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-zmanda.tex}{abstract-zmanda.tex}

 

Wordcount: 216


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Clojure}

Clojure~\cite{hid-sp18-413-clojure} is a fully functional scripting
language. Although it is a complied programming language, all its features are
available at runtime. Clojure is based on Lisp~\cite{hid-sp18-413-lisp} and
uses the same eco system. It provides access to Java framework including hints
and type inference. Clojure’s main advantage is its implementation of
multithreaded programming.Clojure very efficiently breaks a task into subtasks
and places them on different JVM threads for parallel processing. Parallel
programming has three challenges called three goblins; reference cells, mutual
exclusion and dwarves berserkers and Clojure handles them by implementing three
tools called futures, delays and promises.


\begin{IU}

hid-sp18-413

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/abstract-clojure.tex}{abstract-clojure.tex}

 

Wordcount: 88

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Logicalglue}

Logicalglue~\cite{hid-sp18-413-logicalglue} is a predictive analystics software
that is mostly targeted towards insurance sector. It employs fuzzy logic to
generate rules which in turn derive accurate predictions.Logical glue helps in
identifying which data is predictive and can be deployed in cloud. It has its
API which an be integrated into buisnnes's already existing
softwares. Logcalglue employs machine learning and genetic algorithms to
generate outcomes. New and dynamic data can be fed to the model generated and
analysis can be run in realtime. This model accurately works on complete
lifecycle of a project right from customers acquisition to closure.


\begin{IU}

hid-sp18-413

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/abstract-logicalglue.tex}{abstract-logicalglue.tex}

 

Wordcount: 89

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Paxata}

One of the most important and time consuming job of data scientist is to clean
and prepare data from multiple sources in a format that it can be
analyzed. Paxata~\cite{hid-sp18-413-paxata} semi automates the process by using
its own algorithms. It uses machine learning and text mining combined with its
libraries to efficiently clean data. Paxata provides a spreadsheet like
interface where inconsistencies are color coded and instructions are provided to
clean up data. Paxata visualizes the data in form of graphs and creates
associations between various data objects and uses them to resolve data quality
issues. This data can then be consumed by visualizing softwares like
tableau. With this approach Paxata gives anyone ability to run data analytics on
big data sets in a short amount of time.


\begin{IU}

hid-sp18-413

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/abstract-paxata.tex}{abstract-paxata.tex}

 

Wordcount: 118

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Puppet}

Puppet~\cite{hid-sp18-413-puppet} is a open source software configuration and
automation tool. It is written in C++ and Clojure. Puppet is a declarative
language and uses domain specific language for configuration. Puppet uses facter
to gather information about the system and user defines the desired
state. Puppet does not use sequential programming where order of execution is
key but uses graphical representation to represent the order of
execution. Resources are defined in manifests written in Domain specific
language. These manifests are complied into catalogue on puppet master and
supplied to puppet clients. These catalogues are only applied if actual and
desired states are different. \color{blue}``\emph{Kubernetes~\cite{hid-sp18-413-Kubernetes} is
new cluster manager from google}''\color{black} and puppet makes it easy to manage the
kubernetes resources. Puppet is declarative, modular, has code testing features
and therefore managing kubernetes with it is easier.


\begin{IU}

hid-sp18-413

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/abstract-puppet.tex}{abstract-puppet.tex}

 

Wordcount: 123

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 8.89 \%
 
Max Line length: 94
\end{IU}

\section{Zepplin}

Apache~\cite{hid-sp18-413-Zeppelin} Zepplin is open source web based notebook
that has built in data discovery, exploration, visualization and collaborative
features.Zeppelin interface is interactive and seamlessly provides a single
interface to execute code and visualize in the same dashboard. Zepplin’s
architecture has three layers; frontend, Zepplin server and interpreter
processor. Zeppelin’s interpreter supports any language or data processing
backend to act as input. It supports~\cite{hid-sp18-413-ApacheSpark} Apache
Spark out of the box without any configuration.


\begin{IU}

hid-sp18-413

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/abstract-zeppelin.tex}{abstract-zeppelin.tex}

 

Wordcount: 65

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 78
\end{IU}

\section{Hyperledger Burrow}
\index{Hyperledger}

Hyperledger Burrow~\cite{hid-sp18-414-Behlendorf} is an open sourced smart-contract interpreter which
was built to meet the requirements of the Ethereum Virtual
Machine. The Ethereum network has begun to see growth in the
enterprise sector; with well-known companies such as JP Morgan,
Microsoft, Accenture and BP all recently joining the Enterprise
Ethereum Alliance.The importance around interpreting
smart contracts created by Ethereum cannot be understated, as Ethereum
has gained a lot of traction and credibility within the Cryptocurrency
community and currently at the time of writing has a market cap of 82
billion USD only second to Bitcoin. Because of this widespread
adoption of Ethereum, one of Burrow’s claims to fame is that it is the
only Apache-licensed Ethereum VM implementations on the
market.~\cite{hid-sp18-414-Hyperledger_Burrow}
\footnote{citation wrongly placed}



\begin{IU}

hid-sp18-414

ERROR: entry contains a footnote that has not yet been addressed

\href{https://github.com/cloudmesh-community/hid-sp18-414/blob/master//technology/abstract-HyperledgerBurrow.tex}{abstract-HyperledgerBurrow.tex}

 

Wordcount: 111

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 101
\end{IU}

\section{Hyperledger Fabric}
\index{Hyperledger}


Hyperledger Fabric  is one of the oldest and most well known of all the
Linux foundation Hyperledger projects. Initially created by IBM and
Digital Asset , it's intent was to be a foundation for developing
distributed ledger applications. Some of the key features sited by the
team are,Channels for sharing confidential information, Ordering
Service delivers transactions consistently to peers in the network,
Endorsement policies for transactions ,CouchDB world state supports
wide range of queries, Bring-your-own Membership Service
Provider(MSP).~\cite{hid-sp18-414-Hyperledger_Fabric}

With many companies contributing to the growth of the platform, over
159 engineers from 28 different organizations, there is a promising future for the platform as a
variety of businesses begin to explore building products with
Fabric. As stated by Behlendorf the number of projects already being
built is in high hundreds to low thousands.~\cite{hid-sp18-414-Behlendorf_Interview}. 
As distributed ledger technology continues to grow, the
willingness for enterprises across differing industry/sectors to
contribute to this open source platform is key to it's success.



\begin{IU}

hid-sp18-414

\href{https://github.com/cloudmesh-community/hid-sp18-414/blob/master//technology/abstract-HyperledgerFabric.tex}{abstract-HyperledgerFabric.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 26 in ../hid-sp18-414/technology/abstract-HyperledgerFabric.tex line 7:
You ought to remove spaces in front of punctuation. Digital Asset , it's intent
was to be a foundation for developing                ^ Warning 26 in ../hid-
sp18-414/technology/abstract-HyperledgerFabric.tex line 11: You ought to remove
spaces in front of punctuation. Endorsement policies for transactions ,CouchDB
world state supports                                        ^ Warning 36 in
../hid-sp18-414/technology/abstract-HyperledgerFabric.tex line 13: You should
put a space in front of parenthesis. Provider(MSP).~\cite{hid-
sp18-414-Hyperledger_Fabric}           ^
\end{verbatim}
\end{tiny}

Wordcount: 146


Quote ratio: 0.00 \%
 
Max Line length: 96
\end{IU}

\section{Hyperledger Indy}
\index{Hyperledger}


Another one of the newer developments from Hyperledger, Hyperledger Indy 
is all about providing independent digital identities across blockchains 
and distributed ledgers. It is a decentralized identity system and its 
advantage is that identity management is its sole focus. 
As Phillip J. Windley, Ph.D., Chair, Sovrin Foundation states,  
\color{blue}``\emph{Many have proposed distributed ledger technology as a solution, 
however building decentralized identity on top of distributed ledgers 
that were designed to support something else (cryptocurrency or smart contracts, 
for example) leads to compromises and short-cuts.}''\color{black}~\cite{hid-sp18-414-Windley}

This will allow people to securely, quickly and easily share their authenticated 
identity with the groups and organizations of their choosing while providing those 
organizations with the peace of mind of knowing who they are dealing with.As Behlendorf 
states, \color{blue}``\emph{Instead of being an entry in a giant data base, you have your data and deal 
programmatically with different organizations who want to check your identity. And 
companies don’t have to store so much personal data. They can store a pointer to the 
identity}''\color{black}~\cite{hid-sp18-414-Behlendor}.


\begin{IU}

hid-sp18-414

\href{https://github.com/cloudmesh-community/hid-sp18-414/blob/master//technology/abstract-HyperledgerIndy.tex}{abstract-HyperledgerIndy.tex}

 

Wordcount: 168


Quote ratio: 41.34 \%

WARNING: Quote ratio very high
 
Max Line length: 104
\end{IU}

\section{Hyperledger Iroha}
\index{Hyperledger}


Hyperledger Iroha is an open source, mobile focused blockchain
platform.  The Japanese startup, Soramitsu in partnership with Hitachi
started the initiative to create a mobile friendly blockchain
architecture. As one of the new, up and coming Hyperledger projects it
focuses on being simple and easy to include in projects and was
implemented in C++ which allows it to, \color{blue}``\emph{perform well with any small
data projects and focused use cases.}''\color{black}~\cite{hid-sp18-414-Behlendorf}.

As stated by the Linux Foundation, \color{blue}``\emph{Hyperledger Iroha is designed to
be simple and easy to incorporate into infrastructural projects that
require distributed ledger technology. It features a simple
construction, modern, domain-driven C++ design, emphasis on mobile
application development and a new, chain-based Byzantine Fault
Tolerant consensus algorithm, called 
Sumeragi}''\color{black}~\cite{hid-sp18-414-Active_Status_Iroha}.



\begin{IU}

hid-sp18-414

\href{https://github.com/cloudmesh-community/hid-sp18-414/blob/master//technology/abstract-HyperledgerIroha.tex}{abstract-HyperledgerIroha.tex}

 

Wordcount: 109

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 42.45 \%

WARNING: Quote ratio very high
 
Max Line length: 87
\end{IU}

\section{Hyperledger Sawtooth}
\index{Hyperledger}

Hyperledger Sawtooth is an open source, blockchain platform which can
be used to build distributed ledger applications. It’s main
application is to simplify the development of blockchain applications
by isolating the core system from the application domain. \color{blue}``\emph{This
allows for developers to quickly and easily develop and deploy
applications with custom tailored business rules in some of the more
common languages}''\color{black}~\cite{hid-sp18-414-Hyperledger_Sawtooth}.

Some of the core features that make Hyperledger Sawtooth a unique and
interesting distributed ledger technology:
  
\color{blue}``\emph{On-chain governance – Utilize smart contracts to vote on blockchain
configuration settings such as the allowed participants and smart
contracts.  Advanced transaction execution engine – Process
transactions in parallel to accelerate block creation and validation.
Support for Ethereum – Run solidity smart contracts and integrate with
Ethereum tooling.  Dynamic consensus – Upgrade or swap the blockchain
consensus protocol on the fly as your network grows, enabling the
integration of more scalable algorithms as they are available.  Broad
language support – Program smart contracts in your preferred language,
with support including Go, JavaScript, Python and 
more.}''\color{black}~\cite{hid-sp18-414-Linux_Foundation_Sawtooth}


\begin{IU}

hid-sp18-414

\href{https://github.com/cloudmesh-community/hid-sp18-414/blob/master//technology/abstract-HyperledgerSawtooth.tex}{abstract-HyperledgerSawtooth.tex}

 

Wordcount: 160


Quote ratio: 59.79 \%

ERROR: Quote ratio too high
 
Max Line length: 87
\end{IU}

\section{Google App Engine}
\index{Google App Engine}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Google App Engine, generally called App Engine is a Platform as a service cloud 
solution (PaaS). It lets you build and run applications on Google’s cloud 
infrastructure. In this platform the developer does not have to worry about 
infrastructure such as database administration, server configurations and 
load balancing which is done by google. Developers only job is to develop 
source codes.  It claims to be highly scalable as it can automatically 
increases capacity depending upon the
workloads~\cite{hid-sp18-415-www-scalabitity}.
 
Applications in App Engine can be run in either Flexible or Standard Environment or 
both can be used at the same time~\cite{hid-sp18-415-www-cloud-goggle}. 
Automatic scaling of apps, user customization of runtime (Eclipse Jetty 9, 
Python 2.7 and Python 3.6, Node.js, Ruby, PHP, \verb|.NET| core, and Go), operating 
system and even CPU memory  are some of the features of App Engine Flexible 
environment~\cite{hid-sp18-415-www-cloud-goggle}. Applications in Flexible 
environment run in Docker containers on Google Compute virtual machines. While 
in App Engine Standard Environment application instances are run in sandbox with 
prespecified runtime environment of supported language (Python 2.7, Java 7, 
Java 8, PHP 5.5 and GO 1.8,  1.6)~\cite{hid-sp18-415-app-engine}. 
This means if source code uses Python then its instances are run in Python runtime. 


\begin{IU}

hid-sp18-415

\href{https://github.com/cloudmesh-community/hid-sp18-415/blob/master//technology/abstract-googleappengine.tex}{abstract-googleappengine.tex}

 

Wordcount: 204


Quote ratio: 0.00 \%
 
Max Line length: 84
\end{IU}

\section{Google Compute Engine}
\index{Google Compute Engine}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As an infrastructure as a service Google Compute Engine (GCE) provides scalable, 
high performance  virtual machines to their clients~\cite{hid-sp18-415-cloud-google}. 
Its virtual machines vary in CPU and RAM configurations  and Linux distributions 
depending on clients’ need. Network storage are attached to virtual machines are 
attached as persistent disks. Each of these disks’ size can be upto 64TB and 
they are automatically resized based on demands~\cite{hid-sp18-415-cloud-google}. 
This feature of GCE’s virtual machines makes it scalable and reliable. Another 
feature of GCE includes its global load balancing technology which allows 
distribution of multiple instances across different region~\cite{hid-sp18-415-cloud-google}. 
It provides a platform to connect with other virtual machines to form a cluster or 
connect to other data centers or other google services~\cite{hid-sp18-415-cloud-google}. 
It can be managed through RestFul API or command line interface or web console. 
It claims to be cost effective and environmentally friendly compared to its 
competitors like Amazon Web Services.


\begin{IU}

hid-sp18-415

\href{https://github.com/cloudmesh-community/hid-sp18-415/blob/master//technology/abstract-googlecomputeengine.tex}{abstract-googlecomputeengine.tex}

 

Wordcount: 155


Quote ratio: 0.00 \%
 
Max Line length: 93
\end{IU}

\section{Heroku}
\index{Heroku}
 
Heroku, an open cloud platform provides developers a stage where they
can develop and deploy their applications. It is a Platform as a
service solution~\cite{hid-sp18-415-www-heroku}.  Its applications
are run in virtual containers called dynos and services are hosted by
Amazon’s EC2 cloud computing platform
~\cite{hid-sp18-415-wiki-heroku}. Dynos support languages like Node,
Python, Ruby, PHP, Scala, Clojure and Java. The applications or source
code and their framework and dependencies can be written in any of the
above specified languages as heroku supports them
~\cite{hid-sp18-415-how-heroku-works}. Source code dependency vary
according to the language being used. Applications are specified in a
text file called \textit{Procfile}
~\cite{hid-sp18-415-how-heroku-works}. Then these applications are
deployed remotely through git push. Besides Git, applications can be
deployed to Heroku using GitHub integration, Dropbox Sync., and Heroku
API~\cite{hid-sp18-415-how-heroku-works}. After applications are
deployed, compilation of source code, their dependencies and necessary
assets take place. The whole assembly of compilation is called slug
~\cite{hid-sp18-415-how-heroku-works}.  Then Heroku executes
application by running command specified in Procfile.  Commercial and
business applications like Macy’s, Toyota use Heroku cloud platform
because of its high scalability and its continuous
deployment. PostgreSQL, MongoDB, Redis and Cloudant are common
database choices of Heroku~\cite{hid-sp18-415-www-heroku}.


\begin{IU}

hid-sp18-415

\href{https://github.com/cloudmesh-community/hid-sp18-415/blob/master//technology/abstract-heroku.tex}{abstract-heroku.tex}

 

Wordcount: 173


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Microsoft Visual Studio}
\index{Microsoft Visual Studio}


Microsoft Visual Studio (MVS), community edition, is an open source \color{blue}``\emph{integrated
development interface IDE applicable for the development of computer programs, 
websites, web services, web and mobile apps}''\color{black}~\cite{hid-sp18-415-wikipedia-org}. While 
the interface consists of some built-in tools such as code editor, code profiler
and integrated debugger, it also supports plugins dependending on  requirements 
of visual designer. Javascript, C++, XML, CSS, \verb|.NET| are among some of the 
built -in languages for visual studio. But it supports quite a big number, 
that is 36 different, types of languages. Python, Ruby, Node.js and M are 
some of the languages available by plugins.  Multiple instances with their own
set of packages and specific App-id, can be run at the same time. MVS 
connects to windows AZURE making it portable for
developers~\cite{hid-sp18-415-wikipedia-org}.



\begin{IU}

hid-sp18-415

\href{https://github.com/cloudmesh-community/hid-sp18-415/blob/master//technology/abstract-mvs.tex}{abstract-mvs.tex}

 

Wordcount: 128

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 14.75 \%
 
Max Line length: 101
\end{IU}

\section{Pivotal Rabbit MQ}
\index{Pivital Rabbit MQ}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Pivotal RabbitMQ is a messaging broker platform used by various consumer 
applications like financial market data, system monitoring, business 
integration and various social, mobile, big data and cloud 
apps~\cite{hid-sp18-415-www-pivotal}. Its protocol based nature lets 
it connect across various other software components making it an ideal 
messaging platform for cloud computing. It is efficient, scalable, 
portable across most operating systems~\cite{hid-sp18-415-www-pivotal}. Its 
small disk and memory footprint makes it lightweight for use by 
developers. It has simple API and drivers are available for multiple 
languages like Python, PHp, Java. It also supports large scale 
messaging and routing according to topic and 
content~\cite{hid-sp18-415-www-pivotal}. It is one of the new trending 
tools for many web applications.


\begin{IU}

hid-sp18-415

\href{https://github.com/cloudmesh-community/hid-sp18-415/blob/master//technology/abstract-pivotalrabbitmq.tex}{abstract-pivotalrabbitmq.tex}

 

Wordcount: 116

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 78
\end{IU}

\section{ArangoDB}

ArangoDB is a NoSQL database system used to support multiple data models against
a single backend engine. ArangoDB supports three main models which are key-value
pair, document and graph. Compared to MongoDB, which is a document oriented
database, ArangoDB has added benefits such as scalability, lower operation
costs, supporting JOINs and complex transactions
\cite{hid-sp18-416-www-arangodb}. ArangoDB used its’ own query language AQL,
which is similar to SQL,
but has the benefit of querying a schema free database
\cite{hid-sp18-416-www-aql-blog}. ArangoDB provides flexibility in terms of
querying the data because
AQL can be used to query across all supported data models. This ease of use in
ArangoDB allows developers to represent the components of their systems by
models that are much more suitable. This is the reason for the gain in
popularity for native multi-model databases such as ArangoDB
\cite{hid-sp18-416-www-graphdb-blog}.

\begin{IU}

hid-sp18-416

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-416/blob/master//technology/abstract-arangodb.tex}{abstract-arangodb.tex}

 

Wordcount: 123

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Druid}

Druid is a high-performance, column-oriented, distributed datastore written in
Java. Druid can be used to analyze large volumes of real time streaming data as
well as historic data. Druid is horizontally scalable in a cost effective manner
and also has the ability to support multi-tenant
applications~\cite{hid-sp18-416-www-druid-wikipedia}. In addition to the above
mentioned key features Druid also includes the capability to execute fast Online
Analytical Processing (OLAP) queries and is fault-tolerant in
nature~\cite{hid-sp18-416-www-about-druid}. The Druid cluster is built from
components such as Druid segments, historical nodes, coordinator nodes and
broker nodes.  Druid also has been identified as a very fast analytics database
for fast real-time
applications~\cite{hid-sp18-416-www-fast-dataanalytics-druid-blog}.


\begin{IU}

hid-sp18-416

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-416/blob/master//technology/abstract-druid.tex}{abstract-druid.tex}

 

Wordcount: 95

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{MonetDB}

MonetDB is an open-source, column oriented database system. MonetDB
mainly targets being a backend database for business oriented
applications. These applications create very large databases having
millions of rows and hundreds of columns and MonetDB supports
scalability for such systems. MonetDB comprises of three software
components namely the SQL front-end, tactical-optimizers and
abstract-machine kernel~\cite{hid-sp18-416-www-monetdb-features}. In
contrast with MongoDB, the primary database model for MonetDB is a
Relational DBMS, but it also has additional document and key-value
stores. The most notable characteristic is that MonetDB is a
column-store in memory that is optimized for geo-spatial support and
has JSON document
support~\cite{hid-sp18-416-www-monetdb-mongodb-comparison}. MonetDB is
developed by CWI in Netherlands. It targets Big Data and Deep Learning
applications and Online Analytical Processing (OLAP) and it is widely
used in the Netherlands as an analytical software for Customer
Relationship Management (CRM). It can also be considered as a valuable
contribution to the IT industry from the
Dutch~\cite{hid-sp18-416-www-monetdb-dutch}.


\begin{IU}

hid-sp18-416

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-416/blob/master//technology/abstract-monetdb.tex}{abstract-monetdb.tex}

 

Wordcount: 135


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Morpheus}

Morpheus provides cloud and hybrid cloud solutions to improve the
efficiency of continuous development and integration life cycles by
focusing on devops and developer perspectives. The analytics offering
of Morpheus focuses on optimizing resource allocations on VM
environments, such as containers and public clouds, that distributes
over multiple clouds with platform independent discovery services. The
competitive edge that the Morpheus has over other VM boost up package
vendors is the Analytics’ ability to visualize platform wide resource
consumptions. The Analytics pack uses either built in cloud APIs or
specific agents to gather resource consumption information across all
the platforms and does the brokerage of incoming requests to minimize
the incurring resource consumption
costs~\cite{hid-sp18-416-www-morpheus-product-guide}.

Morpheus governance tool provides the ability to index, categorize and
store enterprise artifacts and provide life cycle management of the
artifacts. The integrated Role Based Access Control (RBAC) makes sure
that the artifacts are accessible only to the authorized people and
provides them with the ability to view certain aspects of the
artifacts. The artifacts could be anything that comes under the hood
of SOA governance~\cite{hid-sp18-416-www-soa-governance-wikipedia} and
they are managed by the policies defined and uploaded by the
authorized roles in the system.


\begin{IU}

hid-sp18-416

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-416/blob/master//technology/abstract-morpheus.tex}{abstract-morpheus.tex}

 

Wordcount: 175


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{WSO2 Analytics}

WSO2 Analytics is provided by the WSO2 Stream Processor. The features of the 
stream processor are support for stream and event processing contracts, user 
friendly development interfaces, high availability and scalability, easy 
integration with other components and business friendly analytics dashboards 
\cite{hid-sp18-416-www-wso2-stream-processor}. The stream processor is based on 
the Siddhi Processing engine~\cite{hid-sp18-416-github-siddhi} and is capable 
of  performing real-time analytics for different types of events. The query 
processing engine stays as the central component while the events pass through 
the engine. WSO2 drives the vision of digitizing businesses and analytics is a 
key part according to~\cite{hid-sp18-416-www-srinath-conference-talk}. The WSO2 
team emphasizes the need for analytics in businesses where automation and 
analytics is the highlight while taking the maximum use of contextual data 
\cite{hid-sp18-416-www-business-benefits-analytics}.

\begin{IU}

hid-sp18-416

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-416/blob/master//technology/abstract-wso2analytics.tex}{abstract-wso2analytics.tex}

 

Wordcount: 123

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Ansible}

Ansible is a widely popular open-source tool used for automation of
configuration management, application deployment. Ansible is popular
because of its simplicity. Originally, Ansible Inc was setup to manage
the product. Later in 2015, RedHat acquired Ansible.  \color{blue}``\emph{It uses no
agents and no additional custom security infrastructure, so it’s easy
to deploy and most importantly, it uses a very simple language (YAML,
in the form of Ansible Playbooks) that allow you to describe your
automation jobs in a way that approaches plain
English}''\color{black}~\cite{hid-sp18-417-doc-Ansible}.  An user doesn’t have to
learn a cryptic language to use it.  As no agents are required to be
installed in the nodes, the tool eases the network overhead. Ansible
may use two kinds of server for operation. One is the controlling
server that has Ansible installed.  The controlling server deploys
modules in the nodes through SSH channel.  The basic component of
Ansible archtecture are:

\begin{description}
\item[Modules:] This is the unit of work/task in Ansible. It
  can be written in any standard programming language
\item[Inventory:] Inventory is basically the nodes used
\item[Playbooks:] A play book in Ansible describes in simple
  language the infrastucture used for the deployment of the tool. This
  is written in YAML. 
\end{description}


\begin{IU}

hid-sp18-417

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-417/blob/master//technology/abstract-ansible.tex}{abstract-ansible.tex}

 

Wordcount: 189


Quote ratio: 20.67 \%
 
Max Line length: 84
\end{IU}


\section{Apache CloudStack}

Apache CloudStack is an open source that provides a highly scalable
and available cloud management platform for IT Enterprises and service
providers. CloudStack was originally developed by Cloud.com and was
known by the name VMOps.  In 2011, Citrix acquired the product and
donated it to Apache.  \color{blue}``\emph{CloudStack is being developed to help managed
service providers and enterprise IT departments create and operate
public cloud, private cloud or hybrid clouds with capabilities
equivalent to Amazon's Elastic Compute Cloud (Amazon EC2) It uses
existing hypervisors such as KVM, VMware ESXi|VMware vcenter and
XenServer/XCP for virtualization. In addition to its own API,
CloudStack also supports the Amazon Web Services (AWS) API[3] and the
Open Cloud Computing Interface from the Open Grid
Forum.}''\color{black}~\cite{hid-sp18-417-wiki-cloudStack}.  The key feature of the
product are (1) high availability of resources (2) network management
(3) provides GUI for ease of management (4) compatible with most of
the hypervisor/virtual monitor (5) it provides the snapshot
management. e.g.\ This feature is very useful is saving a state
[snapshot] of a vitual machine.  The vm can later be reverted to the
stored state.  The basic deployment of CloudStack just needs two
machines: A server and a hypervisor that is a monitoring system.  The
process can be over simplified by configuring one machine to serve
both the purpose.  The same simple system can easily be scaled to a
zone or a pod.  Figure~\ref{F:cloudstack-scalabuility} depicts how the
simplest deployment infrastructure can be scaled to provide an
advanced support system.


\begin{figure}[htb]
\includegraphics[width=\textwidth]{images/hid-sp18-417-cloudstack.png}
\caption{CloudStack Scalability~\cite{hid-sp18-417-cloudstack-scaling}}
\label{F:cloudstack-scalabuility}
\end{figure}


\begin{IU}

hid-sp18-417

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-417/blob/master//technology/abstract-apachecloudstack.tex}{abstract-apachecloudstack.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 12 in ../hid-sp18-417/technology/abstract-apachecloudstack.tex line 20:
Interword spacing (`\ ') should perhaps be used. management. e.g.\ This feature
is very useful is saving a state              ^
\end{verbatim}
\end{tiny}

Wordcount: 230


Quote ratio: 26.85 \%
 
Max Line length: 88
\end{IU}

\section{Apache Delta Cloud}

Apache DeltaCloud was developed in collaboration between Apache
Foundation and Redhat to provide a programming application that will
facilitate management of different cloud interfaces and It was
supporting all the major cloud interfaces.  \color{blue}``\emph{Each
Infrastructure-as-a-Service cloud existing today[when?] provides its
own API. The purpose of Deltacloud is to provide one unified
REST-based API that can be used to manage services on any cloud. Each
particular cloud is controlled through an adapter called a
\textit{driver}. As of June 2012, drivers exist for the following
cloud platforms: Amazon EC2, Fujitsu Global Cloud Platform, GoGrid,
OpenNebula, Rackspace, RHEV-M, RimuHosting, Terremark and VMware
vCloud}''\color{black}~\cite{hid-sp18-417-wiki-deltacloud}.

In 2009, DeltaCloud was developed for the purpose of providing one
unified API for the major cloud service.

In 2011, it became a part of the Apache’s top level project. 

Unfortunately, in 2015 the project was closed due to inactivity.  The
application though inactive is chosen for the study to understand the
case behind the termination of the project.  It is primarily because
of lack of popularity RedHat withdrew the sponsorship ultimately
resulting in the termination of the project.


\begin{IU}

hid-sp18-417

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-417/blob/master//technology/abstract-apachedeltacloud.tex}{abstract-apachedeltacloud.tex}

 

Wordcount: 166


Quote ratio: 36.93 \%

WARNING: Quote ratio very high
 
Max Line length: 69
\end{IU}


\section{OpenNebula}

OpenNebula is a useful opensource that enables seamless management and
control of different cloud systems.  The tools can be used for a cloud
implementations to virtualize data centers and also to obtain solution
for cloud infrastructure.  Opennebula can be adopted on top of an
existing cloud setup.  OpenNebula project started on 2005 and
currently the product is available as an open-source under Apache
license.
\color{blue}``\emph{The toolkit includes features for integration, management,
scalability, security and accounting.  It also claims standardization,
interoperability and portability, providing cloud users and
administrators with a choice of several cloud interfaces (Amazon EC2
Query, OGF Open Cloud Computing Interface and vCloud) and hypervisors
(Xen, KVM and VMware), and can accommodate multiple hardware and
software combinations in a data
center}''\color{black}~\cite{hid-sp18-417-opennebula-wiki}.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{images/hid-sp18-417-opennebula.png}
\caption{OpenNebula Deployment Model~\cite{hid-sp18-417-opennebula-deployment}} 
\label{F:opennebula}
\end{figure}


The OpenNebula deployment needs (1) A client node (2) A hypervisor (3)
A data storage system (4) Physical network.  The deployment model is
depicted in~\ref{F:opennebula}.  Due to its long steady growth, the
tool is being used by customers in various industries ranging from
telecom to education.  The wide range of customer base is helpful in
providing a solid support system to the new and existing users as well
as continuous feedback becomes vital in the research and growth of the
project.


\begin{IU}

hid-sp18-417

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-417/blob/master//technology/abstract-opennebula.tex}{abstract-opennebula.tex}

 

Wordcount: 191


Quote ratio: 26.93 \%
 
Max Line length: 80
\end{IU}

\section{Open Refine}

OpenRefine is a useful open source that is used for data visualization
and analysis.  Its predominantly used for cleaning messy data and
transformation of data from one format to other for ease of clarity.
OpenRefine was formerly known as GoogleRefine. The tool is also used
for fetching data from websites and data organization. It can import
data from CSV, TSV, Excel, XML etc. It is written in Java. It works
with data in tabular format like in relational data. The tool has a
user interface that is available to be downloaded.  \color{blue}``\emph{Once you get
used to which commands do what, this is a powerful tool for data
manipulation and analysis that strikes a good balance between
functionality and ease of use}''\color{black}~\cite{hid-sp18-417-openrefine}.



\begin{IU}

hid-sp18-417

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-417/blob/master//technology/abstract-openrefine.tex}{abstract-openrefine.tex}

 

Wordcount: 114

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 22.92 \%
 
Max Line length: 84
\end{IU}

\section{AtomSphere}

Boomi AtomSphere is basically an integration platform that supports all 
application integration processes between cloud platforms, SaaS and local 
systems as well. Boomi AtomSphere allows its customers to design cloud based 
processes called Atoms, which defines the necessities for the integration.
It can dedicate \color{blue}``\emph{separate environments for testing, perform 
parallel processing, message based queuing is a part of its service}''\color{black} and it 
also allows its run time engines to cluster 
\cite{hid-sp18-418-AtomSphere-features}. 


\begin{IU}

hid-sp18-418

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-418/blob/master//technology/abstract-atomsphere.tex}{abstract-atomsphere.tex}

 

Wordcount: 71

ERROR: This abstract is too short.


Quote ratio: 21.13 \%
 
Max Line length: 91
\end{IU}

\section{CloudHub}

CloudHub is a cloud-based integration platform by MuleSoft which is mainly 
used for connecting SaaS, cloud and local applications and Application 
interfaces. CloudHub is an elastic cloud that can scale on demand. We can 
publish REST API's on it. \color{blue}``\emph{The CloudHub Virtual Private Cloud (VPC) offering 
enables to construct a secure pipe to on-premise applications through an 
IPsec VPN tunnel, VPC Peering or Direct Connect}''\color{black} 
\cite{hid-sp18-418-CloudHub-docs}. CloudHub has a REST API which can perform 
tasks such as manage, monitor and scale applications.



\begin{IU}

hid-sp18-418

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-418/blob/master//technology/abstract-cloudhub.tex}{abstract-cloudhub.tex}

 

Wordcount: 82

ERROR: This abstract is too short.


Quote ratio: 29.90 \%
 
Max Line length: 96
\end{IU}

\section{RightScale Cloud Management}

RightScale Cloud Management is basically a platform which acts as a console to
manage different clouds from one environment. Some of its features are
automatic recovery protocols when it detects an escalation, disaster recovery 
architecture, automatic scaling and scripting.\color{blue}``\emph{TThis platform facilitates ways 
to deploy and manage business-critical applications across public, 
private and hybrid clouds and provides configuration, monitoring, automation, 
and governance of cloud computing infrastructure and
applications}''\color{black} ~\cite{hid-sp18-418-RightScale-Cloud-Management-article}.



\begin{IU}

hid-sp18-418

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-418/blob/master//technology/abstract-rightscale-cloud-management.tex}{abstract-rightscale-cloud-management.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 39 in ../hid-sp18-418/technology/abstract-rightscale-cloud-
management.tex line 10: Double space found. applications'' ~\cite{hid-
sp18-418-RightScale-Cloud-Management-article}.                 ^
\end{verbatim}
\end{tiny}

Wordcount: 67

ERROR: This abstract is too short.


Quote ratio: 38.93 \%

WARNING: Quote ratio very high
 
Max Line length: 98
\end{IU}

\section{Sales Cloud}

Sales Cloud is basically a part of the sales module of SalesForce. It is a 
platform which integrates the customer data together and it incorporates 
marketing, sales, customer service and business analytics functionalities.
One of Sales Cloud’s most important feature sets is \color{blue}``\emph{sales performance 
management software. The sales performance management covers incentives,
commissions, quotas, regions, goal setting, training and performance
evaluation}''\color{black} ~\cite{hid-sp18-418-Sales-Cloud-features}. It also has features 
which enable us to construct dashboards and perform real time forecasting 
which are useful for data analytics. It has a mobile application of the same 
thereby providing more ease of access and portability.


\begin{IU}

hid-sp18-418

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-418/blob/master//technology/abstract-sales-cloud.tex}{abstract-sales-cloud.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 39 in ../hid-sp18-418/technology/abstract-sales-cloud.tex line 9: Double
space found. evaluation'' ~\cite{hid-sp18-418-Sales-Cloud-features}. It also has
features                ^
\end{verbatim}
\end{tiny}

Wordcount: 97

ERROR: This abstract is too short.


Quote ratio: 23.43 \%
 
Max Line length: 91
\end{IU}

\section{Teradata Intelliflex}

Teradata Intelliflex is an integrated environment for Data Warehouse 
functionalities which in its own way integrates some of the strategic and 
operational workload onto one Data warehouse. It is available on Intellicloud, 
which is a cloud offering of Teradata. Intelliflex can \color{blue}``\emph{independently 
scale nodes}''\color{black} enabling us to use nodes as required to manage the processing 
power and also \color{blue}``\emph{store data on multiple layers of solid state drives}''\color{black} with 
virtual storage as per our data
requirements~\cite{hid-sp18-418-Teradata-Intelliflex-features}. 


\begin{IU}

hid-sp18-418

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-418/blob/master//technology/abstract-teradata-intelliflex.tex}{abstract-teradata-intelliflex.tex}

 

Wordcount: 76

ERROR: This abstract is too short.


Quote ratio: 15.84 \%
 
Max Line length: 108
\end{IU}

\section{Clive}\index{Clive}\index{Plan 9}\index{NIX}

Clive is an open-source, distributed operating system written in Go by
the Laboratorio De Sistemas at Universidad Rey Juan Carlos in
Madrid~\cite{hid-sp18-419-www-clive-lsub}. The design goal is to create
an environment where applications and services can be compiled along
with libraries that permit them to run on bare hardware without a
software stack~\cite{hid-sp18-419-www-clive-lsub2014}. The design is
based on Plan 9, a research system developed at Bell
Labs in the late 1980s and first released in
1992\cite{hid-sp18-419-www-about-plan9}, and NIX, a
\color{blue}``\emph{purely functional package manager}''\color{black}~\cite{hid-sp18-419-www-about-nix}
derived from Plan 9 that runs on Linux and Mac OS X.


\begin{IU}

hid-sp18-419

\href{https://github.com/cloudmesh-community/hid-sp18-419/blob/master//technology/abstract-clive.tex}{abstract-clive.tex}

 

Wordcount: 85

ERROR: This abstract is too short.


Quote ratio: 5.97 \%
 
Max Line length: 103
\end{IU}

\section{HCatalog}\index{HCatalog}\index{Hive}\index{Pig}\index{MapReduce}

HCatalog, which was originally known as Howl, is a component shipped
with Hive that manages storage and tables. Its purpose is to simplify
data storage and retrieval by providing a shared schema and data type
mechanism between Hive, Pig, and MapReduce and the formats
in which a Hadoop serializer-deserializer can be written (ORC,
RCFile, CSV, JSON, and SequenceFile.) Custom formats can be added as
well. A REST API called WebHCat (originally Templeton) is also
available~\cite{hid-sp18-419-www-hc-wiki}.


\begin{IU}

hid-sp18-419

\href{https://github.com/cloudmesh-community/hid-sp18-419/blob/master//technology/abstract-hcatalog.tex}{abstract-hcatalog.tex}

 

Wordcount: 68

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 74
\end{IU}

\section{Neptune}
\index{Neptune}
\index{AWS}
\index{Graph database}
\index{NoSQL}

Neptune is a graph database service that was announced at the AWS
Re:INVENT conference in November of
2017~\cite{hid-sp18-419-www-tc_neptune}. Graph databases are NoSQL
databases that used graph structures to organize
data~\cite{hid-sp18-419-www-tp-graph-db}. They are commonly used for
social networking applications, but can be used for recommendation
engines, logistics, and other applications. Amazon offers Neptune as a
fully managed product. It supports Apache TinkerPop Gremlin and SPARQL
open source graph APIs. One can choose Gremlin or the W3C standard
Resource Description Framework
model~\cite{hid-sp18-419-www-aws-neptune}.


\begin{IU}

hid-sp18-419

\href{https://github.com/cloudmesh-community/hid-sp18-419/blob/master//technology/abstract-neptune.tex}{abstract-neptune.tex}

 

Wordcount: 71

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{OpenDaylight}\index{OpenDaylight}\index{SDN}\index{NFV}

OpenDaylight is an open source Software Defined Networks (SDN)
controller\cite{hid-sp18-419-www-opendaylight}. SDN separates network
control logic from physical networking equipment. The result is that
networking equipment is programmable like other computing
platforms. SDN facilitates Network Functions Virtualization (NFV),
allowing virtual network services (switching, virtualized appliances,
and virtualized applications) to be deployed without having to deploy
specialized physical devices\cite{hid-sp18-419-www-cio-sdn-nfv}. The
OpenDaylight project was founded by a group of large tech companies,
including Cisco, Citrix, Ericsson, HP, IBM, Microsoft, NEC, Red Hat,
and VMware. Microsoft and VMware have since left the
project\cite{hid-sp18-419-www-sdx-odl}.


\begin{IU}

hid-sp18-419

\href{https://github.com/cloudmesh-community/hid-sp18-419/blob/master//technology/abstract-opendaylight.tex}{abstract-opendaylight.tex}

 

Wordcount: 78

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 69
\end{IU}

\section{Pig}\index{Pig}\index{Pig Latin}\index{Map-Reduce}\index{Apache}\index{Hadoop}

Pig is a part of the Apache Hadoop ecosystem consisting of a scripting
language called Pig Latin and a compiler that produces Map-Reduce
programs. It was initially developed in 2006 at Yahoo! and taken over
by Apache in 2007\cite{hid-sp18-419-die2015datascience}. Pig Latin
allows developers to code multiple interrelated data transformations
as data flow sequences, with the goal fo making the code readable and
easy to maintain. Pig optimized Pig Latin automatically and users can
extend the language with purpose-written
functions\cite{hid-sp18-419-www-pig}. There is some overlap in
functionality between Pig and Hive, an SQL-like language that is also
a part of the Apache Hadoop ecosystem. Pig tends to be favored by
programmers and researchers, whereas Hive is preferred by data
analysts\cite{hid-sp18-419-www-dezyre-pig}.


\begin{IU}

hid-sp18-419

\href{https://github.com/cloudmesh-community/hid-sp18-419/blob/master//technology/abstract-pig.tex}{abstract-pig.tex}

 

Wordcount: 107

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 87
\end{IU}

\section{Amazon Elastic Beanstalk}

AWS Elastic Beanstalk~\cite{hid-sp18-420-amazon-elastic-beanstalk} is a managed
service used for application deployment and management. Using EBS it is easy to
quickly deploy and manage applications in the AWS Cloud. Developers simply
upload their application, and Elastic Beanstalk automatically handles the
deployment details of capacity provisioning, load balancing, auto-scaling, and
application health monitoring~\cite{hid-sp18-420-amazon-elastic-beanstalk-FAQ}.

Elastic Beanstalk supports applications which are developed in Java, PHP, \.NET,
Node.js, Python, and Ruby as well as different container types for each
language. A container is used to define the infrastructure and technology stack
to be used for a given
environment~\cite{hid-sp18-420-amazon-elastic-beanstalk-FAQ}. AWS Elastic
Beanstalk runs on the Amazon Linux AMI and the Windows Server 2012 R2 AMI
provided by Amazon. Initially, it takes some time to create AWS resources
required to run the application. User then can have multiple versions of their
applications running at the same time. Hence, user can create different
environments such as staging and production where each environment runs with its
own configurations and resources. AWS Elastic Beanstalk does not have any extra
charges. Users need to pay for the resources they have used to store and run the
applications such as EC2, S3, RDS or any other resources used.


\begin{IU}

hid-sp18-420

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-420/blob/master//technology/abstract-AmazonElasticBeanstalk.tex}{abstract-AmazonElasticBeanstalk.tex}

 

Wordcount: 180


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Amazon Glacier}

Amazon Glacier is an online file storage web service provided by Amazon which
can be used for data archiving and backup~\cite{hid-sp18-420-Amazon-Glacier}.
Glacier is part of the Amazon Web Services suite designed for long term storage
of data that is accessed infrequently. User can store virtually any kind of data
in any format.

Amazon also provides Simple Storage Service for storing and retrieving data but
Glacier is much cheaper than S3. As per AWS documentation, \color{blue}``\emph{ For Amazon glacier,
storage costs are a consistent \$0.004 per gigabyte per month, which is
substantially cheaper than Amazon's own Simple Storage
Service}''\color{black}~\cite{hid-sp18-420-Amazon-Glacier-FAQ}. \color{blue}``\emph{Customers can store data to
Amazon Glacier with a significant saving as compared to on-premise storage.
Amazon Glacier is designed to provide average annual durability of
99.999999999\% for an archive}''\color{black}~\cite{hid-sp18-420-Amazon-Glacier}. Data is
stored in Amazon Glacier as archives. Archives can be deleted at any point of
time and billing will be updated accordingly.

Amazon Glacier provides three options for access to archives, from a few minutes
to several hours. The AWS Management console is used for Amazon Glacier set up.
User can upload and retrieve data programmatically in later phases.


\begin{IU}

hid-sp18-420

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-420/blob/master//technology/abstract-AmazonGlacier.tex}{abstract-AmazonGlacier.tex}

 

Wordcount: 172


Quote ratio: 27.91 \%
 
Max Line length: 111
\end{IU}

\section{Amazon RDS}

Amazon RDS~\cite{hid-sp18-420-amazon-RDS} stands for Amazon Relational Database
Service. Amazon RDS gives access to MySQL, MariaDB, Oracle, SQL Server, or
PostgreSQL database. It is a managed service provided by AWS which can be used
to manage different database administrative tasks. User can select the type of
RDS instance and accordingly AWS provides capacity. RDS has capacity to resize
as per requirement which enables user to change from one instance type to
another instance type without losing its data. It is cost effective and the
costing depends on the instance type~\cite{hid-sp18-420-amazon-RDS-FAQ}.

\color{blue}``\emph{Amazon RDS can automatically backup database and keep that database software
up to date with its latest version. RDS makes it easy to use replication to
enhance database availability, improve data durability, or scale beyond the
capacity constraints of a single database instance for read-heavy database
workloads}''\color{black}~\cite{hid-sp18-420-amazon-RDS-FAQ}. High availability is achieved by
built-in automated failover from primary database to a replicated secondary
database in case of any failure. This replicated secondary database in sync with
primary database.


\begin{IU}

hid-sp18-420

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-420/blob/master//technology/abstract-AmazonRDS.tex}{abstract-AmazonRDS.tex}

 

Wordcount: 152


Quote ratio: 26.46 \%
 
Max Line length: 96
\end{IU}

\section{Amazon S3}

Amazon S3 is a simple storage service which mainly focuses on a highly-scalable,
reliable, and low-latency data storage infrastructure at low
costs~\cite{hid-sp18-420-amazon-S3-FAQ}. Simple storage Service is a web service
provided by Amazon that can be used to store and retrieve data. It can also be
used for static website hosting for different web applications. Important
feature of S3 is that it is available at any point of time and can be used to
store virtually any kind of data in any
format~\cite{hid-sp18-420-amazon-S3-FAQ}.

One of the important features of using S3 is that it offers a highly durable,
scalable, and secure destination for backing up and archiving critical
data~\cite{hid-sp18-420-amazon-S3}. As per AWS documentation, \color{blue}``\emph{Amazon S3 is
designed to deliver 99.999999999\% durability, and it is used to store data for
millions of applications used by market leaders in every
industry}''\color{black}~\cite{hid-sp18-420-amazon-S3}.

Amazon S3 provides versioning capability to provide even further protection for
stored data. It is easy to define lifecycle rules to automatically migrate less
frequently accessed data. User can store any number of objects. Total volume is
unlimited but one object size can range from 0 bytes to 5 terabytes. With Amazon
S3, user needs to pay only for what the usage is. But price vary as per the
chosen region of S3.


\begin{IU}

hid-sp18-420

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-420/blob/master//technology/abstract-AmazonS3.tex}{abstract-AmazonS3.tex}

 

Wordcount: 193


Quote ratio: 12.09 \%
 
Max Line length: 94
\end{IU}

\section{PostgreSQL}

PostgreSQL, often refered as Postgres~\cite{hid-sp18-420-PostgreSQL_About}, is an
open source, object-relational database management system. PostgreSQL is free,
extensible and supports cross platform feature. Its source code is available
with open source licence. Postgres was created at UCB by a computer science
professor named Michael Stonebraker~\cite{hid-sp18-420-PostgreSQL_History}.

PostgreSQL runs on all major operating systems. Initially it was designed to run
on UNIX platforms. Now it works on 34 platforms of Linux along with other
platforms such as all Windows versions, Mac OS X and Solaris. It supports text,
images, sounds, video and includes programming interfaces for different
languages such as C, C++, Java, Perl, Python, Ruby, Tcl and Open Database
Connectivity.

PostgreSQL is completely ACID compliant and transactional. It has complete
support for different features such as foreign keys, joins, views, triggers, and
stored procedure~\cite{hid-sp18-420-PostgreSQL_Wiki}. It includes almost all
data types that are used in SQL, such as INTEGER, NUMERIC, BOOLEAN, CHAR,
VARCHAR, DATE, INTERVAL, and TIMESTAMP data type. It also supports storage of binary large
objects, including pictures, sounds, or
video~\cite{hid-sp18-420-PostgreSQL_About}.


\begin{IU}

hid-sp18-420

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-420/blob/master//technology/abstract-PostgreSql.tex}{abstract-PostgreSql.tex}

 

Wordcount: 152


Quote ratio: 0.00 \%
 
Max Line length: 90
\end{IU}

\section{Apache Avro}

Avro~\cite{hid-sp18-421-AvroCloud} is a framework for data serialization, where
serialization is a process of translating object or data structure into a format
that can be stored. Apache Avro can translate very high datastructure formats.
It provides binary data format which is very fast and compact. Avro can also
provide Remote procedure calls.

Avro is completely based on schemas. Data is stored in file along with the
schema and can be read by any program, since schema is available when ever data
is read or written it can be used as dynamic scripting languages.

Avro differs from similar systems like Thrift, Protocol Buffers by schema
evaluation, untagged data and dynamic typing.



\begin{IU}

hid-sp18-421

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-421/blob/master//technology/abstract-ApacheAvro.tex}{abstract-ApacheAvro.tex}

 

Wordcount: 101

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Apache Chukwa}

Chukwa~\cite{hid-sp-421-ChukwaCloud} is a data collection system built on top of
Hadoop to monitor large distributed file systems. It collects data from various
data providers and analyses them using MapReduce. Chukwa inherits Hadoop’s
scalability and robustness. Chukwa has mainly four components: Relies on data
agents. Collectors collect data and gives it to stable storage. This data is
parsed and archived using MapReduce jobs. It provides interface to analyse and
display results~\cite{hid-sp-421-ChukwaComponents}.




\begin{IU}

hid-sp18-421

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-421/blob/master//technology/abstract-ApacheChukwa.tex}{abstract-ApacheChukwa.tex}

 

Wordcount: 65

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Apache Whirr}

Apache Whirr provides collection of libraries for running cloud
services in a neutral way. Whirr began as a set of shell scripts for
running Hadoop on Amazon EC2, and later matured to include a Java API
based on the Apache jclouds project.

It defines the layout of clusters, It also has scripts to run
operations to start, stop and terminate new
clusters~\cite{hid-sp18-421-whirrCloud}.



\begin{IU}

hid-sp18-421

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-421/blob/master//technology/abstract-ApacheWhirr.tex}{abstract-ApacheWhirr.tex}

 

Wordcount: 57

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 69
\end{IU}

\section{Apache Zookeeper}

Zookeeper is a open source centralized service that enables synchronization
across cluster. It is also designed to maintain naming, configuration
information, and provide  group services.

An application can create znode in Zookeeper which can be updated by any node in
the cluster and updates on that node can have track of changes to that znode.
This kind of znodes are used to keep track of updates in the entire cluster
which is how it provides centralized 
infrastructure~\cite{hid-sp18-421-zookeeper}.




\begin{IU}

hid-sp18-421

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-421/blob/master//technology/abstract-ApacheZookeeper.tex}{abstract-ApacheZookeeper.tex}

 

Wordcount: 73

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Apache BigTop}

BigTop~\cite{hid-sp18-421-BigTopCloud} is Apache Foundation project for
comprehensive packaging, testing and configuration of bigdata components. It
supports Hadoop eco system. It packages RPMs and DEBs so that we can manage and
maintain Hadoop Cluster. It provides an integrated smoke testing framework.
BigTop provides vagrant recipes, raw images and docker recipes to deploy Hadoop
from zero.



\begin{IU}

hid-sp18-421

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-421/blob/master//technology/abstract-BigTop.tex}{abstract-BigTop.tex}

 

Wordcount: 50

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Apache Couch DB}
\index{CouchDB}

Apache Couch DB is a NoSQL database which uses document instead of tables to
store the data. It simplifies the interaction with application as data can be
fetched or stored in form of JSON objects~\cite{hid-sp18-502-ApacheCouchDB}.
The main advantage of using CouchDB is that it is compatible with variety of
application. It can be used to integrate data from web-based applications,
mobile applications, web browsers to distributed server clusters. It makes
transfer between all these components happen smoothly while providing high
performance and totally reliable framework. It also supports Map-reduce
operations~\cite{hid-sp18-502-ApacheCouchDB}.


\begin{IU}

hid-sp18-502

\href{https://github.com/cloudmesh-community/hid-sp18-502/blob/master//technology/abstract-apachecouchdb.tex}{abstract-apachecouchdb.tex}

 

Wordcount: 84

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 77
\end{IU}

\section{Cloud Bigtable}
\index{Bigtable}

Cloud Bigtable is googles NoSQL big data storage service. This is currently
used by google itself for their own services like search engine, Gmail, Maps.
It is highly scalable big database with low latency and high throughput achieved
by distributed computing. Using this database makes it easy to integrate the
database with other cloud services provided from google such as cloud dataflow.
Also it allows to integrate with big data tools like Hadoop. It can achieve high 
performance of millions of transactions per  
second~\cite{hid-sp18-502-GoogleCloudBigtable}.





\begin{IU}

hid-sp18-502

\href{https://github.com/cloudmesh-community/hid-sp18-502/blob/master//technology/abstract-googlecloudbigtable.tex}{abstract-googlecloudbigtable.tex}

 

Wordcount: 81

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 81
\end{IU}

\section{Presto}
\index{Presto}

Presto is a SQL query engine developed specially for interactive analytics. It
focuses on large commercial data warehouses with capacity of gigabytes to
petabytes. It is open source and used for distributed systems. It is compatible
with relational as well as NoSQL of data sources such as Cassandra and 
Hive~\cite{hid-sp18-502-Presto}.

It is being used by big organizations like Facebook to run interactive queries
against their large data warehouses. The main advantage of using Presto is that
it allows to perform analytics on data from different data sources using single
query. This allows data to be combined across organizations without extra
overhead of separate queries for each data source~\cite{hid-sp18-502-Presto}.



\begin{IU}

hid-sp18-502

\href{https://github.com/cloudmesh-community/hid-sp18-502/blob/master//technology/abstract-presto.tex}{abstract-presto.tex}

 

Wordcount: 100

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Rapid Miner}
\index{RapidMiner}

Rapid miner is open source data science platform for data mining, machine
learning and deep learning related projects. It is also used for
extraction-transaction-loading operations. It eliminates the need to write code
to perform machine learning on data. It is developed in java and can be extended
 by using plugins~\cite{hid-sp18-502-RapidMiner}.
Rapid miner treats each process as operators. Input of each operation goes to
following operator. It is very easy to use and can be integrated with Python or
R projects~\cite{hid-sp18-502-RapidMiner}.





\begin{IU}

hid-sp18-502

\href{https://github.com/cloudmesh-community/hid-sp18-502/blob/master//technology/abstract-rapidminer.tex}{abstract-rapidminer.tex}

 

Wordcount: 75

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Talend}
\index{Talend}

Talend is an open source software that provides variety of tools for integration
 of data of an organization. It also helps to synchronize data between different
 systems. Some tools help to generate native code to deploy the data to hadoop.
Talend is a drag on drop software helps to configure prebuilt components and
clean the data from different sources. It also contains tools to check data
quality for clients so that they can decide whether they need to clean the data
before integrating it with clean datasets~\cite{hid-sp18-502-Talend}.




\begin{IU}

hid-sp18-502

\href{https://github.com/cloudmesh-community/hid-sp18-502/blob/master//technology/abstract-talend.tex}{abstract-talend.tex}

 

Wordcount: 83

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Apache Carbondata}
\index{Carbondata}
\index{Apache Carbondata}

As the amount of data we have increases storing and performing
analytics of this data becomes increasingly difficult.
Apache carbondata is an indexed file format for storing big data
that allows faster analysis on a huge amounts of 
data~\cite{hid-sp18-503-www-carbondata-docs}. 
Carbondata runs on top of hadoop YARN
and spark and can be uses columnar storage, compression and
encoding techniques to perform faster queries on the data.

An Apache Carbondata file system consists of groups of data called blocklets
and stores information like schema, in the header and footer co-located in HDFS.
The Footer is read once to create the index which is later utilized to optimize 
queries~\cite{hid-sp18-503-www-carbondata-docs}.

Apache Carbondata allows operations like creating tables, updating and
deleting them and performing queries on these 
tables~\cite{hid-sp18-503-www-carbondata-mgmt}.





\begin{IU}

hid-sp18-503

\href{https://github.com/cloudmesh-community/hid-sp18-503/blob/master//technology/abstract-carbondata.tex}{abstract-carbondata.tex}

 

Wordcount: 115

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Apache Edgent}
\index{Edgent}
\index{Apache Edgent}

The number of connected devices are constantly increasing.
Many of the devices that form the internet of thingss (IoT) are sensors,
or devices that are lightweight and do not have a lot of storage space
or processing power.
Apache Edgent is a programming model that allows development in Java and
Android environments and provides a way to perform analytics locally on
the edge devices, thus preventing the need to send data back and forth to
servers~\cite{hid-sp18-503-www-edgent}. Apache Edgent can help reduce
the amount of data needed to be stored as analytics can be performed on
the data continuously and, only relevent information such as outliers
that need to be recorded or data that needs further analysis that require
higher computation resources need to be sent to the server
\cite{hid-sp18-503-www-edgent-docs}.

Apache edgent can work along with centralized analytics tools thus providing
a way to do more thorough analysis on the IoT system. Edgent can
communicate with with backend systems using common messaging hubs and
communication protocols like MQTT, Apache Kafka, IBM Watson IoT platform,
and allows custom message hubs as well~\cite{hid-sp18-503-www-edgent-docs}.


\begin{IU}

hid-sp18-503

\href{https://github.com/cloudmesh-community/hid-sp18-503/blob/master//technology/abstract-edgent.tex}{abstract-edgent.tex}

 

Wordcount: 165


Quote ratio: 0.00 \%
 
Max Line length: 76
\end{IU}

\section{Apache Gobblin}
\index{Gobblin}
\index{Apache!Gobblin}

As the amount of data increases and its sources become numerous, it
gets difficult to integrate this data to solve a specific
problem. Apache Gobblin~\cite{hid-sp18-503-www-gobblin} is a distributed
data integration framework that allows users to build different data
integration applications, usually as separate jobs wihch are executed
with the help of a scheduler
\cite{hid-sp18-503-www-gobblin-docs}. Gobblin can be deployed in a
stand alone manner and also supports deployment on a Hadooop, Apache
Mesos or Amazon Elastic Cloud cluster~\cite{hid-sp18-503-www-gobblin}.

Currently Gobblin deployments run independently of each other and
there is no central management o orchestration. However, efforts are
being made to develop Gobblin-as-a-Service which whould manage data
integration jobs on any mode of Gobblin 
deployment~\cite{hid-sp18-503-www-gobblin-docs}.


\begin{IU}

hid-sp18-503

\href{https://github.com/cloudmesh-community/hid-sp18-503/blob/master//technology/abstract-gobblin.tex}{abstract-gobblin.tex}

 

Wordcount: 102

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 72
\end{IU}

\section{Apache Gossip}
\index{Gossip}
\index{Apache Gossip}

Many of the applications that are cloud based and require a huge amount of
computation or data storage in the back end, use cloud based clusters. When
different nodes in a cluster rely on services provided by other nodes, it
becomes important that each node has information about the others, to avoid
failures. Apache Gossip is a protocol that provides a method that allows
nodes to form a peer-to-peer network and allows them to discover other
nodes and check the liveliness of the cluster
\cite{hid-sp18-503-www-gossip}.

The name arises from the family of protocols known as gossip protocols or
epidemic protocols that disemminate information in a manner similar to how
gossip spreads in a community. Each node periodically selects any other
node at random and shares the information it has, thus eliminating the need
to broadcast from every node to every other node in the cluster
\cite{hid-sp18-503-www-gossip-wiki}.


\begin{IU}

hid-sp18-503

\href{https://github.com/cloudmesh-community/hid-sp18-503/blob/master//technology/abstract-gossip.tex}{abstract-gossip.tex}

 

Wordcount: 135


Quote ratio: 0.00 \%
 
Max Line length: 75
\end{IU}

\section{Apache Milagro}
\index{Mlagro}
\index{Apache Milagro}

As an increasing number of connected devices communicate data with
each other, data security must be taken into account. Apache Milagro
is a security framework, built for cloud based software and Internet
of Things (IoT) applications, that reauire to be
scalable~\cite{hid-sp18-503-www-milagro}. Apache milagro is a
pairing-based cryptography system that distriburtes cryptographic
operations among various entities to provide a deeper level of
security as compared to monolithic certificate based systems used
today~\cite{hid-sp18-503-www-milagro-docs}.

Apache milagro avoids the probelms faced by certificate based systems
like Public Key Infrastructure (PKI) such as single point of failure,
by the use of distributed trust authorities (D-TAs), which hold a part
of a client's key each and are isolated from each other. The absence
of a central certificate provider means that anyone can be a D-TA the
key lifecycle is a part of ythe crypto system
itself~\cite{hid-sp18-503-www-milagro-docs}.


\begin{IU}

hid-sp18-503

\href{https://github.com/cloudmesh-community/hid-sp18-503/blob/master//technology/abstract-milagro.tex}{abstract-milagro.tex}

 

Wordcount: 127

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{BigML}
\index{BigML}

BigML~\cite{hid-sp18-504-cloudacad-bigml} is a Machine Learning platform
focused on delivering a wide range of ML solutions, while continuing to 
maintain an easy-to-use and intuitive user experience. Considered as 
a MLaaS~\cite{hid-sp18-504-cloudacad-bigml}, BigML integrates with most 
Cloud storage systems in order to load data and train machine learning 
models to develop predictive insights. BigML tackles machine learning tasks 
ranging from Classification and Regression to Association Discovery and 
Topic Modeling~\cite{hid-sp18-504-bigml}, boasts a rich number of 
visualizations, and offers both a UI and API interfaces to deliver 
insight~\cite{hid-sp18-504-cloudacad-bigml}.


\begin{IU}

hid-sp18-504

\href{https://github.com/cloudmesh-community/hid-sp18-504/blob/master//technology/abstract-bigml.tex}{abstract-bigml.tex}

 

Wordcount: 82

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 76
\end{IU}

\section{Datalab}
\index{Datalab}
\index{Google Datalab}
\index{Google}

Cloud Datalab~\cite{hid-sp18-504-google-datalab} is an open source tool 
part of the Google Cloud Platform suite which focuses on delivering analytic, 
machine learning and visualization solutions. Fully integrated with the 
Platform Suite, Datalab takes full advantage by leveraging the data stored 
in various other Google solutions ranging from BigQuery and Machine Learning 
Engine to Compute Engine and Cloud Storage~\cite{hid-sp18-504-google-datalab}. 
In addition to this integration, Cloud Datalab delivers a simple solution for 
developers to generate rich, insightful reports in a timely fashion that can 
further assist in their development tracks with greater
ease~\cite{hid-sp18-504-techcrunch-datalab}.  
Based on Jupyter~\cite{hid-sp18-504-google-datalab}, Datalab has the 
advantage of a rich knowledge-base to start, and encourages developers 
to extend this open source software by simply 
forking it~\cite{hid-sp18-504-techcrunch-datalab}.


\begin{IU}

hid-sp18-504

\href{https://github.com/cloudmesh-community/hid-sp18-504/blob/master//technology/abstract-datalab.tex}{abstract-datalab.tex}

 

Wordcount: 117

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Distributed Machine Learning Tool Kit}
\index{DMTK}
\index{Microsoft}

Distributed Machine Learning Tool Kit, or otherwise known as 
DMTK~\cite{hid-sp18-504-dmtk}, is Microsoft's response to the resource 
challenges that distributed machine learning faces. Given that machine 
learning trends are leaning towards leveraging more, and more, data to 
produce accurate results, Microsoft's DMTK attempts to mitigate the 
difficulties of keeping up with an equally demanding need for computational 
resources and scalability~\cite{hid-sp18-504-dmtk}. DMTK boasts an increasing 
number of components~\cite{hid-sp18-504-msresearch} ranging from topic 
model training algorithms to NLP algorithms, and counting. By making 
DMTK open source, Microsoft hopes to take advantage of the contributions 
from practitioners and researchers alike in order to enhance the DMTK's 
components and make this powerful tool even more 
versatile~\cite{hid-sp18-504-msresearch}.


\begin{IU}

hid-sp18-504

\href{https://github.com/cloudmesh-community/hid-sp18-504/blob/master//technology/abstract-dmtk.tex}{abstract-dmtk.tex}

 

Wordcount: 114

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 78
\end{IU}

\section{KNIME}
\index{KNIME}
\index{KNIME Analytics Platform}

KNIME Analytics Platform~\cite{hid-sp18-504-pred-knime} is an open source 
platform focused on including machine learning components, data mining, 
analytics, and reporting; all executed through an intuitive visual interface. 
Since coding is optional through KNIME's graphical 
interface~\cite{hid-sp18-504-knime}, workflows can be created to represent the 
individual steps in a dataflow, and allows the user to execute these steps 
selectively and view their output throughout different stages of their 
workflow~\cite{hid-sp18-504-pred-knime}. To add to this platform's flexibility,
KNIME also offers both data and tool blending to allow for various data sources
s (ranging from databases to images and networks) to be included in the same
workflow~\cite{hid-sp18-504-knime}. Since KNIME is based on both Java 
and Eclipse, there are an extensive number of plugins available for 
integration to enhance the analytic experience~\cite{hid-sp18-504-pred-knime}.


\begin{IU}

hid-sp18-504

\href{https://github.com/cloudmesh-community/hid-sp18-504/blob/master//technology/abstract-knime.tex}{abstract-knime.tex}

 

Wordcount: 123

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Orange}
\index{Orange}

Orange~\cite{hid-sp18-504-orange} is a powerful data mining, visualization, 
and machine learning toolkit. Attracting practitioners with varying degrees of
technical background (including complete novices)~\cite{hid-sp18-504-orange}, 
Orange's simple-to-use visual interface allows for the seamless application of 
complex data mining and machine learning concepts to derive insightful 
knowledge. Orange not only simplifies the application of in-depth data 
analysis, but it also helps bridge the gap between domain experts and 
data scientists by communicating the analytic processes clearly and 
effectively through visual representations~\cite{hid-sp18-504-orange}. 
This open source toolkit's latest version (3+) uses various python libraries 
for computation, while utilizing the Qt framework for the visualization 
end~\cite{hid-sp18-504-wiki-orange}.

\begin{IU}

hid-sp18-504

\href{https://github.com/cloudmesh-community/hid-sp18-504/blob/master//technology/abstract-orange.tex}{abstract-orange.tex}

 

Wordcount: 98

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{ESRI}
\index{ESRI}
\index{Environmental Systems Research Institute}

Environmental Systems Research Institute (ESRI) offers geospatial related data
services and process online through its proprietary API.  Features of the ESRI
platform include access to basemaps, geocoding, demographic data, a dynamic
world atlas, and multiple data sets in a open-data resource~\cite{hid-sp18-505-ESRI2018}.


\begin{IU}

hid-sp18-505

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-esri-data-services.tex}{abstract-esri-data-services.tex}

 

Wordcount: 42

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 89
\end{IU}

\section{GitHub Developer}
\index{Github!Developer}

GitHub is a software management platform that offers free and fee-based
services that can be used for managing source code for software projects.
However, GitHub also offers an API through its Developer site.  The API
can be used to analyze a large body of data that is stored at GitHub.  The
data can be used to provide insight into trending software technologies, data
sources that pertain to non-software management domains.  For example, the
GitHub resource OpenRefine~\cite{hid-sp18-505-OpenRefine2018} is a reference
to a variety of data sources that are open for public use~\cite{hid-sp18-505-GitHub2018}.


\begin{IU}

hid-sp18-505

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-github-developer.tex}{abstract-github-developer.tex}

 

Wordcount: 88

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 89
\end{IU}

\section{GraphQL}
\index{GraphQL}

Data services have been an important component in the evolution of the
information age.  In the early 2000s data-based web services relied
heavily on structured data formats like Extensible Markup
Language (XML).  Other data formats also included raw or plain text, or
serialized data objects.  Whether for public or private use, in most
cases there was and has been a necessity for documenting data
services.  In other words, web service consumers need to know what
data is available and how to query that data.  In the case of XML,
web-services were typically developed with
Web Services Description Language (WSDL)~\cite{hid-sp18-505-WSDL2018} as part
of the service.

Over time, challenging issues related to using XML as a data delivery format
emerged.  The JSON~\cite{hid-sp18-505-JSON2018} data format in
conjunction with the REST~\cite{hid-sp18-505-REST2018} architecture emerged
as an alternative to XML and SOAP.  However, REST also has challenges in terms
of documenting the services and data available in this type of REST
architecture.

\color{blue}``\emph{GraphQL has emerged as query language that can reside on top of the REST
architecture and address many of the issues associated with using XML/SOAP and
JSON/REST}''\color{black}~\cite{hid-sp18-505-GraphQL2018}.


\begin{IU}

hid-sp18-505

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-graphql.tex}{abstract-graphql.tex}

 

Wordcount: 171


Quote ratio: 13.22 \%
 
Max Line length: 92
\end{IU}

\section{MapBox}

MapBox is a geospatial data and location platform for multiple application
forms.  The MapBox platform provides services and features that can be used for
storing geospatial data, cartographic map production, and web-based map
interface development tools.  MapBox also provides a software-as-a-service
features that allow users to build base-maps that can be used through various
API tools in custom applications that might require maps or geospatial
data.  Although, a proprietary platform MapBox offers free services and access
for limited use\cite{hid-sp18-505-MapBox2018}.


\begin{IU}

hid-sp18-505

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-mapbox.tex}{abstract-mapbox.tex}

 

Wordcount: 75

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Netflix}
\index{Netflix}

Netflix offers a big-data platform that has both data and services that can be
used to access data as well as processing tools (algorithms) that can be used
to analyze data.  For example, Netflix's platform has tools that can be used to
detect outliers in large data sets~\cite{hid-sp18-505-Wong2015}.  Netflix's data sets can be
consumed and analyzed with their platform analytical tools or the tools can be
used independently on other non-Netflix data sets~\cite{hid-sp18-505-Netflix2018}.


\begin{IU}

hid-sp18-505

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-netflix.tex}{abstract-netflix.tex}

 

Wordcount: 70

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 92
\end{IU}

\section{US Consumer Financial Protection Bureau}

The United States Consumer Financial Protection Bureau (CFPB) Data and Research
organization is a consumer protection agency that was established after the
2008-2009 economic crisis.  The agency was established to help enforce consumer
protection laws and to help protect consumers against illegal financial risks.
The agency has established multiple data sources that are free and open to the
public for consumption and analysis\cite{hid-sp18-505-CFPB2018}.


\begin{IU}

hid-sp18-505

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-united-states-consumer-financial-protection-bureau.tex}{abstract-united-states-consumer-financial-protection-bureau.tex}

 

Wordcount: 63

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{The World Bank}

The world bank is a philanthropic organization that has two main goals.  First,
end extreme poverty and second, to promote shared prosperity.  One of the ways
the World Bank hopes to meet its goals is by developing and sharing an
open-data platform that can be used by the public.  The World Bank hopes the
open data platform can be used to promote knowledge that will ultimately help
with its goal of promoting prosperity and ending extreme poverty\cite{hid-sp18-505-Bank2018}.


\begin{IU}

hid-sp18-505

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-world-bank-open-data.tex}{abstract-world-bank-open-data.tex}

 

Wordcount: 77

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 93
\end{IU}

\section{Cascading}

Cascading is an open source data processing project started in early
2008. Cascading functions as a work flow workhorse within the Apache
Hadoop platform and serves as an alternative API to MapReduce. The
Cascading Ecosystems includes multiple project extensions for
compatibility with multiple languages, platforms, and
functions~\cite{hid-sp18-507-CascadingEco}. Originally written in
Java, the Cascading platform can be run on any JVM and includes
extensions for application development using Domain Specific Languages
(DSLs) such as Python, Ruby, Scala or
Clojure~\cite{hid-sp18-507-GitHubCascading}. Cascading and the
Cascading ecosystem were originally designed to be used with the
Apache Hadoop MapR distribution for the purpose of developing
data-rich applications with analysis and machine learning
capabilities~\cite{hid-sp18-507-MapR}.The open source platform an all
extensions are available through the Apache Public License.


\begin{IU}

hid-sp18-507

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-507/blob/master//technology/abstract-cascading.tex}{abstract-cascading.tex}

 

Wordcount: 107

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Elasticsearch}

The central engine to the Elastic product line, Elasticsearch is a
distributed, RESTful search engine designed to grow with growing
data. Elastic search is a search engine based on Apache's Lucene
search library with the first version being released in early
2010~\cite{hid-sp18-507-ElasticWiki}. Elasticsearch is capable of
searching and storing multiple data types, including numeric data,
text, geo, and varying levels of structured data using s schema-free
JSON format. In adition to the ability to search in real-time,
Elasticsearch is capable of analyzing queried results. Elasticsearch's
use is compatible with multiple languages such as Curl, Java, Python,
C-Sharp, PHP, Perl, JavaScript, and
more~\cite{hid-sp18-507-Elasticsearch}. DB-Engines, a \color{blue}``\emph{Knowledge Base
of Relational and NoSQL Database Management Systems}''\color{black} ranks
Elasticsearch as the top search engine, ahead of both Splunk and
Solr~\cite{hid-sp18-507-DBEngines}.


\begin{IU}

hid-sp18-507

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-507/blob/master//technology/abstract-elasticsearch.tex}{abstract-elasticsearch.tex}

 

Wordcount: 112

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 7.98 \%
 
Max Line length: 88
\end{IU}

\section{JMP}

JMP, commonly referred to as \color{blue}``\emph{Jump}''\color{black}, is an enterprise level
statistical analysis tool developed by SAS. The JMP software package
is designed to handle every data-involved stage from the initial
acquisition of data to the final presentation of findings. JMP was
first released in 1989 and has been designed ever since to provide a
visual-centric interface where the user can analyze, manipulate, and
format data. JMP is capable of complex analysis and machine learning
techniques and can provide the user the back-end software code
generated to produce the visualized results in a variety of common
statistical languages or applications such as Python, R, Matlab, SAS,
and others~\cite{hid-sp18-507-JMP9}. A single JMP license is available
for 1,785 USD~\cite{hid-sp18-507-JMPSAS}. JMP Pro is an even more
capable version of JMP with more advanced analytics and predictive
modeling with cross-validation--available for 14,900
USD~\cite{hid-sp18-507-JMPPro}.


\begin{IU}

hid-sp18-507

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-507/blob/master//technology/abstract-jmp.tex}{abstract-jmp.tex}

 

Wordcount: 124

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 1.69 \%
 
Max Line length: 93
\end{IU}

\section{Openchain}

Openchain is a blockchain ledger technology designed to be built in
seconds and deployed on an enterprise scale for the purposes of
managing asset transactions. The Openchain technology uses a
distributed client-server architecture rather than the slower
peer-to-peer proof of work concept originally adopted by early
blockchain technologies, most notably: Bitcoin. Openchain can serve as
a stand alone ledger or can be scaled as a side-ledger to existing
blockchain platforms~\cite{hid-sp18-507-Openchain}. Openchain's parent
company, Coinprism, has proven technology projects adopted by multiple
large-scale companies like the Open Asset technology used by
NASDAQ~\cite{hid-sp18-507-BitcoinNews}. Openchain is capable of
handling any type of digital asset such as gift cards, legal
documents, contracts, coins or tokens, or client-specific asset data
and this is done through the enacting company serving as its own
administrator by creating and enforcing it's own trust-approver
validation hierarchy~\cite{hid-sp18-507-Coindesk}.


\begin{IU}

hid-sp18-507

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-507/blob/master//technology/abstract-openchain.tex}{abstract-openchain.tex}

 

Wordcount: 122

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Scribe}

Scribe is a server design, originally developed and maintained by
Facebook in 2008, that serves as an aggregation service for streaming
log data. The Scribe server is deployed within each node of a network
and sends the aggregated log data to a central server for
analysis. The data is interpreted by the Scribe servers via a
two-string input by a client: the category or direction, and the
message itself. Scribe has been deployed on thousands of servers on a
single network and is robust to network errors and
failures~\cite{hid-sp18-507-FBScribe}. Scribe was developed by
Facebook to prevent their highly distributed server architecture being
locked into a third party's network topology. The purpose of Scribe is
to solve two major needs of a distributed data system: capturing
events, changes, and errors on the system, and maintaining the
collected and aggregated data through issues common to decentralized
networks such as connection breaks, server downtime, and
scalability~\cite{hid-sp18-507-ScribeNote}. The logging-functionality
of Scribe is now maintained and improved upon through the open source
community. Scribe is available via the Apache License v.2.


\begin{IU}

hid-sp18-507

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-507/blob/master//technology/abstract-scribe.tex}{abstract-scribe.tex}

 

Wordcount: 159


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Microsoft Cognitive Toolkit}
\index{Microsoft Cognitive Toolkit}

The Microsoft Cognitive Toolkit (CNTK) is an open-source project that
can be used for implementing distributed deep learning commercially.
Per Microsoft, \color{blue}``\emph{CNTK allows the user to easily realize and combine
popular model types such as feed-forward DNNs, convolutional neural
networks (CNNs) and recurrent neural networks (RNNs/LSTMs)}''\color{black}. Under the
covers, CNTK implements stochastic gradient descent that are
automatically parallelized across multiple GPUs and
servers~\cite{hid-sp18-510-web-cntk}. As of this date, CNTK can be run
on both Windows and Linux operating systems. \color{blue}``\emph{CNTK also supports the
description of neural networks via C++, Network Definition
Language (NDL) and other descriptive languages such as Python and
C\#}''\color{black}~\cite{hid-sp18-510-kd-cntk}.


\begin{IU}

hid-sp18-510

\href{https://github.com/cloudmesh-community/hid-sp18-510/blob/master//technology/abstract-cntk.tex}{abstract-cntk.tex}

 

Wordcount: 94

ERROR: This abstract is too short.


Quote ratio: 39.33 \%

WARNING: Quote ratio very high
 
Max Line length: 87
\end{IU}

\section{Kubernetes}
\index{Kubernetes}

Kubernetes is an open-source platform designed to automate deploying,
scaling, and operating application containers. \color{blue}``\emph{The name Kubernetes
originates from Greek, meaning helmsman or pilot, and is the root of
governor and cybernetic}''\color{black}~\cite{hid-sp18-510-web-Kubernetes}.
Kubernetes is capable of scheduling and running application containers
on both physical or virtual clusters. Kubernetes allows developers to
design applications that are agnostic of underlying architecture and
allows developers to design applications based on a container-centric
infrastructure rather than host-centric infrastructure, utilizing the
full advantages and benefits of
containers~\cite{hid-sp18-510-med-Kubernetes}. Kubernetes via its
building blocks (primitives) provides mechanism for easily deploying,
maintaining, and scaling applications. Underlying architecture of
Kubernetes is meant to be loosely coupled and extensible so that it
can be used across a wide variety of
workloads~\cite{hid-sp18-510-wiki-Kubernetes}.


\begin{IU}

hid-sp18-510

\href{https://github.com/cloudmesh-community/hid-sp18-510/blob/master//technology/abstract-kubernetes.tex}{abstract-kubernetes.tex}

 

Wordcount: 106

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 11.58 \%
 
Max Line length: 86
\end{IU}

\section{Apache Mahout}
\index{Mahout}
\index{Apache!Mahout}


Apache Mahout, an Apache Software Foundation project, is a distributed
Scala DSL based linear algebra framework designed to aid
mathematicians, statisticians and data scientists in implementing
their own algorithms quickly and
efficiently~\cite{hid-sp18-510-web-Mahout}. Initiated based on Andrew
Ng et al.'s paper \color{blue}``\emph{Map-Reduce for Machine Learning on
Multicore}''\color{black}~\cite{hid-sp18-510-ng-Mahout}, it has evolved over time to
cover other general machine-learning
approaches~\cite{hid-sp18-510-ibm-Mahout}. While Apache Spark is
recommended back end, where core algorithms are implemented on top of
Apache Hadoop, Mahout is also extensible to other back ends and
standalone implementations~\cite{hid-sp18-510-wiki-Mahout}. While
number of algorithms supported by Apache Mahout is increasing, its
core algorithms primarily contain implementations for clustering,
classification, and Collaborative
filtering~\cite{hid-sp18-510-wiki-Mahout}.


\begin{IU}

hid-sp18-510

\href{https://github.com/cloudmesh-community/hid-sp18-510/blob/master//technology/abstract-mahout.tex}{abstract-mahout.tex}

 

Wordcount: 93

ERROR: This abstract is too short.


Quote ratio: 5.53 \%
 
Max Line length: 84
\end{IU}

\section{Apache PredictionIO}
\index{Apache!PredictionIO}
\index{PredictionIO}

Apache PredictionIO is an open source machine learning stack for
building, evaluating and deploying engines with machine learning
algorithms. An open source Machine Learning Server built on top of an
open source stack allows developers and data scientists to create
predictive engines for any machine learning task. It allows developers
to quickly build and deploy an engine as a web service and unify data
from multiple platforms in batch or in real-time for comprehensive
predictive analytics. It supports machine learning and data processing
libraries such as Spark MLLib and
OpenNLP~\cite{hid-sp18-510-web-PredIO}.


\begin{IU}

hid-sp18-510

\href{https://github.com/cloudmesh-community/hid-sp18-510/blob/master//technology/abstract-predictionio.tex}{abstract-predictionio.tex}

 

Wordcount: 82

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Theano}
\index{Theano}

Theano is a numerical computation library built for Python programming
language. Theano uses Numpy syntax for expressing computations that
can be compiled to run efficiently on both CPU and GPU
architectures. Theano is an open source project and most of the
development is primarily contributed by a machine learning group at
the Université de Montréal~\cite{hid-sp18-510-wiki-theano}. Theano is
a Python library that allows developers to efficiently define,
optimize, and evaluate mathematical expressions including
multi-dimensional arrays. Some of the features of Theano include tight
integration with NumPy, transparent use of a GPU to perform data
intensive computations, dynamic C code generation for evaluating
expressions faster and support for extensive unit testing and
self-verification~\cite{hid-sp18-510-web-theano}.


\begin{IU}

hid-sp18-510

\href{https://github.com/cloudmesh-community/hid-sp18-510/blob/master//technology/abstract-theano.tex}{abstract-theano.tex}

 

Wordcount: 99

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Apttus}
\index{CRM}
\index{AI}
\index{Quote-to-cash}

Apttus provides various products for Customer Relationship Management
s (CRM) and use artificial intelligence (AI) to maximize the customer
revenue. These products help customer to automate the process and
maximize the revenue with artificial intelligence (AI). Apttus
products include Enterprise Contract Management, Quote-to-Cash,
Configure Price Quote, Business to Business (B2B) E-Commerce, Buy-Side
Contract Management and Revenue Management. Apttus Quote-to-Cash
artificial intelligent product used for automating the end to end
process of customer's intent to buy the product and ultimately
customer buying the product. Quote-to-Cash artificial intelligent
product take care of all the steps involved in the Quote-to-cash
process and provide full automation of customer relationship life
cycle. These steps requires very less human intervention. This product
helps in increasing the revenue of the customer as Quote-to-Cash is
heart of the business. Apttus provide both on premise and cloud
products~\cite{hid-sp18-511-apttus}.
 


\begin{IU}

hid-sp18-511

\href{https://github.com/cloudmesh-community/hid-sp18-511/blob/master//technology/abstract-apttus.tex}{abstract-apttus.tex}

 

Wordcount: 125

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{IICS}
\index{IICS}

Informatica provides various products in data integration and data
warehousing domain. Informatica provide on premise products for Big
Data, Data Integration, Data Quality, Data Security and Master Data
Management. Informatica also provide cloud products for Integration
Cloud, Data Quality, Governance Cloud, Master Data Management Cloud,
Data Security Cloud and Data As A Service. All the cloud products
comes under Informatica Integration Cloud Services (IICS).
with the cloud approach customer need not to worry about the patching,
high availability of the servers, upgrade etc.\ IICS is
built on micros services architecture and modern user interfaces and
provide complete end-to-end data management approach. IICS
provide New and Modern User Interface Experience, Template Driven
Development, Enterprise Orchestration, File Mass Ingestion, Integrated
Asset Management and APIs that enable Continuous Delivery. Customer
can focus on the logic of the data processing and all the
infrastructure related activities will be taken care by
cloud~\cite{hid-sp18-511-iics}.
 
 


\begin{IU}

hid-sp18-511

\href{https://github.com/cloudmesh-community/hid-sp18-511/blob/master//technology/abstract-iics.tex}{abstract-iics.tex}

 

Wordcount: 134


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Linode}
\index{Two-Factor Authentication}
\index{Scaling}
\index{DNS Manager}
\index{Cloning}
\index{Linode Manager}

Linode is a cloud service provider. It provide compute, storage and
networking services on demand. Linode provide SSD Storage which is
Industry-leading native SSDs for optimal performance, 40Gbit Network
which is 40Gbps throughput with multiple levels of redundancy and
Intel E5 Processors which are The fastest processors in the cloud
market. Linode has three regions and 9 data centers across the world.
These regions and data centers help in data recovery and fault
tolerance in case of failures. Customers use Linode services on demand
as infrastructure for their websites, web services and applications.
Linode provide Two-Factor Authentication\index{Two-Factor
  Authentication}, IPv6 Support, Rescue Mode, DNS Manager\index{DNS
  Manager}, Scaling\index{Scaling}, Cloning\index{Cloning} of the
configuration, Supported Distributions for various distribution
images. The Linode easy interface Linode Manager\index{Linode Manager}
allows to deploy, boot, resize and clone in just a few clicks. Linode
offer different billing plans and customer can select the plan as per
their requirement~\cite{hid-sp18-511-linode}.


\begin{IU}

hid-sp18-511

\href{https://github.com/cloudmesh-community/hid-sp18-511/blob/master//technology/abstract-linode.tex}{abstract-linode.tex}

 

Wordcount: 141


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{RapidMiner}
\index{Machine Learning}

RapidMiner is a data science software platform that provides an
integrated environment for data preparation, machine
learning\index{Machine Learning} and model deployment into production.
It provide the facility to connect to various sources of different
format and different scale to collect the data, data exploration to
discover the pattern in the data, identity the issues, blending to
find the relevant data for the modeling, clean the data for advanced
algorithms. Once the data is prepared models are built using a nice
workflow designer tool. It provides Classification, regression and
clustering techniques. Validation is performed for cross validation
and split validation after the model development for the accuracy of
the model. On the successful validation of the model it is deployed
into production with governance and security. It is Open and
extensible and provide native R and Python support. The source code is
available under an aGPL license. The platform provides an easy drag
and drop environment for easy and fast data science related
operations~\cite{hid-sp18-511-rapidminer}.


\begin{IU}

hid-sp18-511

\href{https://github.com/cloudmesh-community/hid-sp18-511/blob/master//technology/abstract-rapidminer.tex}{abstract-rapidminer.tex}

 

Wordcount: 148


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{TreasureData}
\index{data cleaning}

Treasure Data provide a platform to consolidate the customer data from
different sources, integrate them and see actionable customer view.
This is helpful for Marketing Analytics, Sales Operations Analytics.
This helps in understanding customer behavior easily and fast and take
the actions accordingly for business benefit. We need to access
various level of historical and real time data to get 360 degree view
of data. Treasure Data enables the connection to various sources
easily and reduce the data cleaning\index{data cleaning} process as it
has it's own in built tool for cleaning the data. Treasure Data work
on structured and semi structured data. It has almost 100 connectors
to connect to various sources including social media, pull the data in
real time and maintain a single view of the customer using advanced
machine learning technologies. Data is available within short time (in
minutes) for the actions and business
decision~\cite{hid-sp18-511-treasuredata}.


\begin{IU}

hid-sp18-511

\href{https://github.com/cloudmesh-community/hid-sp18-511/blob/master//technology/abstract-treasuredata.tex}{abstract-treasuredata.tex}

 

Wordcount: 135


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

ERROR: Illegal quotes in the file skipping inclusion. Please fix the folllowing file:

\begin{IU}

hid-sp18-512

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-aurora.tex}{abstract-amazon-aurora.tex}

 

Wordcount: 84

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 577
\end{IU}

ERROR: Illegal quotes in the file skipping inclusion. Please fix the folllowing file:

\begin{IU}

hid-sp18-512

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-cloudfront.tex}{abstract-amazon-cloudfront.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 2 in ../hid-sp18-512/technology/abstract-amazon-cloudfront.tex line 1:
Non-breaking space (`~') should have been used. A Content Delivery Network is a
“globally distributed network of webservers” \cite{hid-sp18-512-amazon-
cloudfront-b} over the internet at different geographical locations. They form a
huge part of internet services today and are deployed at different locations to
ensure faster content load times, and lower bandwidths over a network. This
technology is highly “useful to companies that require higher response
times”\cite{hid-sp18-512-amazon-cloudfront-b} and distribution of large files to
many users at a given time. It helps accelerate delivery by moving the content
close to the end-user therefore reducing hops through the internet. This is
often done through caching the content inside a server that is closer to the
user. With this, network performance is accelerated, including global presence,
and smart computing. Amazon CloudFront is a Content Delivery Network (CDN) that
is integrated in Amazons AWS service. It is one of the largest in the world,
including others such as Akamai, MaxCDN, and Rackspac
^
\end{verbatim}
\end{tiny}

Wordcount: 181


Quote ratio: 0.00 \%
 
Max Line length: 1265
\end{IU}

\section{Amazon DynamoDB}

NoSQL refers to a non-relational database the provides high performance and using 
various data models such as document, key-value, graph, and columnar.  Compared 
to Non-relational databases, the do not often enforce the use of a schema. 
DynamoDB is type of NoSQL database provided by Amazon. It is a cloud based 
database that is full managed and capable of supporting both key-value and 
document based models. It comes~\cite{hid-sp18-512-amazon-dynamodb} with a 
very flexible model and through-put capacity that makes it great for various 
devices and applications such as those suitable for gaming, IoT et cetera. 
With DynamoDB~\cite{hid-sp18-512-amazon-dynamodb_faq}, customers don’t have 
to worry about the burdens of operating distributed services such as hardware 
setup, configurations and software patches.



\begin{IU}

hid-sp18-512

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-dynamodb.tex}{abstract-amazon-dynamodb.tex}

 

Wordcount: 117

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 82
\end{IU}

ERROR: Illegal quotes in the file skipping inclusion. Please fix the folllowing file:

\begin{IU}

hid-sp18-512

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-vpc.tex}{abstract-amazon-vpc.tex}

 

Wordcount: 159


Quote ratio: 0.00 \%
 
Max Line length: 1082
\end{IU}

ERROR: Illegal quotes in the file skipping inclusion. Please fix the folllowing file:

\begin{IU}

hid-sp18-512

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazonml.tex}{abstract-amazonml.tex}

 

Wordcount: 158


Quote ratio: 0.00 \%
 
Max Line length: 1038
\end{IU}

\section{datameer}

Datameer is self service, schema free Big Data Analytics tool which provides 
end to end analytics. Datameer, a data analytics application purpose-built for 
Hadoop. It offers big data integration, analytics, visualization, smart 
execution, technology, app market, cloud, and smart analytics products. 
Datameer provides Smart Execution, which selects and combines computation 
frameworks for Datameer workloads. It also offers Smart Analytics, which 
provides advanced functions, including clustering, decision trees, column 
dependencies, and recommendations to find groups and relationships. Datameer is
also used in cleansing the data that was ingested or as it is being ingested, 
and then it has the ability to query the data using Hive/Spark and provide 
visualization for the queried data. Datameer scales up to thousands of nodes
and is available for all major Hadoop distributions ~\cite{hid-sp18-513-datameer}.
Datameer specializes in analysis of large volumes of data. Datameer provides 
end to end solution from data extraction, profiling, cleansing, transforming,
merging, securing and finally visualization. 


\begin{IU}

hid-sp18-513

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-513/blob/master//technology/abstract-datameer.tex}{abstract-datameer.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 39 in ../hid-sp18-513/technology/abstract-datameer.tex line 14: Double
space found. and is available for all major Hadoop distributions ~\cite{hid-
sp18-513-datameer}.                                                      ^
\end{verbatim}
\end{tiny}

Wordcount: 151


Quote ratio: 0.00 \%
 
Max Line length: 82
\end{IU}

\section{docker}
Docker is an open platform for developers and sysadmins to build, ship, 
and run distributed applications, whether on laptops, data center VMs, 
or the cloud ~\cite{hid-sp18-513-docker}. It is designed to make it easier to
create, deploy, and run applications by using containers. Containers allow a 
developer to package up an application with all of the parts it needs, 
such as libraries and other dependencies, and ship it all out as one package. 
Docker is like a virtual machine but it does not require to create a whole 
virtual operating system, Docker allows applications to use the same Linux 
kernel as the system that they're running on and only requires applications 
be shipped with things not already running on the host computer. This gives 
a significant performance boost and reduces the size of the application 
~\cite{hid-sp18-513-opensource}.



\begin{IU}

hid-sp18-513

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-513/blob/master//technology/abstract-docker.tex}{abstract-docker.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 39 in ../hid-sp18-513/technology/abstract-docker.tex line 4: Double
space found. or the cloud ~\cite{hid-sp18-513-docker}. It is designed to make it
easier to               ^
\end{verbatim}
\end{tiny}

Wordcount: 134


Quote ratio: 0.00 \%
 
Max Line length: 78
\end{IU}

\section{neo4j}

Neo4j is a graph database developed by Neo4j, Inc. It is an
ACID-compliant transactional database with graph storage and
processing which in turn help data scientists to gain new perspectives
on data. Neo4j's Graph Platform is specifically optimized to map,
analyze, store and traverse networks of connected data to reveal
invisible contexts and hidden relationships to help enterprises tackle
challenges such as Artificial Intelligence and Machine Learning, Fraud
Detection, Master Data Management ~\cite{hid-sp18-513-neo4j}. Neo4j is one 
of the popular Graph Databases and Cypher Query Language (CQL). 
Neo4j is written in Java Language. Neo4j provides a flexible simple and yet 
powerful data model, which can be easily changed according to the applications 
and industries. Neo4j provides results based on real-time data and it is highly
available for large enterprise real-time applications and also it does not 
require complex joins to retrieve the data. Neo4j can connect to REST API to 
work with programming languages such as Java, Spring, Scala etc.


\begin{IU}

hid-sp18-513

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-513/blob/master//technology/abstract-neo4j.tex}{abstract-neo4j.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 39 in ../hid-sp18-513/technology/abstract-neo4j.tex line 10: Double
space found. Detection, Master Data Management ~\cite{hid-sp18-513-neo4j}. Neo4j
is one                                     ^
\end{verbatim}
\end{tiny}

Wordcount: 151


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Oozie}
Apache Oozie is a workflow scheduler to manage Hadoop jobs. Workflows are 
defined as a collection of control flow and action nodes in a directed 
acyclic graph. Oozie jobs can either be triggered based on the frequency 
or based on when the data becomes available. Oozie is integrated with the 
Hadoop stack supporting several types of Hadoop jobs such as map-reduce, Pig,
Hive, Sqoop, Java programs and shell scripts. Oozie is an Open Source Java 
Web-Application available under Apache license 2.0. Oozie is a scalable, 
reliable and extensible system. It is responsible for triggering the workflow 
actions, which in turn uses the Hadoop execution engine to actually execute 
the task. Hence,Oozie is able to leverage the existing Hadoop machinery for
load balancing, fail-over, etc ~\cite{hid-sp18-513-oozie}.


\begin{IU}

hid-sp18-513

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-513/blob/master//technology/abstract-oozie.tex}{abstract-oozie.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 39 in ../hid-sp18-513/technology/abstract-oozie.tex line 12: Double
space found. load balancing, fail-over, etc ~\cite{hid-sp18-513-oozie}.
^
\end{verbatim}
\end{tiny}

Wordcount: 123

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 78
\end{IU}

\section{tableau}

Tableau is the data visualization software that helps people to see and 
understand the data. Tableau can connect to almost any database, drag and drop
to create visualizations, and share with a click. We can either schedule to get
data refreshed or have real time updates with live connection.  We can explore
data from any sources from spreadsheets to databases to Hadoop to cloud 
services in minutes and  dashboard can be published live on the web and on
mobile devices ~\cite{hid-sp18-513-tableau}. Tableau is a Business Intelligence 
tool for visually analyzing the data. Users can create and distribute an 
interactive and shareable dashboard, which depict the trends, variations, and
density of the data in the form of graphs and charts. Tableau can connect to 
files, relational and Big Data sources to acquire and process data. It also
allows data blending and real-time collaboration, which makes it very unique. 
Tableau is also positioned as a leader Business Intelligence and Analytics 
Platform in Gartner Magic Quadrant.


\begin{IU}

hid-sp18-513

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-513/blob/master//technology/abstract-tableau.tex}{abstract-tableau.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 39 in ../hid-sp18-513/technology/abstract-tableau.tex line 9: Double
space found. mobile devices ~\cite{hid-sp18-513-tableau}. Tableau is a Business
Intelligence                  ^
\end{verbatim}
\end{tiny}

Wordcount: 159


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Apache Curator}
\index{Curator}

Apache Curator is a collection of \color{blue}``\emph{Java/JVM based client libraries for
Apache Zookeeper, a centralized distributed
service}''\color{black}\cite{hid-sp18-514-apachecurator}.  \color{blue}``\emph{It includes a high level
API framework and utilities to make using Apache Zookeeper much easier
and more reliable.  It also includes recipes for common use cases and
extensions such as service discovery and a Java 8 asynchronous
DSL}''\color{black}\cite{hid-sp18-514-apachecurator}.  The Curator framework consists
of set of API’s that prominently streamline using Zookeeper.  This
framework adds various features \color{blue}``\emph{that build on Zookeeper and handles
the complexity of managing connections to the Zookeeper cluster and
retrying operations.  Some of the features are: Automatic connection
management, Cleaner API, Recipe
implementations}''\color{black}\cite{hid-sp18-514-apachecuratorfeatures}


\begin{IU}

hid-sp18-514

\href{https://github.com/cloudmesh-community/hid-sp18-514/blob/master//technology/abstract-apachecurator.tex}{abstract-apachecurator.tex}

 

Wordcount: 98

ERROR: This abstract is too short.


Quote ratio: 59.57 \%

ERROR: Quote ratio too high
 
Max Line length: 103
\end{IU}

\section{Apache Geode}
\index{Geode}

Apache Geode is an in-memory distributed \color{blue}``\emph{data management platform
that provides real-time, consistent access to data-intensive
application through extensively distributed cloud
architectures}''\color{black}~\cite{hid-sp18-514-apachegeodewiki}. Apache Geode
initially built by GemStone Systems. It was named as GemFire, 

\color{blue}``\emph{it was
first installed in the financial sector as the transactional,
low-latency data engine used in Wall Street tradingplatforms}''\color{black}~\cite{hid-sp18-514-apachegeodewiki}.
Distributed cache servers are generalization that define the nodes. In
each cache we define regions, regions are equivalent to tables in any
relational databases or XSD schema structure and manage data in the
distributed environment. For high availability the data is replicated
to multiple regions (same data is available on each cache servers) by
which it ensures high availability as one member goes down still copy
is available on other cache member. Locater’s responsibility to
determine and load balance client (MapReduce, JTA, spring, REST service
call, or API) requests to be processed by available cache servers.
Locators get notifications continuously if there is any issue in the
cluster members, based on this the client request will be navigated
appropriately~\cite{hid-sp18-514-apachegeodewiki}.The main features of
this framework are high performance, scalability, fault-tolerance for
any data grid platform and can be integrated to other open sources
technologies – Spring Data
Gemfire~\cite{hid-sp18-514-geodespringgemfire}, Spring
Cache~\cite{hid-sp18-514-geodespringcache}, and
Python~\cite{hid-sp18-514-geodepython}.


\begin{IU}

hid-sp18-514

\href{https://github.com/cloudmesh-community/hid-sp18-514/blob/master//technology/abstract-apachegeode.tex}{abstract-apachegeode.tex}

 

Wordcount: 181


Quote ratio: 17.68 \%
 
Max Line length: 113
\end{IU}

ERROR: Illegal quotes in the file skipping inclusion. Please fix the folllowing file:

\begin{IU}

hid-sp18-514

\href{https://github.com/cloudmesh-community/hid-sp18-514/blob/master//technology/abstract-apachekaraf.tex}{abstract-apachekaraf.tex}

 

Wordcount: 97

ERROR: This abstract is too short.


Quote ratio: 11.07 \%
 
Max Line length: 86
\end{IU}

\section{Oracle Coherence - DataGrid}
\index{Coherence}

Oracle Coherence In-Memory Data Grid is a data management platform
\color{blue}``\emph{for application objects that are shared across multiple servers,
require low response time, very high throughput, predictable
scalability, continuous availability and information
reliability}''\color{black}\cite~{hid-sp18-514-OracleCoherence}. Initially this
framework was built by Tangosol Inc. \color{blue}``\emph{The Tangosol Inc., was acquired
by Oracle Corporation in 2007}''\color{black}~\cite{hid-sp18-514-coherencewiki} and
named that framework as Oracle Coherence.Oracle coherence is best
suitable for computational intensive, stateful middle-ware
applications that runs in a distributed platform. Coherence is
directed to run application layer, and is often run in-memory with
application itself. \color{blue}``\emph{Data Grid is a system of composed of multiple
servers that work to manage information and related operations.
Coherence provides the ideal infrastructure for building Data Grid
services, and the client and server-based applications that use a Data
Grid. At a basic level, Coherence can manage an immense amount of data
across a large number of servers in a grid; it can provide close to
zero latency access for that data; it supports parallel queries across
that data; and it supports integration with database and EIS systems
that act as the system of record for that data. Additionally,
Coherence provides several services that are ideal for building
effective data grids}''\color{black}~\cite{hid-sp18-514-OracleCoherence}. This
framework comes by default with Oracle WebLogic 12c server. Oracle
also provides standalone coherence server, which can be used in any
Big Data environment to store any database data, inflight or any
dataset to be processed by MapReduce or any other Hadoop component.
Cache servers (JVM’s) can be configured in the clusters. This framework
can be used in the Big Data environment without HDFS for in-memory
distributed data storage. There are DOT NET, JAVA, C++ and other API’s
available along with REST service to access Coherence
cache\cite{hid-sp18-514-OracleCoherence}. Oracle coherence can be
integrated from Spring based distributed applications as
well\cite{hid-sp18-514-CoherenceSpringInt}.


\begin{IU}

hid-sp18-514

\href{https://github.com/cloudmesh-community/hid-sp18-514/blob/master//technology/abstract-oraclecoherence.tex}{abstract-oraclecoherence.tex}

 

Wordcount: 270


Quote ratio: 41.83 \%

WARNING: Quote ratio very high
 
Max Line length: 88
\end{IU}

\section{Tibco DataSynapse GridServer}
\index{DSGridServer}

\color{blue}``\emph{DataSynapse was founded by two ex-investment bankers with an idea to
speed up calculations by running them in parallel, distributed over
multiple machines. The first product live cluster was released in
2001. In 2004 this product was renamed to GridServer. Gridserver was
developed to support larger and larger grid of network
computers}''\color{black}~\cite{hid-sp18-514-datasynapsewiki}. DataSyanpse was
acquired by Tibco in 2009 and later this product was renamed as: Tibco
DataSynapse GridServer\cite{hid-sp18-514-tibcodatasynapsewiki}.
\color{blue}``\emph{GridServer is a highly scalable software infrastructure that allows
application services to operate in a virtualized fashion, unattached
to specific hardware resources. Client applications submit requests to
the Grid environment and GridServer dynamically provisions services to
respond to the request. Multiple client applications can submit
multiple requests in parallel and GridServer dynamically creates
multiple service instances to handle requests in parallel on different
Grid nodes. This architecture is therefore highly scalable in both
speed and throughput. For example, a single client will see scalable
performance gains in the processing of multiple requests, and many
applications and users will see scalable throughput of the aggregate
load}''\color{black}~\cite{hid-sp18-514-tibcods}. Data Synapse grid server has the
capabilities of compute grid and data grid. The main components of the
grid server are Engines, Directors and Brokers. All these components
are JVM’s built in Java. Each component has their own
responsibilities. The applications are deployed in the engines and
computation and processing is also done in the engines. Engines are
light weight containers. Directors receive the client requests and
then navigate to the broker. Broker act as a load balancer to navigate
the request to the available engines in the grid environment based on
engine load, availability and etc.\ As there are multiple nodes in the
grid, there will be primary and secondary director, broker, and
several engines to support high availability and fault
tolerance\cite{hid-sp18-514-tibcods}.



\begin{IU}

hid-sp18-514

\href{https://github.com/cloudmesh-community/hid-sp18-514/blob/master//technology/abstract-tibcodatasynapsegridserver.tex}{abstract-tibcodatasynapsegridserver.tex}

 

Wordcount: 268


Quote ratio: 49.82 \%

WARNING: Quote ratio very high
 
Max Line length: 88
\end{IU}

\section{CouchDB}
\index{CouchDB}

CouchDB\cite{hid-sp18-515-www-couchdb} is a database designed for web, 
which use JSON as the file format to store data. You can use web 
browser to get access to the documents via HTTP. You can use JavaScript 
to query, combine, and transform your documents. CouchDB is suitable 
to work with modern web and mobile apps. CouchDB’s incremental replication 
helps you distribute your data efficiently. You can setup the CouchDB 
as master-master with automatic conflict detection. CouchDB makes 
web development a breeze because its suite of features, such as 
on-the-fly document transformation and real-time change notifications. 
It even helps use web easily with the administration console, which 
is served directly out of CouchDB. CouchDB is easy to be distributed 
scaling, because it's highly available and partition tolerant, but 
is also eventually consistent. CouchDB puts your data safely with 
the fault-tolerant storage engine.



\begin{IU}

hid-sp18-515

\href{https://github.com/cloudmesh-community/hid-sp18-515/blob/master//technology/abstract-CouchDB.tex}{abstract-CouchDB.tex}

 

Wordcount: 138


Quote ratio: 0.00 \%
 
Max Line length: 75
\end{IU}

ERROR: Illegal quotes in the file skipping inclusion. Please fix the folllowing file:

\begin{IU}

hid-sp18-515

\href{https://github.com/cloudmesh-community/hid-sp18-515/blob/master//technology/abstract-HBase.tex}{abstract-HBase.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 18 in ../hid-sp18-515/technology/abstract-HBase.tex line 10: Use either
`` or '' as an alternative to `"'. Apache HBase is modeled after Google's
Bigtable: "A Distributed Storage
^ Warning 18 in ../hid-sp18-515/technology/abstract-HBase.tex line 11: Use
either `` or '' as an alternative to `"'. System for Structured Data by Chang et
al."                                             ^
\end{verbatim}
\end{tiny}

Wordcount: 97

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 76
\end{IU}

\section{Apache Hadoop}
\index{Apache Hadoop}

The Apache Hadoop~\cite{hid-sp18-515-www-hadoop} is an open-source 
software designed for reliable, scalable, distributed computing. 
The Apache Hadoop software library is a framework using simple programming 
models that allows allows for the distributed processing of large data sets across clusters of computers.
It is designed to scale up from single servers to thousands of machines, 
each offering local computation and storage.
The library is designed to detect and handle failures at the application 
layer rather than rely on hardware to deliver high-availability.
Therefore, each of the computers in the cluster may be prone to failures 
because it delivers a highly-available service on top of a cluster of computers. 
The project includes these modules:
1. Hadoop Common:
The common utilities that support the other Hadoop modules.
2. Hadoop Distributed File System (HDFS):
A distributed file system that provides high-throughput access to 
application data.
3. Hadoop YARN:
A framework for job scheduling and cluster resource management. 
4. Hadoop MapReduce: 
A YARN-based system for parallel processing of large data sets.


\begin{IU}

hid-sp18-515

\href{https://github.com/cloudmesh-community/hid-sp18-515/blob/master//technology/abstract-Hadoop.tex}{abstract-Hadoop.tex}

 

Wordcount: 158


Quote ratio: 0.00 \%
 
Max Line length: 105
\end{IU}

\section{Oracle Nosql Database}
\index{Oracle Nosql Database}

Oracle NoSQL Database\cite{hid-sp18-515-www-oraclenosql} is a scalable, 
distributed NoSQL database, designed to provide highly reliable, flexible 
and available data management across a configurable set of storage nodes.
Data in Oracle NoSQL Database can be modeled as both relational-database-style 
tables, JSON documents or key-value pairs.
Based on the hashed value of the primary key, Oracle NoSQL Database is 
a sharded s (shared-nothing) system which distributes the data uniformly 
across the multiple shards in the cluster.
Storage nodes are replicated to ensure high availability, rapid failover 
in the event of a node failure and optimal load balancing of queries
within each shard. 
NoSQL Database provides Java, C, Python and Node.js drivers and a 
REST API to simplify application development.
A wide variety of related Oracle and open source applications are 
integrated in Oracle NoSQL Database, in order to simplify and streamline 
the development and deployment of modern big data applications.
Oracle NoSQL Database is available in the following editions: 
Enterprise Edition - Oracle Commercial License
Basic Edition * - Oracle Database Enterprise Edition Commercial 
License
Community Edition - Open source license.


\begin{IU}

hid-sp18-515

\href{https://github.com/cloudmesh-community/hid-sp18-515/blob/master//technology/abstract-OracleNosqlDB.tex}{abstract-OracleNosqlDB.tex}

 

Wordcount: 174


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Ranger}
\index{Ranger}

Apache Ranger\cite{hid-sp18-515-www-ranger} is a framework to enable, 
monitor and manage comprehensive data security across the Hadoop 
platform.
In order to provide comprehensive security across the Apapche Hadoop 
ecosystem, the vision with Ranger was designed.
The Hadoop platform can now support a true data lake architecture with
the advent of Apache YARN.
In a multitenant environment, Enterprises can potentially run multiple 
workloads.
Data security within Hadoop needs to evolve to support multiple use cases 
for data access, while also providing a framework for central 
administration of security policies and monitoring of user access.
Please read the FAQs if you need to understand how it works over 
Apache Hadoop components.


\begin{IU}

hid-sp18-515

\href{https://github.com/cloudmesh-community/hid-sp18-515/blob/master//technology/abstract-Ranger.tex}{abstract-Ranger.tex}

 

Wordcount: 102

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 74
\end{IU}

\section{Jaspersoft}
\index{JasperReports}
\index{BI}
\index{reporting software}

Jaspersoft is a Business Intelligence (BI) platform that provides its
customers with highly interactive reports, analytics and dashboards. This tool
has been downloaded more than 14.5 million times which makes it the most
cost-effective, flexible and widely used platforms in the 
world~\cite{hid-sp18-516-www-finances-online}. It is embedded with data 
visualization, analytics, and reporting capabilities that allows its
customers to gain insight from various data sources and enables them to make
better decisions~\cite{hid-sp18-516-www-finances-online}. This reporting
software is designed to take input from one or more
data sources, including Big Data~\cite{hid-sp18-516-www-wiki-bigdata}, 
NoSQL~\cite{hid-sp18-516-www-wiki-nosql}, JDBC~\cite{hid-sp18-516-www-wiki-jdbc}, 
XML~\cite{hid-sp18-516-www-wiki-xml}, JSON~\cite{hid-sp18-516-www-wiki-json}, 
CSV~\cite{hid-sp18-516-www-wiki-csv}, 
Hibernate~\cite{hid-sp18-516-www-wiki-hibernate}, 
POJO~\cite{hid-sp18-516-www-wiki-pojo} and Web Services and present it in an 
easy-to-read, highly interactive format for business 
users~\cite{hid-sp18-516-www-jaspersoft-overview}. The print ready, interactive 
reports and dashboards enables organizations to interact with their data both 
inside or outside their organizations which also enables for faster business 
decision-making~\cite{hid-sp18-516-www-jaspersoft-overview}. This tool
provides a centralized repository, in which a customer can store user-profiles,
dashboards, analytic views, reports, and more. It supports thousands of users
and is designed for small, medium and big organizations including
enterprises~\cite{hid-sp18-516-www-finances-online}. It has a Java-based
reporting library
\textit{JasperReports}~\cite{hid-sp18-516-www-finances-online}, that provides
pixel-perfect documents and generates ad hoc based reports for the web, the
printer or mobile device~\cite{hid-sp18-516-www-jaspersoft-overview}.
JasperReports is an open source reporting tool that can be
used in any Java-enabled
applications~\cite{hid-sp18-516-www-wiki-jasperreports}. Jaspersoft is a gold
partner with MySQL and was recently acquired by TIBCO in April 28,
2014~\cite{hid-sp18-516-www-wiki-jasperreports}.


\begin{IU}

hid-sp18-516

\href{https://github.com/cloudmesh-community/hid-sp18-516/blob/master//technology/abstract-jaspersoft.tex}{abstract-jaspersoft.tex}

 

Wordcount: 209


Quote ratio: 0.00 \%
 
Max Line length: 82
\end{IU}

\section{Lingual}
\index{Cascading}
\index{ANSI-SQL}
\index{Hadoop}

Lingual is a free, open source project designed to build Big Data applications
on Apache Hadoop~\cite{hid-sp18-516-www-drivenio-lingual}. All dependencies are
installed through Maven~\cite{hid-sp18-516-www-wiki-maven}, thereby allowing the 
developers to focus on simply creating the applications which makes this tool 
easy to use~\cite{hid-sp18-516-www-cascading}. Lingual leverages the platform 
support of \textit{Cascading}~\cite{hid-sp18-516-www-wiki-cascading}, a 
stand-alone open source Java application framework used for building 
data-intensive, enterprise Big Data applications and frameworks on 
Hadoop~\cite{hid-sp18-516-www-drivenio-lingual}. Whether on-premise or in the 
cloud, Lingual is compatible with all major distributions of 
Hadoop~\cite{hid-sp18-516-www-drivenio-lingual}. Its ANSI-standard 
SQL~\cite{hid-sp18-516-www-wiki-sql} interface allows SQL users to utilize 
their existing SQL skills to access data locked on the 
Hadoop~\cite{hid-sp18-516-www-wiki-hadoop} clusters, thereby allowing them 
to create Big data applications instantly without undergoing any new 
training~\cite{hid-sp18-516-www-drivenio-lingual}. It also provides a 
JDBC~\cite{hid-sp18-516-www-wiki-jdbc} driver, that can be integrated with many 
existing BI tools and application servers~\cite{hid-sp18-516-www-cascading}. 
Being ANSI-SQL compliant, Lingual enables companies to query and export data 
from Hadoop directly into traditional BI tools~\cite{hid-sp18-516-www-cascading}. 
Lingual also provides other features like a SQL shell which is an interactive 
SQL command interface to interact with Hadoop and a Catalog to map the database 
tables into Hadoop files and resources~\cite{hid-sp18-516-www-cascading}. The 
ability to migrate workloads on to Hadoop either through Cascading applications 
or with the use of legacy SQL statements, significantly reduces the computing
costs~\cite{hid-sp18-516-www-cascading}. Due to it's ease of creating
applications using SQL, JDBC or traditional BI tools, it overcomes the barriers
of integrating Hadoop with the existing data management systems enabling fast
and simple Big Data application development on Apache
Hadoop~\cite{hid-sp18-516-www-drivenio-lingual}.


\begin{IU}

hid-sp18-516

\href{https://github.com/cloudmesh-community/hid-sp18-516/blob/master//technology/abstract-lingual.tex}{abstract-lingual.tex}

 

Wordcount: 241


Quote ratio: 0.00 \%
 
Max Line length: 82
\end{IU}

\section{MariaDB}
\index{MariaDB}
\index{MyRocks}


MariaDB is an open source relational database. It has 12 million users worldwide 
and one of the fastest growing databases in the 
world~\cite{hid-sp18-516-www-mariadb}. It powers applications at leading 
companies like booking.com, Virgin Mobile, HP 
etc.~\cite{hid-sp18-516-www-mariadb}. It was created by the original developers
of MySQL~\cite{hid-sp18-516-www-wiki-mysql} and Michael \textit{Monty} Widenius 
is the lead developer of MariaDB, also the founder of MySQL 
AB~\cite{hid-sp18-516-www-wiki-mariadb}. It was developed to be an enhanced, 
drop-in replacement for MySQL~\cite{hid-sp18-516-www-mariadb-foundation}. It is 
available in Debian~\cite{hid-sp18-516-www-debian-org} and 
Ubuntu~\cite{hid-sp18-516-www-wiki-ubuntu}, and is now the default database on 
many Linux~\cite{hid-sp18-516-www-wiki-linux} distributions. It supports a broad 
set of use cases by using different storage engines for different use cases. These 
pluggable storage engines make MariaDB a flexible, robust and scalable database 
solution~\cite{hid-sp18-516-www-mariadb-server}. Up until 10.1 version of MariaDB, 
it used Percona's XtraDB as the default storage engine, but from version 10.2, 
MariaDB uses InnoDB as the default storage 
engine~\cite{hid-sp18-516-www-wiki-xtradb}. Additional featured storage engines
in 10.2 version include \textit{MyRocks} for better performance and efficiency,
and \textit{Spider} for scaling out with distributed storage, however these are
under technical preview~\cite{hid-sp18-516-www-mariadb-server}. MariaDB intends
to maintain compatibility with MySQL~\cite{hid-sp18-516-www-wiki-mariadb}, but
it also aims at providing a rich ecosystem of storage engines, plugins and many
other tools to make it versatile for a wide variety of use
cases~\cite{hid-sp18-516-www-mariadb-foundation}. It is used to turn data into
structured information for a wide variety of applications, ranging from banking
to websites. MariaDB also supports GIS and JSON features in its latest
versions~\cite{hid-sp18-516-www-mariadb-foundation}. It is highly secure,
reliable and trusted by the world's leading brands. It is used to support
enterprise needs from OLTP to analytics~\cite{hid-sp18-516-www-mariadb}.


\begin{IU}

hid-sp18-516

\href{https://github.com/cloudmesh-community/hid-sp18-516/blob/master//technology/abstract-mariadb.tex}{abstract-mariadb.tex}

 

Wordcount: 249


Quote ratio: 0.00 \%
 
Max Line length: 83
\end{IU}

\section{Apache Solr}
\index{Lucene}
\index{search platform}
\index{restful}

Apache Solr is an open-source search platform used
to build search applications. It leverages the power of Apache
Lucene~\cite{hid-sp18-516-www-wiki-lucene}, which is a Java-based 
search library providing the core operations required by any search 
application like Indexing and 
Searching~\cite{hid-sp18-516-www-tutorialspoint-solr}. Apache Lucene and Apache
Solr were merged together in 2010 and since then, produced by the Apache
Software Foundation development team. It has an active development community 
and regular releases. Solr has RESTful API's like 
HTTP/XML~\cite{hid-sp18-516-www-wiki-xml} or 
JSON~\cite{hid-sp18-516-www-wiki-json} to communicate with it that can be used 
from most popular programming languages~\cite{hid-sp18-516-www-wiki-solr}. 
It has all capabilities required for a full-text search server such as tokens, 
spell check, wildcard, phrases and auto-complete. It is fast, highly scalable, 
reliable, fault-tolerant and enterprise-ready and can be deployed in any kind 
of systems such as standalone, distributed or 
cloud~\cite{hid-sp18-516-www-tutorialspoint-solr}. Other major features include 
hit highlighting, built-in security, distributed search through sharding, 
database integration, faceted search, rich document (e.g., Word, PDF)
handling~\cite{hid-sp18-516-www-wiki-solr}. As Hadoop can handle large amounts
of data, Solr can be used with Hadoop~\cite{hid-sp18-516-www-wiki-hadoop} to 
find the required information from a large source. Apart from search, Solr also 
has NoSQL~\cite{hid-sp18-516-www-wiki-nosql} features and it can be used
as a non-relational data storage and processing technology. The components of
Solr can be customized easily by extending and configuring its Java classes
thereby making it flexible and 
extensible~\cite{hid-sp18-516-www-tutorialspoint-solr}. Solr provides navigation
features to world's largest internet sites like Netflix, Instagram, Best Buy,
eBay etc.~\cite{hid-sp18-516-www-apacheorg-solr}. It is packaged as the built-in
search in many applications such as content management systems and enterprise
content management systems~\cite{hid-sp18-516-www-wiki-solr}.


\begin{IU}

hid-sp18-516

\href{https://github.com/cloudmesh-community/hid-sp18-516/blob/master//technology/abstract-solr.tex}{abstract-solr.tex}

 

Wordcount: 244


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{TokuDB}
\index{fractal-tree}
\index{Percona}
\index{high-performance}

TokuDB is an open-source storage engine for 
MySQL~\cite{hid-sp18-516-www-wiki-mysql} and 
MariaDB~\cite{hid-sp18-516-www-wiki-mariadb} used for high-performance in 
write-intensive environments. It uses fractal-tree index data structure, 
that keeps the data sorted and allows searches and sequential data access 
simultaneously, thereby providing improved 
performance~\cite{hid-sp18-516-www-wiki-tokudb}. TokuDB compresses all data 
on disk including indexes, thereby reducing the disk and flash-drive storage 
requirements. It eliminates slave lag with read free 
replication~\cite{hid-sp18-516-www-percona-server-tokudb}. It is ACID and MVCC
compliant and offers online schema-modifications. It is also included in
Percona 
server~\cite{hid-sp18-516-www-percona-tokudb}~\cite{hid-sp18-516-www-wiki-tokudb}. 
The use of fractal-tree technology also enables TokuDB to speed indexing by 10 
times or more, thereby improving the performance of large databases (typically 
50 GB or more). Its exceptional indexing feature makes it an ideal solution for 
applications that must simultaneously query and update huge volumes of rapidly 
arriving data~\cite{hid-sp18-516-www-blackbird-si}. This also makes it scalable 
and improves operational efficiency. TokuDB is well-suited for the demanding
requirements of big data applications as it lowers the infrastructure costs
associated with scaling and optimization
efforts~\cite{hid-sp18-516-www-percona-tokudb}. It has zero-maintenance downtime
which makes it highly available in both public and private environments
including
cloud~\cite{hid-sp18-516-www-percona-server-tokudb}~\cite{hid-sp18-516-www-percona-tokudb}.


\begin{IU}

hid-sp18-516

\href{https://github.com/cloudmesh-community/hid-sp18-516/blob/master//technology/abstract-tokudb.tex}{abstract-tokudb.tex}

 

Wordcount: 172


Quote ratio: 0.00 \%
 
Max Line length: 91
\end{IU}

\section{Hue}


Hue or Hadoop User Interface is an open source tool licensed under Apache v2
license. It sits on top of the data at the visualization layer and provides a
graphical user interface to operate and develop applications for performing
self-service data analytics.  The latest version of Hue available is v4.1.0,
released October 4th 2017 and can be downloaded from the website
~\cite{hid-sp18-517-hue-apache}. Hue is an open source Analytics Workbench for
browsing, querying and visualizing data. Hue works well for a variety of
technologies in the Hadoop ecosystem such as Hive, Impala, Pig, MapReduce,
Spark. Query tool works with SparkSQL, Solr SQL and Phoenix. Further it works
well with RDBMS such as Oracle and MySQL~\cite{hid-sp18-517-Hue-wiki}. Some of
the applications of Hue are analytics dashboards, job scheduling, workflows, it
also serves as an interface for jobs, HDFS, S3 files, SQL Tables, Indexes, Git
files, Sentry permissions, Sqoop and more~\cite{hid-sp18-517-Hue-wiki}.Hue is
available in all major Hadoop distributions such as Cloudera, Hortonworks, MapR
and AWS~\cite{hid-sp18-517-Hue-wiki}.


\begin{IU}

hid-sp18-517

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-hue.tex}{abstract-hue.tex}

 

Wordcount: 148


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Kafka}

Kafka is an open source distributed streaming platform that belongs 
to the Apache Hadoop family. It is mainly used in data ingestion and 
building real time data pipelines. Since Kafka is fault tolerant, 
scalable and efficient it is in production today in thousands of 
companies. Kafka also works as messaging system based on the 
publisher-subscriber model where the publisher produces data and 
the subscriber consumes the data. It can be compared to ActiveMQ 
in the messaging space~\cite{hid-sp18-517-ApacheKafka}.The two main
real-time streaming applications of Kafka are, building  data
pipe-lines to transport data between systems or applications, and
building applications that transform or react to stream of data. A few
use cases of Kafka are website activity tracking, messaging 
and log aggregation~\cite{hid-sp18-517-ApacheKafka}.



\begin{IU}

hid-sp18-517

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-kafka.tex}{abstract-kafka.tex}

 

Wordcount: 117

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\section{Kudu}

Apache Kudu was designed to fit into the Hadoop ecosystem and it 
serves as the storage layer that enables fast analytics on fast 
data~\cite{hid-sp18-517-ApacheKudu}.

Kudu internally follows the columnar storage approach rather than 
storing data in rows. This columnar approach helps in efficient 
encoding and compression. Kudu serves as a good alternative to HDFS 
and Apache HBase. It works best especially with use cases that 
require fast analytics on fast data. It is also efficient and 
designed to take advantage of next generation hardware and 
in-memory processing~\cite{hid-sp18-517-ApacheKudu}.





\begin{IU}

hid-sp18-517

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-kudu.tex}{abstract-kudu.tex}

 

Wordcount: 86

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 68
\end{IU}

\section{Kylin}

Apache Kylin is an Online analytical processing or OLAP engine developed 
specially for Big Data applications. Analysts around the world today 
use SQL interfaces to query data from traditional database systems, 
Kylin fits in well due to this as it provides SQL interface as well 
as OLAP capability for Hadoop ecosystem which no other tool provides 
today. Kylin is very efficient and can return billions of rows in 
minimum time. It integrates well other BI tools such as Tableau. 
Kylin is rich in features and provides, incremental refresh of cubes, 
web interface for monitoring and management and ldap integration as 
well. Since Kylin was specially built for Hadoop and big data 
applications, it depends on some of the components such as HBase, 
Hive and HDFS. HBase is used to store the data cube, map reduce does 
the cube refresh or partial refresh job and HDFS stores intermediate 
files during cube building process~\cite{hid-sp18-517-ApacheKylin}.



\begin{IU}

hid-sp18-517

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-kylin.tex}{abstract-kylin.tex}

 

Wordcount: 152


Quote ratio: 0.00 \%
 
Max Line length: 73
\end{IU}

\section{Metron}

Apache Metron is yet another open source project that serves as a big data
solution for CyberSecurity applications. It provides a framework to ingest,
process and also store data such as application logs, network logs and so on so
that can be analyzed by Information security teams so that they can detect
anomalies and respond to cyber threats. It provides the storage solution in the
form of security data lake or vault where the logs can be stored long term on
cost effective storage. In addition to SIEM or Security information and event
management features Metron also provides packet replay utilities which can be of
immense help for the security analysts.  Metron also support applying machine
learning algorithms on real time data that is being ingested through continuous
streams~\cite{hid-sp18-517-metron-apache}.  Apache Metron caters to personnels
at all levels in the CyberSecurity operations from CISO to SOC analyst to
security Data Scientist~\cite{hid-sp18-517-metron-hortonworks}.  It provides a
single view of the risk to CISO or Chief Information Security Officer while
automatically performs that analytics so security investigator does not have to
spend time on finding co-relation in the data. Metron also has the capability to
create incidents and can integrate with the ITSM or Information technology
service management systems to provide
traceability~\cite{hid-sp18-517-metron-hortonworks}.


\begin{IU}

hid-sp18-517

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-metron.tex}{abstract-metron.tex}

 

Wordcount: 193


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{MongoDB}

MongoDB is document database that belongs to the NoSQL family of 
databases. MongoDB is free and open-source, published under the 
GNU Affero General Public License. It is known for its scalability 
and flexibility. MongoDB stores data in flexible JSON-like 
documents. MongoDB's HA features include automatic failover and 
data redundancy, this is achieved using replica set, which is 
nothing but group of MongoDB servers that maintain the same data. 
MongoDB supports sharding by distributing the data across the 
cluster of machines~\cite{hid-sp18-517-MongoDB-intro}.




\begin{IU}

hid-sp18-517

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-mongodb.tex}{abstract-mongodb.tex}

 

Wordcount: 80

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 67
\end{IU}

\section{Pulsar}

Apache Pulsar which is also an open source project of the Apache foundation was
originally developed by Yahoo. It is a messaging solution that enables high
performance server to server messaging.  Similar to Kafka Pulsar is based on
publisher-subscriber model.  Some of the key features of Pulsar include low
latency in publishing, guaranteed message delivery, scalability and so on.  The
publish-subscribe pattern involves components such as producers, consumers,
topics and subscription wherein; topics are channels that transmit data from
source to target or in other words from producers to consumers, producers job is
to publish a message and a consumer process is the one that receives the
message.  Subscriptions are set of rules that determine how messages flow in the
system from producers to consumers and have three modes namely exclusive,
failover and shared~\cite{hid-sp18-517-pulsar-apache}.  Pulsar can be installed
and run in standalone mode or standalone cluster, it can also be run multiple
clusters. Pulsar installation involves installing an instance which can be
installed across clusters when installed in multi-cluster environment. In this
setup clusters can be running within the data center or can span across multiple
data centers.  Pulsar also support geo-replication so the clusters can replicate
with each other. Pulsar can also be installed on Kubernetes on Google Kubernetes
or AWS~\cite{hid-sp18-517-pulsar-apache}.


\begin{IU}

hid-sp18-517

ERROR: index is missing

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-pulsar.tex}{abstract-pulsar.tex}

 

Wordcount: 200


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Sqoop}

The primary application of Sqoop is data transfer between the traditional or
relational database management systems and Hadoop platforms. It also has the
capability to transfer data from mainframes to Hadoop. Sqoop works with Oracle,
MySQL and can import data from these sources into the Hadoop distributed File
systems or HDFS. In addition it can also transform data in map-reduce or even
export it to the database such as Oracle.  Sqoop works in batch mode and cannot
move data real time.  ~\cite{hid-sp18-517-Sqoop}. Sqoop relies on the database
to describe the schema of the data being imported. It uses MapReduce to import
and export the data, which provides parallel operation as well as fault
tolerance.  For databases, Sqoop reads the table row-by-row into HDFS.  For
mainframe datasets, Sqoop reads records from each mainframe dataset into
HDFS. The output of this import process is a set of files containing a copy of
the imported table or datasets. Since the import process runs in parallel
processes each process creates a file causing multiple files being
created. These text files can use different delimiters such as comma, pipe and
so on ~\cite{hid-sp18-517-Sqoop}. Sqoop relies on the database to describe the
schema of the data being imported. It uses MapReduce to import and export the
data, which provides parallel operation as well as fault tolerance.  For
databases, Sqoop reads the table row-by-row into HDFS.  For mainframe datasets,
Sqoop reads records from each mainframe dataset into HDFS. The output of this
import process is a set of files containing a copy of the imported table or
datasets. Since the import process runs in parallel processes each process
creates a file causing multiple files being created. These text files can use
different delimiters such as comma, pipe and so on~\cite{hid-sp18-517-Sqoop}.
\footnote{citation wrongly placed}


\begin{IU}

hid-sp18-517

ERROR: index is missing

ERROR: entry contains a footnote that has not yet been addressed

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-sqoop.tex}{abstract-sqoop.tex}

 
chktex:
\begin{tiny}
\begin{verbatim}
Warning 39 in ../hid-sp18-517/technology/abstract-sqoop.tex line 9: Double space
found. move data real time.  ~\cite{hid-sp18-517-Sqoop}. Sqoop relies on the
database                        ^ Warning 39 in ../hid-
sp18-517/technology/abstract-sqoop.tex line 18: Double space found. so on
~\cite{hid-sp18-517-Sqoop}. Sqoop relies on the database to describe the
^
\end{verbatim}
\end{tiny}

Wordcount: 278


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{AWS API Gateway}
\index{AWS API Gateway}
\index{API Gateway}

The AWS API Gateway~\cite{hid-sp18-518-AWS-APIGateway} is used to manage
multiple RESTful services in a defined way. You can set up the API Gateway
using the CLI, CloudFormation or even Swagger templates. The API Gateway is
serverless and AWS will manage all of the underlying infrastructure for you. 
The design allows you to configure the API mapping and integrations. The API 
Gateway can then help you define authentication/authorization controls, 
define the lifecycle for the services and even track transactions for uses
like billing.

% Notes

% Note that this section has references missing such as acc-stat. File
% names must be lower case and not contain an underscore. The abstract
% is contained in a file called abstract-<tech>.tex. The bib file is
% contained in a file called <hid>.bib.

% Make sure that bib labels have the prefix of your hid. In our case it
% is something like hid-sp18-999. Make sure you do not under any
% circumstances use underscores in bib labels as they break our scripts.

% Make sure you resolve bibtex warnings and errors.

% Make sure to use the Makefile (and modify it accordingly) to check if
% your latex file compiles. Only check it into git if it compiles. If
% you do not know how to use Makefiles, please lear nit or use alternative
% commands in the terminal. Look at the Makefile in which order you
% need to execute them if you do not use makefiles. 

% UNDER NO CIRCUMSTANCES COMMIT THE GENERATED PDF INTO GITHUB. COMMIT
% EVERY FILE INDIVIDUALLY TO MAKE SURE YOU AVOID THIS. WE WILL
% DEDUCT YOU POINTS IF (a) YOU COMMITTED A PDF FILE (b) YOUR LATEX FILE
% DOES NOT COMPILE OR CONTAINS ERRORS (c) YOUR BIB FILE IS INCOMPLETE OR
% CONTAINS ERRORS. (d) YOUR TEXT IS NOT FORMATTED TO HAVE A MAXIMUM OF
% 80 CHARACTERS IN EACH LINE. 

% Points in case of a, b, c you will get 0 points as you will cause our
% scripts to break. In case of d you will get a 50\% point deduction. We
% want to set with this simple example a mechanism for you to check
% larger papers. It is not sufficient to say but my paper compiles in
% sharelatex. It is your responsibility to make sure that what is in your
% directory compiles properly in LaTeX. You are allowed to use a native
% LaTeX deployment if you have one set up. Make sure to install ALL of
% latex and not just the reduced version. 



\begin{IU}

hid-sp18-518

\href{https://github.com/cloudmesh-community/hid-sp18-518/blob/master//technology/abstract-AWS-APIGateway.tex}{abstract-AWS-APIGateway.tex}

 

Wordcount: 403


Quote ratio: 0.00 \%
 
Max Line length: 77
\end{IU}

\section{CloudTrail}
\index{CloudTrail}
\index{AWS CloudTrail}

The AWS CloudTrail~\cite{hid-sp18-518-CloudTrail} service is an activity 
recording service provided by Amazon Web Services. The service allows you 
to track the history of account usage for your AWS instances. The service 
is not on by default yet when configured, it will record all API calls from
all sources like the console, CLE, SDKs or CloudFormation. The data is 
written into an S3 bucket via JSON and would include attributes lik user,
IP address, timestamp and the action the user took.


% Notes

% Note that this section has references missing such as tacc-stat. File
% names must be lower case and not contain an underscore. The abstract
% is contained in a file called abstract-<tech>.tex. The bib file is
% contained in a file called <hid>.bib.

% Make sure that bib labels have the prefix of your hid. In our case it
% is something like hid-sp18-999. Make sure you do not under any
% circumstances use underscores in bib labels as they break our scripts.

% Make sure you resolve bibtex warnings and errors.

% Make sure to use the Makefile (and modify it accordingly) to check if
% your latex file compiles. Only check it into git if it compiles. If
% you do not know how to use Makefiles, please lear nit or use alternative
% commands in the terminal. Look at the Makefile in which order you
% need to execute them if you do not use makefiles. 

% UNDER NO CIRCUMSTANCES COMMIT THE GENERATED PDF INTO GITHUB. COMMIT
% EVERY FILE INDIVIDUALLY TO MAKE SURE YOU AVOID THIS. WE WILL
% DEDUCT YOU POINTS IF (a) YOU COMMITTED A PDF FILE (b) YOUR LATEX FILE
% DOES NOT COMPILE OR CONTAINS ERRORS (c) YOUR BIB FILE IS INCOMPLETE OR
% CONTAINS ERRORS. (d) YOUR TEXT IS NOT FORMATTED TO HAVE A MAXIMUM OF
% 80 CHARACTERS IN EACH LINE. 

% Points in case of a, b, c you will get 0 points as you will cause our
% scripts to break. In case of d you will get a 50\% point deduction. We
% want to set with this simple example a mechanism for you to check
% larger papers. It is not sufficient to say but my paper compiles in
% sharelatex. It is your responsibility to make sure that what is in your
% directory compiles properly in LaTeX. You are allowed to use a native
% LaTeX deployment if you have one set up. Make sure to install ALL of
% latex and not just the reduced version. 



\begin{IU}

hid-sp18-518

\href{https://github.com/cloudmesh-community/hid-sp18-518/blob/master//technology/abstract-CloudTrail.tex}{abstract-CloudTrail.tex}

 

Wordcount: 400


Quote ratio: 0.00 \%
 
Max Line length: 75
\end{IU}

\section{CloudWatch}
\index{CloudWatch}
\index{AWS CloudWatch}

The AWS CloudWatch~\cite{hid-sp18-518-CloudWatch} service is the monitoring 
service provided by Amazon Web Services. Everything from metrics for resource
usage, billing usage, and up to including custom data can be used to 
group elements into graphs. You can summarize across all instances or you
can configure dimensions to allow to focus on certain aspects. Dimensions are
a name/value pair that you can establish to target (ex. ServiceName/awskms) yet
only certain AWS services are available for aggregation. You can stream the 
log data to an S3 bucket, to a Lambda function or to Elastic Search. It can 
also be used to collect logs from your Windows and Linux instances and if
you develop an API for your application, it can pull from there as well.


% Notes

% Note that this section has references missing such as tacc-stat. File
% names must be lower case and not contain an underscore. The abstract
% is contained in a file called abstract-<tech>.tex. The bib file is
% contained in a file called <hid>.bib.

% Make sure that bib labels have the prefix of your hid. In our case it
% is something like hid-sp18-999. Make sure you do not under any
% circumstances use underscores in bib labels as they break our scripts.

% Make sure you resolve bibtex warnings and errors.

% Make sure to use the Makefile (and modify it accordingly) to check if
% your latex file compiles. Only check it into git if it compiles. If
% you do not know how to use Makefiles, please lear nit or use alternative
% commands in the terminal. Look at the Makefile in which order you
% need to execute them if you do not use makefiles. 

% UNDER NO CIRCUMSTANCES COMMIT THE GENERATED PDF INTO GITHUB. COMMIT
% EVERY FILE INDIVIDUALLY TO MAKE SURE YOU AVOID THIS. WE WILL
% DEDUCT YOU POINTS IF (a) YOU COMMITTED A PDF FILE (b) YOUR LATEX FILE
% DOES NOT COMPILE OR CONTAINS ERRORS (c) YOUR BIB FILE IS INCOMPLETE OR
% CONTAINS ERRORS. (d) YOUR TEXT IS NOT FORMATTED TO HAVE A MAXIMUM OF
% 80 CHARACTERS IN EACH LINE. 

% Points in case of a, b, c you will get 0 points as you will cause our
% scripts to break. In case of d you will get a 50\% point deduction. We
% want to set with this simple example a mechanism for you to check
% larger papers. It is not sufficient to say but my paper compiles in
% sharelatex. It is your responsibility to make sure that what is in your
% directory compiles properly in LaTeX. You are allowed to use a native
% LaTeX deployment if you have one set up. Make sure to install ALL of
% latex and not just the reduced version. 



\begin{IU}

hid-sp18-518

\href{https://github.com/cloudmesh-community/hid-sp18-518/blob/master//technology/abstract-CloudWatch.tex}{abstract-CloudWatch.tex}

 

Wordcount: 440


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Cognito}
\index{Cognito}
\index{AWS Cognito}

The AWS Cognito~\cite{hid-sp18-518-Cognito} service is used to federate your
user registration and their ability to sign into your services. The solution
allows you to easily manage user pools and can integrate with multiple SDKs 
like Java, Python, PHP and Ruby. The client application can be configured to
use SAML, OIDC or other backend user directory services. The service is 
intended to be used in conjuction with AWS IAM and STS.


% Notes

% Note that this section has references missing such as tacc-stat. File
% names must be lower case and not contain an underscore. The abstract
% is contained in a file called abstract-<tech>.tex. The bib file is
% contained in a file called <hid>.bib.

% Make sure that bib labels have the prefix of your hid. In our case it
% is something like hid-sp18-999. Make sure you do not under any
% circumstances use underscores in bib labels as they break our scripts.

% Make sure you resolve bibtex warnings and errors.

% Make sure to use the Makefile (and modify it accordingly) to check if
% your latex file compiles. Only check it into git if it compiles. If
% you do not know how to use Makefiles, please lear nit or use alternative
% commands in the terminal. Look at the Makefile in which order you
% need to execute them if you do not use makefiles. 

% UNDER NO CIRCUMSTANCES COMMIT THE GENERATED PDF INTO GITHUB. COMMIT
% EVERY FILE INDIVIDUALLY TO MAKE SURE YOU AVOID THIS. WE WILL
% DEDUCT YOU POINTS IF (a) YOU COMMITTED A PDF FILE (b) YOUR LATEX FILE
% DOES NOT COMPILE OR CONTAINS ERRORS (c) YOUR BIB FILE IS INCOMPLETE OR
% CONTAINS ERRORS. (d) YOUR TEXT IS NOT FORMATTED TO HAVE A MAXIMUM OF
% 80 CHARACTERS IN EACH LINE. 

% Points in case of a, b, c you will get 0 points as you will cause our
% scripts to break. In case of d you will get a 50\% point deduction. We
% want to set with this simple example a mechanism for you to check
% larger papers. It is not sufficient to say but my paper compiles in
% sharelatex. It is your responsibility to make sure that what is in your
% directory compiles properly in LaTeX. You are allowed to use a native
% LaTeX deployment if you have one set up. Make sure to install ALL of
% latex and not just the reduced version. 



\begin{IU}

hid-sp18-518

\href{https://github.com/cloudmesh-community/hid-sp18-518/blob/master//technology/abstract-Cognito.tex}{abstract-Cognito.tex}

 

Wordcount: 389


Quote ratio: 0.00 \%
 
Max Line length: 76
\end{IU}

\section{FoundationBenchmarks}
\index{Foundation Benchmarks}
\index{AWS Foundation Benchmarks}

The AWS Foundation Benchmarks~\cite{hid-sp18-518-FoundationBenchmarks} project
is a repository of Python scripts that can be used to evaluate your AWS
account and it's configuration. The project is intended to be integrated
into your CloudFormation stack so that it can run the benchmarks on every
iteration of your code pipeline. The benchmarks are sourced from the Center
of Internet Security and it can help you find issues in your IAM, your VPC
configuration, your S3 bucket permissions and many other places that are
commonly left open by default or accident.


% Notes

% Note that this section has references missing such as tacc-stat. File
% names must be lower case and not contain an underscore. The abstract
% is contained in a file called abstract-<tech>.tex. The bib file is
% contained in a file called <hid>.bib.

% Make sure that bib labels have the prefix of your hid. In our case it
% is something like hid-sp18-999. Make sure you do not under any
% circumstances use underscores in bib labels as they break our scripts.

% Make sure you resolve bibtex warnings and errors.

% Make sure to use the Makefile (and modify it accordingly) to check if
% your latex file compiles. Only check it into git if it compiles. If
% you do not know how to use Makefiles, please lear nit or use alternative
% commands in the terminal. Look at the Makefile in which order you
% need to execute them if you do not use makefiles. 

% UNDER NO CIRCUMSTANCES COMMIT THE GENERATED PDF INTO GITHUB. COMMIT
% EVERY FILE INDIVIDUALLY TO MAKE SURE YOU AVOID THIS. WE WILL
% DEDUCT YOU POINTS IF (a) YOU COMMITTED A PDF FILE (b) YOUR LATEX FILE
% DOES NOT COMPILE OR CONTAINS ERRORS (c) YOUR BIB FILE IS INCOMPLETE OR
% CONTAINS ERRORS. (d) YOUR TEXT IS NOT FORMATTED TO HAVE A MAXIMUM OF
% 80 CHARACTERS IN EACH LINE. 

% Points in case of a, b, c you will get 0 points as you will cause our
% scripts to break. In case of d you will get a 50\% point deduction. We
% want to set with this simple example a mechanism for you to check
% larger papers. It is not sufficient to say but my paper compiles in
% sharelatex. It is your responsibility to make sure that what is in your
% directory compiles properly in LaTeX. You are allowed to use a native
% LaTeX deployment if you have one set up. Make sure to install ALL of
% latex and not just the reduced version. 



\begin{IU}

hid-sp18-518

\href{https://github.com/cloudmesh-community/hid-sp18-518/blob/master//technology/abstract-FoundationBenchmarks.tex}{abstract-FoundationBenchmarks.tex}

 

Wordcount: 405


Quote ratio: 0.00 \%
 
Max Line length: 78
\end{IU}

\section{OpenNN}
\index{OpenNN}

\color{blue}``\emph{OpenNN is an open source class library written in C++ programming language
which implements neural networks, a main area of machine learning research.
The main advantage of OpenNN is its high performance. It is developed in 
C++ for better memory management and higher processing speed, and 
implements CPU parallelization. The library implements any number of layers
of non-linear processing units for supervised learning. This deep 
architecture allows the design of neuralnetworks with universal approximation
properties}''\color{black}~\cite{hid-sp18-520-OpenNN}.
\color{blue}``\emph{Neural Designer has been developed from OpenNN, which is a advanced analytical
tool and contains a graphical user interface which simplifies data entry
and interpretation of results}''\color{black}~cite{hid-sp18-520-OpenNN}.
\color{blue}``\emph{The class of neural network implemented in OpenNN is based on the multilayer 
perceptron. That model is extended here to contain scaling, unscaling, bounding,
probabilistic and conditions layers}''\color{black}~\cite{hid-sp18-520-OpenNNn}.
\color{blue}``\emph{OpenNN includes project files for Qt Creator. When working with another 
compiler is needed,a project for it must be created}''\color{black}~\cite{hid-sp18-520-OpenNNb}. 



\begin{IU}

hid-sp18-520

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-OpenNN.tex}{abstract-OpenNN.tex}

 

Wordcount: 141


Quote ratio: 81.11 \%

ERROR: Quote ratio too high
 
Max Line length: 98
\end{IU}

\section{OrientDB}
\index{OrientDB}

\color{blue}``\emph{OrientDB is the open source NoSQL multi-model database that works with graph 
databases}''\color{black}~\cite{hid-sp18-520-OrientDB}. Along with this it can be set up as 
document database and Object-Oriented database. 
\color{blue}``\emph{Graph databases are NoSQL databases which use the graph data model comprised 
of vertices, which is an entity such as a person, place, object or relevant 
piece of data and edges, which represent the relationship between two nodes.
Graph databases are particularly helpful because they highlight the links and
relationships between relevant data similarly to how we do so ourselves.
Even though graph databases are awesome, they are not enough on their own.
Advanced second-generation NoSQL products like OrientDB are the future. The 
modern multi-model database provides more functionality and flexibility while
being powerful enough to replace traditional 
DBMSs}''\color{black}~\cite{hid-sp18-520-OrientDB-graph}.
Graph databases are usefull for developing application related to social 
networking and establish relationships between objects with respect to there 
properties. It maintains class relation using documents and links in document 
model.


\begin{IU}

hid-sp18-520

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-OrientDB.tex}{abstract-OrientDB.tex}

 

Wordcount: 152


Quote ratio: 61.66 \%

ERROR: Quote ratio too high
 
Max Line length: 97
\end{IU}

\section{PyTorch}
\index{PyTorch}

\color{blue}``\emph{PyTorch is a open source python package that has high level features of 
Tensor computation with strong GPU acceleration and Deep 
Neural Networks built on a tape-based autograd system}''\color{black}
~\cite{hid-sp18-520-PyTorch}.
PyTorch has many packages and are used for deep learning, multi processing,
loading data. It is fast and has high computation speed when run with any size 
of datasets.
Out of many libraries of PyTorch, \color{blue}``\emph{A PyTorch Tensor is conceptually identical 
to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many
functions for operating on these Tensors. Like numpy arrays, PyTorch Tensors do 
not know anything about deep learning or computational graphs or gradients; they
are a generic tool for scientific computing}''\color{black}~\cite{hid-sp18-520-PyTorchtensor}.
PyTorch supports dynamic computation graphs, where the computational graph can 
be created in real run time.
\color{blue}``\emph{Respect to Grad, This is especially useful when you want to freeze part of your
model, or you know in advance that you are not going to use gradients w.r.t. 
some parameters. If there is a single input to an operation that requires 
gradient, its output will also require gradient. Conversely, only if all inputs 
do not require gradient, the output also will not require it. Backward 
computation is never performed in the subgraphs, where all Variables did not 
require gradients}''\color{black}~\cite{hid-sp18-520-PyTorchgrad}.


\begin{IU}

hid-sp18-520

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-PyTorch.tex}{abstract-PyTorch.tex}

 

Wordcount: 204


Quote ratio: 66.06 \%

ERROR: Quote ratio too high
 
Max Line length: 99
\end{IU}

\section{RabbitMQ}
\index{RabbitMQ}

RabbitMQ technology is open source message broker, which supports multiple 
messaging protocols. It has many features such as asynchronous messaging, 
which supports message queuing, receive and deliver acknowledgments, routing
any message queues with broadcasting to logs or messages to multiple users.
\color{blue}``\emph{A RabbitMQ broker is a logical grouping of one or
several Erlang nodes, each running the RabbitMQ application and sharing users,
virtual hosts, queues, exchanges, bindings, and runtime parameters. Sometimes 
we refer to the collection of nodes as a 
cluster}''\color{black}~\cite{hid-sp18-520-RabbitMQCluster}.
\color{blue}``\emph{RabbitMQ has pluggable support for various SASL authentication mechanisms. 
Among many, with PLAIN authentication, its enabled by default in the RabbitMQ 
server and clients}''\color{black}~\cite{hid-sp18-520-RabbitMQauth}.
\color{blue}``\emph{It was originally implemented Advance Message Queuing Protocoland has been 
extended to support Streaming Text Oriented Messaging Protocol and other 
protocols}''\color{black}~\cite{hid-sp18-520-RabbitMQ-wiki}.
\color{blue}``\emph{The rabbitmq-management plugin provides an HTTP-based API for management and 
monitoring of your RabbitMQ server, along with a browser-based UI and a 
command line tool, rabbitmqadmin}''\color{black}~\cite{hid-sp18-520-RabbitMQ-mana}.


\begin{IU}

hid-sp18-520

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-RabbitMQ.tex}{abstract-RabbitMQ.tex}

 

Wordcount: 145


Quote ratio: 58.20 \%

ERROR: Quote ratio too high
 
Max Line length: 97
\end{IU}

\section{Redis}
\index{Redis}

\color{blue}``\emph{Redis is an open source (BSD licensed), in-memory data structure store, used
as a database, cache and message broker. It supports data structures such as
strings, hashes, lists, sets, sorted sets with range queries, bitmaps, 
hyperloglogs and geospatial indexes with radius queries. Redis has built-in 
replication, Lua scripting, LRU eviction, transactions and different levels 
of on-disk persistence, and provides high availability via Redis Sentinel and
automatic partitioning with Redis Cluster.
It can run atomic operations on these types, like appending to a string, 
incrementing the value in a hash; pushing an element to a list; computing set
intersection, union and difference, or getting the member with highest 
ranking in a sorted set.
In order to achieve its outstanding performance, Redis works with an in-memory
dataset. Depending on your use case, you can persist it either by dumping the
dataset to disk every once in a while, or by appending each command to a log.
Redis also supports trivial-to-setup master-slave asynchronous replication, 
with very fast non-blocking first synchronization, auto-reconnection with 
partial resynchronization on net split}''\color{black}~\cite{hid-sp18-520-Redis}.
Redis is No SQL database, supports Key value databases by mapping its key
to type of values.


\begin{IU}

hid-sp18-520

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-Redis.tex}{abstract-Redis.tex}

 

Wordcount: 180


Quote ratio: 86.65 \%

ERROR: Quote ratio too high
 
Max Line length: 96
\end{IU}

\section{TensorFlow}
\index{TensorFlow}

\color{blue}``\emph{TensorFlow is an open source software library for numerical computation using
data flow graphs. Nodes in the graph represent mathematical operations, while the 
graph edges represent the multidimensional data arrays (tensors) communicated 
between them. The flexible architecture allows you to deploy computation to one 
or more CPUs or GPUs in a desktop, server, or mobile device with a single API. 
TensorFlow was originally developed by researchers and engineers working on the 
Google Brain Team within Google Machine Intelligence research organization for 
the purposes of conducting machine learning and deep neural networks research, 
but the system is general enough to be applicable in a wide variety of other 
domains as well}''\color{black}~\cite{hid-sp18-520-TensorFlow}.
TensorFlow provides a platform for implementing and execute machine learning
algorithms and is highly popular with deep neural network models and algorithms.
TensorFlow is cross-platform, it can support mobile and embedded platforms. It has
lot of API to support complex computations and algorithms.


\begin{IU}

hid-sp18-520

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-TensorFlow.tex}{abstract-TensorFlow.tex}

 

Wordcount: 148


Quote ratio: 64.63 \%

ERROR: Quote ratio too high
 
Max Line length: 97
\end{IU}

\section{Amazon Athena}
\index{Amazon Athena}

Amazon Athena~\cite{hid-sp18-521-athena-faq} is a service from AWS that
allows the user to analyze their data stored on Amazon S3 using SQL code. 
It was created with the purpose of allowing anyone with SQL skills 
to quickly analyze large datasets. Athena will allow a user to run 
on demand SQL queries without the need to load or gather the data 
outside of S3 and can process structured, semi-structured and 
unstructured data sets. It it serverless so there is no need to 
deal with the setup or managing of infrastructure. This also allows
Athena to scale automatically in order to be able to handle large datasets
and complex queries. Athena utilizes Presto which is an open source SQL query
engine designed to query data wherever it is stored. You can access
Athena in multiple ways including the AWS Management Console, API or
JDBC driver. It also integrates into Amazon Quicksight allowing you
to visualize the data stored in your S3 environment based on your
Athena queries. Athena is great for fast on demand querying, but can
be used for complex joins, window functions and arrays as well.


\begin{IU}

hid-sp18-521

\href{https://github.com/cloudmesh-community/hid-sp18-521/blob/master//technology/abstract-athena.tex}{abstract-athena.tex}

 

Wordcount: 178


Quote ratio: 0.00 \%
 
Max Line length: 77
\end{IU}

\section{Amazon EMR}
\index{Amazon EMR}

Amazon EMR~\cite{hid-sp18-521-AmazonEMR} is a Hadoop framework that allows
the user to process data on the AWS platform using their EC2 technology to
spread the load across multiple EC2 instances. Elasticity a major benefit
of this product as it can be set to auto scale up or down the number of
EC2 instances that EMR is running in a cluster. The user can choose to run 
a few additional frameworks supported on EMR in addition to Hadoop,
such as Spark, HBase, Flink and Presto. It allows the user to focus on the
processing of the data and not have to deal with the setup, management or
tuning of a Hadoop cluster. Using EMR allows a user to setup and provision
a Hadoop cluster quickly and you can scale your compute resources up or
down as needed. You can interact with EMR through a web service interface
or you can also use the AWS Management Console to launch and monitor your
clusters.


\begin{IU}

hid-sp18-521

\href{https://github.com/cloudmesh-community/hid-sp18-521/blob/master//technology/abstract-emr.tex}{abstract-emr.tex}

 

Wordcount: 151


Quote ratio: 0.00 \%
 
Max Line length: 75
\end{IU}

\section{Fission}
\index{Fission}

Fission~\cite{hid-sp18-521-FissionBlog} is an open source, serverless
framework for Kubernetes. It allows you to create HTTP services
on Kubernetes from functions and can help make Kubernetes easier
to work with by allowing a user to create services without having
much knowledge Kubernetes itself. Fissions method of making things easier
for the user is to allow the majority of users to be able to work at the
source level. It can abstract away containers from the user. To use 
it, you create functions using a variety of languages and then add them
with a CLI tool. Functions are called when their trigger fires and they
only consume CPU and memory while they are running. Idle functions
consume no resources with the exception of storage. Some of the 
suggested uses for Fission are chatbots, webhooks, Rest APIs and 
Kubernetes events. The only languages supported for it right now
are NodeJS, PHP, Go, C\# and Python.


\begin{IU}

hid-sp18-521

\href{https://github.com/cloudmesh-community/hid-sp18-521/blob/master//technology/abstract-fission.tex}{abstract-fission.tex}

 

Wordcount: 142


Quote ratio: 0.00 \%
 
Max Line length: 73
\end{IU}

\section{AWS Greengrass}
\index{AWS Greengrass}

AWS Greengrass~\cite{hid-sp18-521-Greengrass} is a product that allows a
developer to create serverless code residing on AWS that can then be run
locally on your devices. Those devices are then able to act locally on the data
it creates while being able to utilize the cloud for handling the infrastructure. 
Devices can still communicate with each other even when unable to connect to the
Internet. Filters can also be added locally to each device allowing the user to be 
able to control what data is being sent back to the cloud. It can help users create
IoT solutions which allow connectivity between many different types of devices
and the cloud simultaneously. Greengrass supports many different programming 
languages. One of its core features is machine learning inference, which allows
a device to directly perform inference, or applying an already trained and 
optimized model to new data, before sending the appropriately filtered data 
back to the cloud.  


\begin{IU}

hid-sp18-521

\href{https://github.com/cloudmesh-community/hid-sp18-521/blob/master//technology/abstract-greengrass.tex}{abstract-greengrass.tex}

 

Wordcount: 151


Quote ratio: 0.00 \%
 
Max Line length: 83
\end{IU}

\section{Amazon SageMaker}
\index{Amazon SageMaker}

Amazon SageMaker~\cite{hid-sp18-521-Sagemaker} can help users
be able to develop machine learning models in a more streamlined way. It
includes functionality that allows a user to speed up the process of
building, training and deploying their models. To help simplify the 
building of ML models, Jupyter notebooks are included which will allow 
the user to make it more convenient to explore and visualize training 
data stored in S3. The 12 most common machine learning algorithms have 
also come pre-installed and configured, as well as the frameworks Tensorflow
and MXNet, with the option of also using your own specific framework. 
Using SageMaker to train your models allows you to scale up underlying 
infrastructure as needed up based on your storage needs. Automatic tuning
of models is also included. SageMaker also takes advantage of EC2 in order
to create highly available and elastic clusters where you can deploy your
model. A/B testing capabilities are built into the product as well. 


\begin{IU}

hid-sp18-521

\href{https://github.com/cloudmesh-community/hid-sp18-521/blob/master//technology/abstract-sagemaker.tex}{abstract-sagemaker.tex}

 

Wordcount: 153


Quote ratio: 0.00 \%
 
Max Line length: 76
\end{IU}

\section{Google Dremel}
\index{Google Dremel}


With vast amount of publicly available data over the internet/cloud, 
there was a need of technological system/framework that is deployed on 
cloud which can execute on demand queries in faster and scalable way 
for read only multi level nested data. Along with that a system that 
uses structured query language, which is widely adapted and extensively 
used by the developers for writing queries to avoid the learning curve of 
new language. To fill this gap Google came up with Dremel. It is a 
interactive ad hoc query system that lets the user query the large 
dataset providing them results with much faster speed compared to 
traditional technologies~\cite{hid-sp18-523-www-dremel}. \color{blue}``\emph{By combining 
multi-level execution trees and columnar data layout, it is capable of 
running aggregation queries over trillion-row tables in 
seconds}''\color{black}~\cite{hid-sp18-523-www-dremel}. \color{blue}``\emph{Dremel is capable of scaling 
up to thousands of CPUs and petabytes of data}''\color{black}~\cite{hid-sp18-523-www-dremel}.
MapReduce framework and technologies thar are built over it such as Pig, Hive 
etc has latency issue between running the job and getting output. 
Dremel on the other hand took a different approach, it uses execution engine
based on on aggregating trees algorithm that provides output almost realtime 
for queries.


\begin{IU}

hid-sp18-523

\href{https://github.com/cloudmesh-community/hid-sp18-523/blob/master//technology/abstract-dremel.tex}{abstract-dremel.tex}

 

Wordcount: 192


Quote ratio: 17.46 \%
 
Max Line length: 105
\end{IU}

\section{Google Genomics}
\index{Google Genomics}

With the size of medical data getting increased exponentially from 
petabytes to exabytes rapidly, Google came up with Google Genomics 
as extension to Google cloud platform.It helps the life science 
community organize the world’s genomic information and make it 
accessible and useful~\cite{hid-sp18-523-www-genomics}. 
Researchers are able to apply Google powerful technologies such as 
Google Search and Maps to securely store, process, explore, and share 
large, complex genomics datasets~\cite{hid-sp18-523-www-genomics}.
\color{blue}``\emph{Multiple genome repositories data can be processed using Google 
Genomics within seconds as it is backed by Google bigtable and 
Spanner technologies}''\color{black}~\cite{hid-sp18-523-www-genomics}. 
\color{blue}``\emph{It is based on open standard from Global Alliance of Genomics and 
Health achieving higher level of interoperability for genomics 
data}''\color{black}~\cite{hid-sp18-523-www-genomics}. It is fully integrated with 
Google cloud virtual machine, storage and SQL/NoSQL 
databases~\cite{hid-sp18-523-www-genomics}.\color{blue}``\emph{It helps analysing Genomic 
data in real-time with BigQuery, in literate programming style 
with Cloud Datalab, in batch with GATK on Google Genomics, with 
Apache Spark or Cloud Dataflow, or with a Grid Engine 
cluster}''\color{black}~\cite{hid-sp18-523-www-genomics}.


\begin{IU}

hid-sp18-523

\href{https://github.com/cloudmesh-community/hid-sp18-523/blob/master//technology/abstract-genomics.tex}{abstract-genomics.tex}

 

Wordcount: 158


Quote ratio: 37.58 \%

WARNING: Quote ratio very high
 
Max Line length: 90
\end{IU}

\section{H20}
\index{H2o}


\color{blue}``\emph{H20 is a open source platform that can execute highly advanced 
and complex machine learning algorithms in faster and scalable 
way,regardless of the size, format and location of the 
data}''\color{black}~\cite{hid-sp18-523-www-h2o}. \color{blue}``\emph{It achieves this by serializing 
the data faster between nodes and clusters that stores huge amount 
of data. Data processing is done in memory thus providing faster 
response}''\color{black}~\cite{hid-sp18-523-www-h2o}.\color{blue}``\emph{It uses fine grain parallelism 
technique for processing of distributed data archiving 100x faster 
speed as compared to traditional mapreduce without compromising 
on accuracy}''\color{black}~\cite{hid-sp18-523-www-h2o}. H2O4GPU, Sparking Water
and driverless AI are popular products of H2O. Many companies
across different domain such as banks, insurance, online sales 
are using H2O platform for their machine learning and AI related
research.







\begin{IU}

hid-sp18-523

\href{https://github.com/cloudmesh-community/hid-sp18-523/blob/master//technology/abstract-h2O.tex}{abstract-h2O.tex}

 

Wordcount: 112

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 57.24 \%

ERROR: Quote ratio too high
 
Max Line length: 104
\end{IU}

\section{Google Cloud Spanner}
\index{Google Spanner}

\color{blue}``\emph{Cloud Spanner is the enterprise-grade, globally-distributed database 
service built for the cloud}''\color{black}~\cite{hid-sp18-523-www-google-spanner}. 
This technology combines the benefits of relational database structure 
with non-relational horizontal scale. \color{blue}``\emph{This is unique combination that 
delivers high-performance transactions and strong consistency across rows, 
regions, and continents with high availability and enterprise-grade 
security}''\color{black}~\cite{hid-sp18-523-www-google-spanner}. Cloud Spanner 
revolutionizes database administration and management and makes 
application development more efficient~\cite{hid-sp18-523-www-google-spanner}.
\color{blue}``\emph{It is fully managed, can be easily deployed and has built in synchronous 
replication and maintenance feature}''\color{black}~\cite{hid-sp18-523-www-google-spanner}. 
It takes advantages of all critical features of relational database—such 
as schemas, ACID transactions, and SQL queries (ANSI 2011) thus reducing 
the need of high learning curve for developers who are well proficient in 
structured query language. 
\color{blue}``\emph{Client libraries that can connect to spanner is language independent. 
These libraries can be developed in C sharp, Go, Java, Node.js, PHP, Python, 
and Ruby. Already existing JDBC driver with popular third-party tools can be 
used to connect with spanner}''\color{black}~\cite{hid-sp18-523-www-google-spanner}. It is 
purposely built for global transactional consistency.


\begin{IU}

hid-sp18-523

\href{https://github.com/cloudmesh-community/hid-sp18-523/blob/master//technology/abstract-spanner.tex}{abstract-spanner.tex}

 

Wordcount: 164


Quote ratio: 43.38 \%

WARNING: Quote ratio very high
 
Max Line length: 93
\end{IU}

\section{Google Vision}
\index{Google Vision}

\color{blue}``\emph{Google Cloud Vision API has made the herculean task of correct 
labeling/classification of images simple. With exponential increase 
in different types of data including images, voice, video are 
transformed into digital form, stored and transmitted over network. 
There was a dire need of automated technology solution that can 
correctly classify / label images with high level of confidence; 
Google Vision API provides such platform to researchers and developers. 
It quickly classify images into thousands of predefined 
meaning ful categories}''\color{black}~\cite{hid-sp18-523-www-google-vision}. 
It does this by encapsulating powerful machine learning 
models (KNN, Regression) etc for classification of images. 
\color{blue}``\emph{It help detects objects and faces within images, finds and reads 
printed words contained within images through 
OCR}''\color{black}~\cite{hid-sp18-523-www-google-vision}. 
\color{blue}``\emph{Developers can build meta data on their image catalog, moderate 
offensive content, or launch new marketing campaign/scenarios through 
image sentiment analysis. It can be accessed 
through REST API}''\color{black}~\cite{hid-sp18-523-www-google-vision}.



\begin{IU}

hid-sp18-523

\href{https://github.com/cloudmesh-community/hid-sp18-523/blob/master//technology/abstract-vision.tex}{abstract-vision.tex}

 

Wordcount: 143


Quote ratio: 70.64 \%

ERROR: Quote ratio too high
 
Max Line length: 85
\end{IU}

\section{IBM Data Science Experience}
\index{IBM Data Science Experience}

IBM has compiled data science tools in one location, 
called Data Science Experience. This one location provides
access to the IBM Cloud, and allows a customer to run 
applications from the public or private cloud 
\cite{hid-sp18-525-dsx}.
Data Science Experience also allows desktop operations 
of popular tools\cite{hid-sp18-525-dsx}.
IBM’s Watson computing platform is available in Data Science
Experience. Machine learning models through Watson utilizing
its vast computing resources. Additionally, open source 
applications and technologies, such as Python, R, and Apache 
Spark give users a robust data science toolkit from which to
work\cite{hid-sp18-525-dsx}.
Every aspect of data science is available in Data Science 
Experience. Visual- izations of results are a part of Data 
Science Experience. Tools such as PixieDust and Brunel are 
available, without programming
experience\cite{hid-sp18-525-dsx}.



\begin{IU}

hid-sp18-525

\href{https://github.com/cloudmesh-community/hid-sp18-525/blob/master//technology/abstract-dsx.tex}{abstract-dsx.tex}

 

Wordcount: 121

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 61
\end{IU}

\section{IBM Cloud}
\index{IBM Cloud}


In 2017, IBM fully committed to cloud computing. IBM BlueMix
is now IBM Cloud. The changes go far beyond the name. The new 
platform gives IBM a new, singular way to engage customers 
\cite{hid-sp18-525-cloud}. Now, services are available on public or 
private clouds, with added capabilities, including database, 
artificial intelligence, and blockchain\cite{hid-sp18-525-cloud}.
In IBM Cloud, many popular services and applications are 
available, with public or private access No cloud presence 
is possible without a strong net- work. IBM has included an 
industry leading level of network capabilities. To ensure 
security, access, and redundancy, IBM operates 60 data centers 
\cite{hid-sp18-525-cloud}.
The IBM Cloud connects data science and other tools, such as 
VMware, SAP, Spark, Jupyter, R, and many others 
\cite{hid-sp18-525-cloud}. Both open source and proprietary 
applications and services are part of IBM Cloud.
As an industry leader in blockchain, IBM’s use of the 
technology is featured in IBM Cloud. Blockchain is becoming 
the most known product IBM offers, and it is a major componet
of IBM Cloud\cite{hid-sp18-525-cloud}.


\begin{IU}

hid-sp18-525

\href{https://github.com/cloudmesh-community/hid-sp18-525/blob/master//technology/abstract-ibmcloud.tex}{abstract-ibmcloud.tex}

 

Wordcount: 163


Quote ratio: 0.00 \%
 
Max Line length: 68
\end{IU}

\section{JavaScript}
\index{JavaScript}


JavaScript is a ubiquitous programming language with an
enormous number of uses. It is best known as the language
used to createweb pages\cite{hid-sp18-525-java}. However, its 
uses are far more than would appear. JavaScript is a flexible 
language that can function as object oriented, as well as 
procedural\cite{hid-sp18-525-java}. JavaScript has had a storied 
past\cite{hid-sp18-525-java}. Because of the universal adoption 
of JavaScript, it has a bright future. JavaScript is not 
confined to rendering web content. As an object-oriented 
language, JavaScript has unlimited potential as a 
programming platform\cite{hid-sp18-525-java}.


\begin{IU}

hid-sp18-525

\href{https://github.com/cloudmesh-community/hid-sp18-525/blob/master//technology/abstract-javascript.tex}{abstract-javascript.tex}

 

Wordcount: 83

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 65
\end{IU}

\section{Node.js}
\index{Node.js}


Node.js is a JavaScript runtime that provides a scalable 
option for net-work applications. Node.js requires less 
system resources to perform thanis common
\cite{hid-sp18-525-nodejs}. Using less system resources does 
allowNode.js to scale. This is ideal for large web 
applications, such as libraries\cite{hid-sp18-525-nodejs}. 
Generally, Node.js creates server-side applications 
\cite{hid-sp18-525-nodejs}. JavaScript language is used by 
Node.js, and the two often compliment each
other, on opposite sides of a transaction 
\cite{hid-sp18-525-nodejs}.


\begin{IU}

hid-sp18-525

\href{https://github.com/cloudmesh-community/hid-sp18-525/blob/master//technology/abstract-node.js.tex}{abstract-node.js.tex}

 

Wordcount: 65

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 61
\end{IU}

\section{IBM Watson}
\index{IBM Watson}
\index{Watson}

\subsection{Old: IBM Watson}

IBM Watson is a super computer built on cognitive technology that
processes information like the way human brain does by understanding
the data in a natural language as well as analyzing structured and
unstructured data~\cite{www-ibmwatson-wiki}.  It was initially
developed as a question and answer tool more specifically to answer
questions on the quiz show *Jeopardy* but now it has been seen as
helping doctors and nurses in the treatment of cancer. It was
developed by IBM's DeepQA research team led by David Ferrucci. With
Watson you can create bots that can engage in conversation with you
\cite{www-ibmwatson}. You can even provide personalized
recommendations to Watson by understanding a user's personality, tone
and emotion. Watson uses the Apache Hadoop framework in order to
process the large volume of data needed to generate an answer by
creating in-memory datasets used at run-time. Watson's DeepQA UIMA
s (Unstructured Information Management Architecture) annotators were
deployed as mappers in the Hadoop Map-Reduce framework. Watson is
written in multiple programming languages like Java, C++, Prolog and
it runs on the SUSE Linux Enterprise Server. Today Watson is available
as a set of open source APIs and Software As a Service product as
well\cite{www-ibmwatson}.


\subsection{New: IBM Watson}

IBM’s Watson computer, named after the company’s first CEO, Thomas
Watson, was created to fulfill an engineering challenge: to defeated
Jeapordy! champions\cite{hid-sp18-525-watson}. Although Watson defeated
for champions, that victory was years in the making.  Numerous
adjustments were required to make all 3 contestants equal.  For
example, Watson was required to push a button, like the other
contestants\cite{hid-sp18-525-watson}. Since the early days of Watson,
many advances in computing have happened. The explosion of machine
learning algorithms and artificial intelligence have greatly expanded
the use of Watson. Watson has developed into a mar- keting slogan, and
is even co-branded with companies, such as H\&R
Block\cite{hid-sp18-525-watson}. Advancements in hardware enabled the
revolution in data analysis that is currently happening. Super
computing, like Watson, along with enhanced al- gorithms have fueled
the revolution. Based on such advances, Watson is now a stand-alone
division at IBM\cite{hid-sp18-525-watson}. Being a separate division and
an individual brand gives IBM the ability to deploy Watson to a myriad
of activities. Watson is the hub of IBM’s cloud computing business.
Watson also forecasts the weather and conducts research from any
number of organizations\cite{hid-sp18-525-watson}.



\begin{IU}

hid-sp18-525

\href{https://github.com/cloudmesh-community/hid-sp18-525/blob/master//technology/abstract-watson.tex}{abstract-watson.tex}

 

Wordcount: 350


Quote ratio: 0.00 \%
 
Max Line length: 72
\end{IU}

\section{Apache Accumulo}
\index{Apache Accumulo}
\index{HDFS}

Based on Google's BigTable design, Apache has their own data store called
Accumulo\cite{hid-sp18-526-www-apache-accumulo}. Accumulo overlays the
Hadoop Distributed File System (HDFS) and Apache Zookeeper. Originally
created by the US National Security Agency, Accumulo has a large focus on
security and access control. Every key-value pair in Accumulo has its own
user restrictions. Accumulo is used mostly in other open source projects
and in other Apache projects such as Fluo, Gora, Hive, Pig, and Rya.

Accumulo is a distributed storage system for data, which is simpler than a
typical key-value pair system. Each record in Accumulo has the following
properties: \color{blue}``\emph{Key}''\color{black}, \color{blue}``\emph{Value}''\color{black}, \color{blue}``\emph{Row ID}''\color{black}, \color{blue}``\emph{Column}''\color{black}, \color{blue}``\emph{Timestamp}''\color{black},
\color{blue}``\emph{Family}''\color{black}, \color{blue}``\emph{Qualifier}''\color{black}, and \color{blue}``\emph{Visibility}''\color{black}. The records are stored across
many machines, with Accumolo keeping track of the properties. A monitor is
also provided for information on the current states of the system. A garbage
collector, tablet server (table partition manager), and tracer (for timing)
are also included as well as iterators for data management.



\begin{IU}

hid-sp18-526

\href{https://github.com/cloudmesh-community/hid-sp18-526/blob/master//technology/abstract-apache-accumulo.tex}{abstract-apache-accumulo.tex}

 

Wordcount: 147


Quote ratio: 11.19 \%
 
Max Line length: 230
\end{IU}

\section{Apache Phoenix}
\index{Apache Phoenix}
\index{OLTP}

Apache Phoenix\cite{hid-sp18-526-www-apache-phoenix} is an open-source
database engine by Apache that works in tandem with Hadoop and HBase. Because
it uses HBase, Phoenix is a noSQL store. The system supports online
transaction processing (OLTP). Using SQL and Java database connectivity
(JDBC), Phoenix allows for queries over millions of rows to be executed in
milliseconds. Phoenix overlays HBase and allows data to be accessed directly
via SQL queries (through JDBC). It allows for indexing and parallelization
to greatly reduce query time. Phoenix is compatible with the host of Hadoop
products such as Spark, Hive, and MapReduce. Phoenix allows table modifications
through DDL (Data Definition Language) commands. These commands use simple
SQL statements to create or alter tables. Phoenix also uses ACID (Atomicity,
Consistency, Isolation, Durability) transactions.

In order to begin using Phoenix, you must first install java and download
the Phoenix jar file. Phoenix is currently used by many large corporations
such as eBay, Salesforce, and Bloomberg.



\begin{IU}

hid-sp18-526

\href{https://github.com/cloudmesh-community/hid-sp18-526/blob/master//technology/abstract-apache-phoenix.tex}{abstract-apache-phoenix.tex}

 

Wordcount: 143


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{Fluentd}
\index{Fluentd}

Fluentd is a data collector used by many organizations such as Amazon,
Microsoft, and Google\cite{hid-sp18-526-www-fluentd}. It is open source
and available on GitHub. Fluentd creates a layer of abstraction between the
source of the data and backend, known as the \color{blue}``\emph{Unified Logging Layer}''\color{black}. This
centralized system of data collection ensures security and reliability. Logs
contain important information, but due to modern data sizes, they are no
longer for just human use. The purpose of the logging layer is to allow more
machine reading of logs as opposed to human reading.

In the logging layer, data is converted to json, then sent to the
backend. Fluentd has a plugin system for many different programs,
such as Python and Node.js. There are also custom versions of Fluentd in
production. Google, for example, uses their own version of fluentd as their
logging layer in conjuction with Google BigQuery. The Fluentd project also
includes \color{blue}``\emph{Fluent Bit}''\color{black} which is a data forwarding system.



\begin{IU}

hid-sp18-526

\href{https://github.com/cloudmesh-community/hid-sp18-526/blob/master//technology/abstract-fluentd.tex}{abstract-fluentd.tex}

 

Wordcount: 145


Quote ratio: 5.22 \%
 
Max Line length: 108
\end{IU}

\section{Synthea}
\index{Synthea}

Synthea~\cite{hid-sp18-526-www-synthea} is an open-source medical patient
generator. Synthea allows for the full synthetic generation of medical patients
and patient records, which solves the privacy problems of using real-world
patient data. It also allows medical researchers to generate data on-demand
and test scaling, stress, etc. Synthea uses a Generic Model Framework (GMF) to
model and track disease progression as well. Each patient generated by Synthea
is a full-model: from birth to present with full demographics. This type of
data can be used for small and large-scale health analysis. The data underlying
these models are generated based on current academic research. Therefore,
the data can also be used to run analysis on the synthetic patients.

According to its website, Synthea is useful for \color{blue}``\emph{Academic Research}''\color{black}, the
\color{blue}``\emph{Health IT Industry}''\color{black}, and \color{blue}``\emph{Policy Formation}''\color{black}. Synthea is a product
of MITRE Corporation written in Java, and supports both C-CDA and FHIR
formats. It can also generate graphs using Graphviz.



\begin{IU}

hid-sp18-526

\href{https://github.com/cloudmesh-community/hid-sp18-526/blob/master//technology/abstract-synthea.tex}{abstract-synthea.tex}

 

Wordcount: 141


Quote ratio: 7.73 \%
 
Max Line length: 134
\end{IU}

\section{Synthetic Data Vault}
\index{Synthetic Data Vault}

The most notable synthetic data generator is the Synthetic Data Vault
(SDV)\cite{hid-sp18-526-patki-sdv}. Developed at MIT by Neha Patki, 
Roy Wedge, and Kalyan Veeramachaneni, the Synthetic Data Vault uses 
machine learning techniques to model database structure and content. 
The models can then be used to generate entirely synthetic tables 
and relationships which are true to the form of the originals. 
Because the synthetic data is generated and modeled mathematically
according to the original data, very little, if any, insight is lost. Here
we will explore why we should use synthetic data, how SDV generates synthetic
data, and how to use the data generated by SDV.

SDV is written in Python and is, therefore, cross-platform. A separate file
is required for each database in the table. Also, each database requires a
configuration file in json format. The specifications for the configuration
file will be shown later, but first we will discuss why such a product is
necessary.


\begin{IU}

hid-sp18-526

\href{https://github.com/cloudmesh-community/hid-sp18-526/blob/master//technology/abstract-synthetic-data-vault.tex}{abstract-synthetic-data-vault.tex}

 

Wordcount: 150


Quote ratio: 0.00 \%
 
Max Line length: 77
\end{IU}

\section{Backblaze}
\index{Backblaze}

Backblaze is a cloud backup service~\cite{hid-sp18-601-www-bblaze-about}, 
providing solutions to business~\cite{hid-sp18-601-www-bblaze-business} and 
private users~\cite{hid-sp18-601-www-bblaze-personal}. Backblaze plans do not
have a limit for the amount of data you can backup and the software continuously 
uploads data present on your device. 

If the backup is ever needed it can be downloaded. For cases were the amount of 
data is to much to be downloaded Backblaze offers a service to mail a encripted 
flash drive or hard drive instead.

\begin{IU}

hid-sp18-601

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/abstract-backblaze.tex}{abstract-backblaze.tex}

 

Wordcount: 74

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 81
\end{IU}

\section{Google Docs}
\index{GoogleDocs}

Google Docs is a free office suite offered by 
Google~\cite{hid-sp18-601-www-gdoc-about}. It is a part of Google 
Drive, therefore cloud syncing is native. Google Docs is a collaborative tool
for creating and editing documents in real time. It allows for multiple users
to edit the same file. While it has apps in Android and iOS it only has PC
programs on Chrome Books.

\begin{IU}

hid-sp18-601

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/abstract-google-docs.tex}{abstract-google-docs.tex}

 

Wordcount: 60

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 77
\end{IU}

\section{OneDrive}
\index{OneDrive}

Microsoft's OneDrive is a data storage service 
~\cite{hid-sp18-601-www-odrive-website}. It allows accessibility  to files
 stored in any computer connected to the web. OneDrive has sharing functions and
 integration with Microsoft Office. It's intended audience is both personal 
users and companies.

\begin{IU}

hid-sp18-601

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/abstract-onedrive.tex}{abstract-onedrive.tex}

 

Wordcount: 40

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{ShareLatex}
\index{ShareLatex}

ShareLatex is a cloud service accesable via a website. It allows real time 
collaboration and compilation of 
\LaTeX~documents~\cite{hid-sp18-601-www-slatex-documentation} as well as the 
storage of them. 
 
ShareLatex has ease of use features, such as having several packages included 
in it server side. It provides several templates for presentations, papers among
 others. ShareLatex provides paid accounts as well as the free one. With the 
 paid account you can see a history of the document and sinc the files to github
 or DropBox~\cite{hid-sp18-601-www-slatex-plans}.


\begin{IU}

hid-sp18-601

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/abstract-sharelatex.tex}{abstract-sharelatex.tex}

 

Wordcount: 82

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 80
\end{IU}

\section{Share Point}
\index{SharePoint}

Sharepoint is a web-based document management and storage system platform that 
integrates with Microsoft Office~\cite{hid-sp18-601-www-spoint-website}. It was
 launched in 2001 by Microsoft, now it 
has different editions with different functions.

Sharepoint allows for a few different applications. It can be used as a real 
time collaboration tool for Microsoft Office documents. Also providing a 
file history and keeping records. Sharepoint also provides a Social Network 
~\cite{hid-sp18-601-www-spoint-new-sharepoint}, 
helping to centralize project management. It is integrated with Microsoft's 
OneDrive, allowing for mobility.

\begin{IU}

hid-sp18-601

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/abstract-sharepoint.tex}{abstract-sharepoint.tex}

 

Wordcount: 80

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 79
\end{IU}

\section{AWS CodeStar}
\index{AWS CodeStar}

AWS CodeStar is a developer tool used to develop projects and easily deploy on 
AWS cloud. It includes all of the tools and services needed for a project development.    
It supports various templates to set up projects using AWS Lambda, Amazon EC2, or AWS Elastic Beanstalk 
and IDE platforms such as AWS Cloud9 Eclipse,Visual Studio, CLT.It comes pre-configured with a 
project management dashboard, an automated continuous delivery pipeline.  

Additionally, AWS CodeStar integrates with Atlassian JIRA Software to 
provide project management and issue tracking system for software project 
team directly from the AWS CodeStar console\cite{hid-sp18-602-www-awscodestar-blog}.



\begin{IU}

hid-sp18-602

\href{https://github.com/cloudmesh-community/hid-sp18-602/blob/master//technology/abstract-awscodestar.tex}{abstract-awscodestar.tex}

 

Wordcount: 100

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 104
\end{IU}

\section{Cloud AutoML}
\index{Cloud AutoML}

Cloud AutoML is an innovative tool with simple graphical user interface to train
and test users custom machine learning models~\cite{hid-sp18-602-www-cloudautoml-main}.
And these models can be directly used from Google cloud via REST API. 

The main purpose of developing Cloud AutoML is to enable users with limited 
machine learning expertise to train high quality ML models. It is built on Google
learning to learn, transfer learning, and Neural Architecture Search 
technologies.

Google has recently launched first product under Cloud AutoML: AutoML Vision 
which is a service to access a pre-trained model or create a custom ML models using
Cloud ML Engine, for image recognition. It offers drag-and-drop interface to 
upload images, train and manage models, and then deploy those trained models 
directly on Google Cloud. For instance, Disney and Zoological Society of London are
actively using AutoML Vision~\cite{hid-sp18-602-wwww-cloudautoml-blog}.



\begin{IU}

hid-sp18-602

\href{https://github.com/cloudmesh-community/hid-sp18-602/blob/master//technology/abstract-cloudautoml.tex}{abstract-cloudautoml.tex}

 

Wordcount: 133


Quote ratio: 0.00 \%
 
Max Line length: 87
\end{IU}

\section{Google Load Balancing}
\index{Google Load Balancing}

Google Cloud Load Balancing is a high performance, scalable load balancing, 
which has state of the art auto scaling feature which distributes traffic 
intelligetly such that application still have resources in spite of huge increase 
in traffic and it does not require pre-warming, as it quickly reaches 
from zero to full-throttle. 
Google Load balancing support different flavors such as HTTP,TCP/SSL and UDP 
Load Balancing\cite{hid-sp18-602-www-loadbalancing-main}. Also the new UI enables users to integrate any of these flavors 
easily through a single interface.



\begin{IU}

hid-sp18-602

\href{https://github.com/cloudmesh-community/hid-sp18-602/blob/master//technology/abstract-googleloadbalancing.tex}{abstract-googleloadbalancing.tex}

 

Wordcount: 85

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 122
\end{IU}

\section{Google Stackdriver}
\index{Google Stackdriver}

With increasing cloud-based applications it is hard for Devops engineers to keep
track of performance, availability and issues associated with these applications.
Google Stackdriver is a powerful service for monitoring, logging, and diagnostics. 
It support applications deployed on Google Cloud Platform,Amazon Web Services, and both combined
\cite{hid-sp18-602-www-google-stackdriver-main}.

Stackdriver provides a wide variety of features such metrics, dashboards, 
alerting, log management, reporting, and tracing capabilities, which 
ultimately enables users to find and fix issues faster\cite{hid-sp18-602-www-google-stackdriver-main}. 


\begin{IU}

hid-sp18-602

\href{https://github.com/cloudmesh-community/hid-sp18-602/blob/master//technology/abstract-googlestackdriver.tex}{abstract-googlestackdriver.tex}

 

Wordcount: 73

ERROR: This abstract is too short.


Quote ratio: 0.00 \%
 
Max Line length: 103
\end{IU}

\section{Spinnaker}
\index{Spinnaker}

Spinnaker is an open source, multi-cloud continuous delivery platform
that helps you release software updates with high velocity and
confidence.  It provides two core features: cluster management to view
and manage your resources in the cloud and deployment management to
construct and manage continuous delivery
workflows~\cite{hid-sp18-602-www-spinnaker-io}. The main advantage of
Spinnaker is it holds a modern software development concept of
continuous delivery that is application updates should be delivered
when they are ready, instead of on a fixed schedule.  Also, it
improves the speed, stability of application deployment processes
along with supporting deployments across different platforms by
several different cloud providers.

Although the project Spinnaker first started out with Netflix and then
google joined in 2014, the Spinnaker community now includes dozens of
organizations such as Microsoft, Oracle, Target, Veritas,
Schibsted, Armory and Kenzan~\cite{hid-sp18-602-www-spinnaker-gc}.



\begin{IU}

hid-sp18-602

\href{https://github.com/cloudmesh-community/hid-sp18-602/blob/master//technology/abstract-spinnaker.tex}{abstract-spinnaker.tex}

 

Wordcount: 121

WARNING: Short Abstract: Is there enough information for a reader to understand what it is?


Quote ratio: 0.00 \%
 
Max Line length: 70
\end{IU}

\part{Biographies}
\chapter{Volume Contributors}
\input{../hid-sp18-203/bio-clarke-jack.tex}

\begin{IU}

hid-sp18-203

\url{https://github.com/cloudmesh-community/hid-sp18-203}

\url{https://github.com/cloudmesh-community/hid-sp18-203/issues}

hid-sp18-203.bib is missing

\end{IU}
\input{../hid-sp18-204/bio-Gruenberg-Max.tex}

\begin{IU}

hid-sp18-204

\url{https://github.com/cloudmesh-community/hid-sp18-204}

\url{https://github.com/cloudmesh-community/hid-sp18-204/issues}

hid-sp18-204.bib is missing

\end{IU}
\input{../hid-sp18-206/bio-mhatre-krish-hemant.tex}

\begin{IU}

hid-sp18-206

\url{https://github.com/cloudmesh-community/hid-sp18-206}

\url{https://github.com/cloudmesh-community/hid-sp18-206/issues}

hid-sp18-206.bib is missing

\end{IU}
\input{../hid-sp18-208/bio-fanbo-sun.tex}

\begin{IU}

hid-sp18-208

\url{https://github.com/cloudmesh-community/hid-sp18-208}

\url{https://github.com/cloudmesh-community/hid-sp18-208/issues}

hid-sp18-208.bib is missing

\end{IU}
\input{../hid-sp18-209/bio-tugman-anthony.tex}

\begin{IU}

hid-sp18-209

\url{https://github.com/cloudmesh-community/hid-sp18-209}

\url{https://github.com/cloudmesh-community/hid-sp18-209/issues}

hid-sp18-209.bib is missing

\end{IU}
\input{../hid-sp18-210/bio-whelan-aidan.tex}

\begin{IU}

hid-sp18-210

\url{https://github.com/cloudmesh-community/hid-sp18-210}

\url{https://github.com/cloudmesh-community/hid-sp18-210/issues}

hid-sp18-210.bib is missing

\end{IU}
\input{../hid-sp18-401/bio-arra-goutham.tex}

\begin{IU}

hid-sp18-401

\url{https://github.com/cloudmesh-community/hid-sp18-401}

\url{https://github.com/cloudmesh-community/hid-sp18-401/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-401/blob/master//technology/abstract-lightgbm.tex}{abstract-lightgbm.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-401/blob/master//technology/abstract-keras.tex}{abstract-keras.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-401/blob/master//technology/abstract-xgboost.tex}{abstract-xgboost.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-401/blob/master//technology/abstract-apache-impala.tex}{abstract-apache-impala.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-401/blob/master//technology/abstract-apache-ambari.tex}{abstract-apache-ambari.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-401/blob/master//technology/hid-sp18-401.bib}{../hid-sp18-401/technology/hid-sp18-401.bib}

\end{IU}
\input{../hid-sp18-402/bio-athaley-sushant.tex}

\begin{IU}

hid-sp18-402

\url{https://github.com/cloudmesh-community/hid-sp18-402}

\url{https://github.com/cloudmesh-community/hid-sp18-402/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-402/blob/master//technology/abstract-apatar.tex}{abstract-apatar.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-402/blob/master//technology/abstract-bluemix.tex}{abstract-bluemix.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-402/blob/master//technology/abstract-gephi.tex}{abstract-gephi.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-402/blob/master//technology/abstract-dokku.tex}{abstract-dokku.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-402/blob/master//technology/abstract-bmc.tex}{abstract-bmc.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-402/blob/master//technology/hid-sp18-402.bib}{../hid-sp18-402/technology/hid-sp18-402.bib}

\end{IU}
\input{../hid-sp18-403/bio-axthelm-alex.tex}

\begin{IU}

hid-sp18-403

\url{https://github.com/cloudmesh-community/hid-sp18-403}

\url{https://github.com/cloudmesh-community/hid-sp18-403/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-403/blob/master//technology/abstract-odbc.tex}{abstract-odbc.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-403/blob/master//technology/abstract-drake.tex}{abstract-drake.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-403/blob/master//technology/abstract-pool.tex}{abstract-pool.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-403/blob/master//technology/abstract-dbi.tex}{abstract-dbi.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-403/blob/master//technology/abstract-dbplyr.tex}{abstract-dbplyr.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-403/blob/master//technology/hid-sp18-403.bib}{../hid-sp18-403/technology/hid-sp18-403.bib}

\end{IU}
\input{../hid-sp18-404/bio-carmickle-rick.tex}

\begin{IU}

hid-sp18-404

\url{https://github.com/cloudmesh-community/hid-sp18-404}

\url{https://github.com/cloudmesh-community/hid-sp18-404/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-404/blob/master//technology/abstract-pivotal.tex}{abstract-pivotal.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-404/blob/master//technology/abstract-apachedrill.tex}{abstract-apachedrill.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-404/blob/master//technology/abstract-caffe.tex}{abstract-caffe.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-404/blob/master//technology/abstract-mesosphere.tex}{abstract-mesosphere.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-404/blob/master//technology/abstract-apachemesos.tex}{abstract-apachemesos.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-404/blob/master//technology/hid-sp18-404.bib}{../hid-sp18-404/technology/hid-sp18-404.bib}

\end{IU}
\input{../hid-sp18-405/bio-Chen-Min.tex}

\begin{IU}

hid-sp18-405

\url{https://github.com/cloudmesh-community/hid-sp18-405}

\url{https://github.com/cloudmesh-community/hid-sp18-405/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-405/blob/master//technology/abstract-nifi.tex}{abstract-nifi.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-405/blob/master//technology/abstract-lumify.tex}{abstract-lumify.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-405/blob/master//technology/abstract-stardog.tex}{abstract-stardog.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-405/blob/master//technology/abstract-samoa.tex}{abstract-samoa.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-405/blob/master//technology/abstract-linkedinwherehows.tex}{abstract-linkedinwherehows.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-405/blob/master//technology/hid-sp18-405.bib}{../hid-sp18-405/technology/hid-sp18-405.bib}

\end{IU}
\input{../hid-sp18-406/bio-dg-ramyashree.tex}

\begin{IU}

hid-sp18-406

\url{https://github.com/cloudmesh-community/hid-sp18-406}

\url{https://github.com/cloudmesh-community/hid-sp18-406/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-406/blob/master//technology/abstract-mlab.tex}{abstract-mlab.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-406/blob/master//technology/abstract-rackspace.tex}{abstract-rackspace.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-406/blob/master//technology/abstract-alibabacloud.tex}{abstract-alibabacloud.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-406/blob/master//technology/abstract-owncloud.tex}{abstract-owncloud.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-406/blob/master//technology/abstract-twilio.tex}{abstract-twilio.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-406/blob/master//technology/hid-sp18-406.bib}{../hid-sp18-406/technology/hid-sp18-406.bib}

 ERROR: Citation Label wrong: \verb| mLab |

\end{IU}
\input{../hid-sp18-407/bio-hickman-keith.tex}

\begin{IU}

hid-sp18-407

\url{https://github.com/cloudmesh-community/hid-sp18-407}

\url{https://github.com/cloudmesh-community/hid-sp18-407/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-407/blob/master//technology/hid-sp18-407.bib}{../hid-sp18-407/technology/hid-sp18-407.bib}

\end{IU}
\input{../hid-sp18-408/bio-Joshi-Manoj.tex}

\begin{IU}

hid-sp18-408

\url{https://github.com/cloudmesh-community/hid-sp18-408}

\url{https://github.com/cloudmesh-community/hid-sp18-408/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-408/blob/master//technology/abstract-Jelastic.tex}{abstract-Jelastic.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-408/blob/master//technology/abstract-IBMBigReplicate.tex}{abstract-IBMBigReplicate.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-408/blob/master//technology/abstract-IBMDb2BigSql.tex}{abstract-IBMDb2BigSql.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-408/blob/master//technology/abstract-TeradataIntelliBase.tex}{abstract-TeradataIntelliBase.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-408/blob/master//technology/abstract-TeradataKylo.tex}{abstract-TeradataKylo.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-408/blob/master//technology/hid-sp18-408.bib}{../hid-sp18-408/technology/hid-sp18-408.bib}

\end{IU}
\input{../hid-sp18-409/bio-kadupitige-kadupitiya.tex}

\begin{IU}

hid-sp18-409

\url{https://github.com/cloudmesh-community/hid-sp18-409}

\url{https://github.com/cloudmesh-community/hid-sp18-409/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-409/blob/master//technology/abstract-databricks.tex}{abstract-databricks.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-409/blob/master//technology/abstract-instabug.tex}{abstract-instabug.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-409/blob/master//technology/abstract-firepad.tex}{abstract-firepad.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-409/blob/master//technology/abstract-pubnub.tex}{abstract-pubnub.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-409/blob/master//technology/abstract-firebase.tex}{abstract-firebase.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-409/blob/master//technology/hid-sp18-409.bib}{../hid-sp18-409/technology/hid-sp18-409.bib}

\end{IU}
\input{../hid-sp18-410/bio-kamatgi-karan.tex}

\begin{IU}

hid-sp18-410

\url{https://github.com/cloudmesh-community/hid-sp18-410}

\url{https://github.com/cloudmesh-community/hid-sp18-410/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-410/blob/master//technology/abstract-Cloudlet.tex}{abstract-Cloudlet.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-410/blob/master//technology/abstract-ELKStack.tex}{abstract-ELKStack.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-410/blob/master//technology/abstract-AmazonRedshift.tex}{abstract-AmazonRedshift.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-410/blob/master//technology/abstract-IntelCloudFinder.tex}{abstract-IntelCloudFinder.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-410/blob/master//technology/abstract-EdgeComputing.tex}{abstract-EdgeComputing.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-410/blob/master//technology/hid-sp18-410.bib}{../hid-sp18-410/technology/hid-sp18-410.bib}

\end{IU}
\input{../hid-sp18-411/bio-kaveripakam-venkateshaditya.tex}

\begin{IU}

hid-sp18-411

\url{https://github.com/cloudmesh-community/hid-sp18-411}

\url{https://github.com/cloudmesh-community/hid-sp18-411/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-411/blob/master//technology/abstract-apachemahout.tex}{abstract-apachemahout.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-411/blob/master//technology/abstract-amazonelasticbeanstalk.tex}{abstract-amazonelasticbeanstalk.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-411/blob/master//technology/abstract-googlefirebase.tex}{abstract-googlefirebase.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-411/blob/master//technology/abstract-apachekylin.tex}{abstract-apachekylin.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-411/blob/master//technology/abstract-skytap.tex}{abstract-skytap.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-411/blob/master//technology/hid-sp18-411.bib}{../hid-sp18-411/technology/hid-sp18-411.bib}

Error: you did not use howpublished = \{Web Page\},

\end{IU}
\input{../hid-sp18-412/bio-Karan-Kotabagi.tex}

\begin{IU}

hid-sp18-412

\url{https://github.com/cloudmesh-community/hid-sp18-412}

\url{https://github.com/cloudmesh-community/hid-sp18-412/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-ApacheAtlas.tex}{abstract-ApacheAtlas.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-zmanda.tex}{abstract-zmanda.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-orientDB.tex}{abstract-orientDB.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-talend.tex}{abstract-talend.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-appfog.tex}{abstract-appfog.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/abstract-appscale.tex}{abstract-appscale.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-412/blob/master//technology/hid-sp18-412.bib}{../hid-sp18-412/technology/hid-sp18-412.bib}

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-Apache_Atlas_by_Maven |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-Apache_Atlas_architecture |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-talend_open_studio |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-talend_products |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-zmanda_crunchbase |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-zmanda_amanda |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-zmanda_webinar |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-wiki_appscale |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-git_appscale |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-century_link_appfog |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-orientDB_by_CallidusCloud |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-orientDB_multimodel |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-orientDB_graph |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-412-orientDB_wiki |

\end{IU}
\input{../hid-sp18-413/bio-lavania-anubhav.tex}

\begin{IU}

hid-sp18-413

\url{https://github.com/cloudmesh-community/hid-sp18-413}

\url{https://github.com/cloudmesh-community/hid-sp18-413/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/abstract-zeppelin.tex}{abstract-zeppelin.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/abstract-clojure.tex}{abstract-clojure.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/abstract-paxata.tex}{abstract-paxata.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/abstract-puppet.tex}{abstract-puppet.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/abstract-weka.tex}{abstract-weka.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/abstract-logicalglue.tex}{abstract-logicalglue.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-413/blob/master//technology/hid-sp18-413.bib}{../hid-sp18-413/technology/hid-sp18-413.bib}

\end{IU}
\input{../hid-sp18-415/bio-mudvari-khatiwada-janaki.tex}

\begin{IU}

hid-sp18-415

\url{https://github.com/cloudmesh-community/hid-sp18-415}

\url{https://github.com/cloudmesh-community/hid-sp18-415/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-415/blob/master//technology/abstract-googlecomputeengine.tex}{abstract-googlecomputeengine.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-415/blob/master//technology/abstract-heroku.tex}{abstract-heroku.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-415/blob/master//technology/abstract-pivotalrabbitmq.tex}{abstract-pivotalrabbitmq.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-415/blob/master//technology/abstract-googleappengine.tex}{abstract-googleappengine.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-415/blob/master//technology/abstract-mvs.tex}{abstract-mvs.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-415/blob/master//technology/hid-sp18-415.bib}{../hid-sp18-415/technology/hid-sp18-415.bib}

\end{IU}
\input{../hid-sp18-416/bio-Ossen-Sabra.tex}

\begin{IU}

hid-sp18-416

\url{https://github.com/cloudmesh-community/hid-sp18-416}

\url{https://github.com/cloudmesh-community/hid-sp18-416/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-416/blob/master//technology/abstract-morpheus.tex}{abstract-morpheus.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-416/blob/master//technology/abstract-monetdb.tex}{abstract-monetdb.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-416/blob/master//technology/abstract-arangodb.tex}{abstract-arangodb.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-416/blob/master//technology/abstract-wso2analytics.tex}{abstract-wso2analytics.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-416/blob/master//technology/abstract-druid.tex}{abstract-druid.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-416/blob/master//technology/hid-sp18-416.bib}{../hid-sp18-416/technology/hid-sp18-416.bib}

\end{IU}
\input{../hid-sp18-417/bio-ray-rashmi.tex}

\begin{IU}

hid-sp18-417

\url{https://github.com/cloudmesh-community/hid-sp18-417}

\url{https://github.com/cloudmesh-community/hid-sp18-417/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-417/blob/master//technology/abstract-opennebula.tex}{abstract-opennebula.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-417/blob/master//technology/abstract-ansible.tex}{abstract-ansible.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-417/blob/master//technology/abstract-apachedeltacloud.tex}{abstract-apachedeltacloud.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-417/blob/master//technology/abstract-openrefine.tex}{abstract-openrefine.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-417/blob/master//technology/abstract-apachecloudstack.tex}{abstract-apachecloudstack.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-417/blob/master//technology/hid-sp18-417.bib}{../hid-sp18-417/technology/hid-sp18-417.bib}

 ERROR: Citation Label wrong: \verb|  hid-sp18-417-wiki-deltacloud |

\end{IU}
\input{../hid-sp18-418/bio-sekar-suryaprakash.tex}

\begin{IU}

hid-sp18-418

\url{https://github.com/cloudmesh-community/hid-sp18-418}

\url{https://github.com/cloudmesh-community/hid-sp18-418/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-418/blob/master//technology/abstract-rightscale-cloud-management.tex}{abstract-rightscale-cloud-management.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-418/blob/master//technology/abstract-sales-cloud.tex}{abstract-sales-cloud.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-418/blob/master//technology/abstract-atomsphere.tex}{abstract-atomsphere.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-418/blob/master//technology/abstract-cloudhub.tex}{abstract-cloudhub.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-418/blob/master//technology/abstract-teradata-intelliflex.tex}{abstract-teradata-intelliflex.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-418/blob/master//technology/hid-sp18-418.bib}{../hid-sp18-418/technology/hid-sp18-418.bib}

Error: you did not use howpublished = \{Web Page\},

\end{IU}
\input{../hid-sp18-419/bio-sobolik-bertolt.tex}

\begin{IU}

hid-sp18-419

\url{https://github.com/cloudmesh-community/hid-sp18-419}

\url{https://github.com/cloudmesh-community/hid-sp18-419/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-419/blob/master//technology/abstract-clive.tex}{abstract-clive.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-419/blob/master//technology/abstract-neptune.tex}{abstract-neptune.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-419/blob/master//technology/abstract-opendaylight.tex}{abstract-opendaylight.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-419/blob/master//technology/abstract-hcatalog.tex}{abstract-hcatalog.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-419/blob/master//technology/abstract-pig.tex}{abstract-pig.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-419/blob/master//technology/hid-sp18-419.bib}{../hid-sp18-419/technology/hid-sp18-419.bib}

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-419-www-tc_neptune |

\end{IU}
\input{../hid-sp18-420/bio-sowani-swarnima.tex}

\begin{IU}

hid-sp18-420

\url{https://github.com/cloudmesh-community/hid-sp18-420}

\url{https://github.com/cloudmesh-community/hid-sp18-420/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-420/blob/master//technology/abstract-AmazonS3.tex}{abstract-AmazonS3.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-420/blob/master//technology/abstract-AmazonGlacier.tex}{abstract-AmazonGlacier.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-420/blob/master//technology/abstract-AmazonElasticBeanstalk.tex}{abstract-AmazonElasticBeanstalk.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-420/blob/master//technology/abstract-AmazonRDS.tex}{abstract-AmazonRDS.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-420/blob/master//technology/abstract-PostgreSql.tex}{abstract-PostgreSql.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-420/blob/master//technology/hid-sp18-420.bib}{../hid-sp18-420/technology/hid-sp18-420.bib}

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-420-PostgreSQL_About |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-420-PostgreSQL_Wiki |

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-420-PostgreSQL_History |

\end{IU}
\input{../hid-sp18-421/bio-Vijjigiri-Priyadarshini.tex}

\begin{IU}

hid-sp18-421

\url{https://github.com/cloudmesh-community/hid-sp18-421}

\url{https://github.com/cloudmesh-community/hid-sp18-421/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-421/blob/master//technology/abstract-ApacheAvro.tex}{abstract-ApacheAvro.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-421/blob/master//technology/abstract-ApacheWhirr.tex}{abstract-ApacheWhirr.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-421/blob/master//technology/abstract-ApacheZookeeper.tex}{abstract-ApacheZookeeper.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-421/blob/master//technology/abstract-ApacheChukwa.tex}{abstract-ApacheChukwa.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-421/blob/master//technology/abstract-BigTop.tex}{abstract-BigTop.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-421/blob/master//technology/hid-sp18-421.bib}{../hid-sp18-421/technology/hid-sp18-421.bib}

\end{IU}
\input{../hid-sp18-501/bio-tolu-agunbiade.tex}

\begin{IU}

hid-sp18-501

\url{https://github.com/cloudmesh-community/hid-sp18-501}

\url{https://github.com/cloudmesh-community/hid-sp18-501/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-501/blob/master//technology/abstract-google-bigquery.tex}{abstract-google-bigquery.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-501/blob/master//technology/abstract-azure-cosmosdb.tex}{abstract-azure-cosmosdb.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-501/blob/master//technology/abstract-apache-ignite.tex}{abstract-apache-ignite.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-501/blob/master//technology/abstract-oracle-cloud-machine.tex}{abstract-oracle-cloud-machine.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-501/blob/master//technology/abstract-azure-blob-storage.tex}{abstract-azure-blob-storage.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-501/blob/master//technology/hid-sp18-501.bib}{../hid-sp18-501/technology/hid-sp18-501.bib}

 ERROR: Citation Label wrong: \verb| CosmosDB |

 ERROR: Citation Label wrong: \verb| GoogleCP_BQ |

 ERROR: No underscore allowed in citation lables: \verb| GoogleCP_BQ |

 ERROR: Citation Label wrong: \verb| OracleCloud |

 ERROR: Citation Label wrong: \verb| RedGate |

 ERROR: Citation Label wrong: \verb| Stackify |

 ERROR: Citation Label wrong: \verb| TechTarget |

 ERROR: Citation Label wrong: \verb| ApacheIgnite |

 ERROR: Citation Label wrong: \verb| GridGain |

 ERROR: Citation Label wrong: \verb| ApacheIgnite |

\end{IU}
\input{../hid-sp18-502/bio-alshi-ankita.tex}

\begin{IU}

hid-sp18-502

\url{https://github.com/cloudmesh-community/hid-sp18-502}

\url{https://github.com/cloudmesh-community/hid-sp18-502/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-502/blob/master//technology/abstract-talend.tex}{abstract-talend.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-502/blob/master//technology/abstract-presto.tex}{abstract-presto.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-502/blob/master//technology/abstract-apachecouchdb.tex}{abstract-apachecouchdb.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-502/blob/master//technology/abstract-rapidminer.tex}{abstract-rapidminer.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-502/blob/master//technology/abstract-googlecloudbigtable.tex}{abstract-googlecloudbigtable.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-502/blob/master//technology/hid-sp18-502.bib}{../hid-sp18-502/technology/hid-sp18-502.bib}

\end{IU}
\input{../hid-sp18-503/bio-arnav-arnav.tex}

\begin{IU}

hid-sp18-503

\url{https://github.com/cloudmesh-community/hid-sp18-503}

\url{https://github.com/cloudmesh-community/hid-sp18-503/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-503/blob/master//technology/abstract-edgent.tex}{abstract-edgent.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-503/blob/master//technology/abstract-gobblin.tex}{abstract-gobblin.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-503/blob/master//technology/abstract-gossip.tex}{abstract-gossip.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-503/blob/master//technology/abstract-milagro.tex}{abstract-milagro.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-503/blob/master//technology/abstract-carbondata.tex}{abstract-carbondata.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-503/blob/master//technology/hid-sp18-503.bib}{../hid-sp18-503/technology/hid-sp18-503.bib}

\end{IU}
\input{../hid-sp18-504/bio-moeen-arshad.tex}

\begin{IU}

hid-sp18-504

\url{https://github.com/cloudmesh-community/hid-sp18-504}

\url{https://github.com/cloudmesh-community/hid-sp18-504/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-504/blob/master//technology/abstract-datalab.tex}{abstract-datalab.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-504/blob/master//technology/abstract-bigml.tex}{abstract-bigml.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-504/blob/master//technology/abstract-dmtk.tex}{abstract-dmtk.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-504/blob/master//technology/abstract-orange.tex}{abstract-orange.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-504/blob/master//technology/abstract-knime.tex}{abstract-knime.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-504/blob/master//technology/hid-sp18-504.bib}{../hid-sp18-504/technology/hid-sp18-504.bib}

\end{IU}
\input{../hid-sp18-505/bio-cate-averill.tex}

\begin{IU}

hid-sp18-505

\url{https://github.com/cloudmesh-community/hid-sp18-505}

\url{https://github.com/cloudmesh-community/hid-sp18-505/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-netflix.tex}{abstract-netflix.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-graphql.tex}{abstract-graphql.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-esri-data-services.tex}{abstract-esri-data-services.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-github-developer.tex}{abstract-github-developer.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-united-states-consumer-financial-protection-bureau.tex}{abstract-united-states-consumer-financial-protection-bureau.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-world-bank-open-data.tex}{abstract-world-bank-open-data.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/abstract-mapbox.tex}{abstract-mapbox.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-505/blob/master//technology/hid-sp18-505.bib}{../hid-sp18-505/technology/hid-sp18-505.bib}

\end{IU}
\input{../hid-sp18-506/bio-esteban-orly.tex}

\begin{IU}

hid-sp18-506

\url{https://github.com/cloudmesh-community/hid-sp18-506}

\url{https://github.com/cloudmesh-community/hid-sp18-506/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-506/blob/master//technology/hid-sp18-506.bib}{../hid-sp18-506/technology/hid-sp18-506.bib}

\end{IU}
\input{../hid-sp18-507/bio-giuliani-stephen.tex}

\begin{IU}

hid-sp18-507

\url{https://github.com/cloudmesh-community/hid-sp18-507}

\url{https://github.com/cloudmesh-community/hid-sp18-507/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-507/blob/master//technology/abstract-jmp.tex}{abstract-jmp.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-507/blob/master//technology/abstract-elasticsearch.tex}{abstract-elasticsearch.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-507/blob/master//technology/abstract-openchain.tex}{abstract-openchain.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-507/blob/master//technology/abstract-cascading.tex}{abstract-cascading.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-507/blob/master//technology/abstract-scribe.tex}{abstract-scribe.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-507/blob/master//technology/hid-sp18-507.bib}{../hid-sp18-507/technology/hid-sp18-507.bib}

\end{IU}
\input{../hid-sp18-508/bio-guo-yue.tex}

\begin{IU}

hid-sp18-508

\url{https://github.com/cloudmesh-community/hid-sp18-508}

\url{https://github.com/cloudmesh-community/hid-sp18-508/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-508/blob/master//technology/abstract-azureblas.tex}{abstract-azureblas.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-508/blob/master//technology/abstract-futuregrid.tex}{abstract-futuregrid.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-508/blob/master//technology/abstract-openvz.tex}{abstract-openvz.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-508/blob/master//technology/abstract-sqlite.tex}{abstract-sqlite.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-508/blob/master//technology/abstract-amazonec2.tex}{abstract-amazonec2.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-508/blob/master//technology/hid-sp18-508.bib}{../hid-sp18-508/technology/hid-sp18-508.bib}

 ERROR: Citation Label wrong: \verb| www-openvz |

 ERROR: Citation Label wrong: \verb| www-openvz-intro |

 ERROR: Citation Label wrong: \verb| www-sqlite |

 ERROR: Citation Label wrong: \verb| www-sqlite-archi |

 ERROR: Citation Label wrong: \verb| www-amazonec2 |

 ERROR: Citation Label wrong: \verb| www-azure |

 ERROR: Citation Label wrong: \verb| www-azure-opensource |

 ERROR: Citation Label wrong: \verb| www-azure-choices |

 ERROR: Citation Label wrong: \verb| www-futuregrid |

\end{IU}
\input{../hid-sp18-510/bio-kaul-naveen.tex}

\begin{IU}

hid-sp18-510

\url{https://github.com/cloudmesh-community/hid-sp18-510}

\url{https://github.com/cloudmesh-community/hid-sp18-510/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-510/blob/master//technology/abstract-predictionio.tex}{abstract-predictionio.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-510/blob/master//technology/abstract-theano.tex}{abstract-theano.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-510/blob/master//technology/abstract-kubernetes.tex}{abstract-kubernetes.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-510/blob/master//technology/abstract-mahout.tex}{abstract-mahout.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-510/blob/master//technology/abstract-cntk.tex}{abstract-cntk.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-510/blob/master//technology/hid-sp18-510.bib}{../hid-sp18-510/technology/hid-sp18-510.bib}

\end{IU}
\input{../hid-sp18-511/bio-Khandelwal-Sandeep.tex}

\begin{IU}

hid-sp18-511

\url{https://github.com/cloudmesh-community/hid-sp18-511}

\url{https://github.com/cloudmesh-community/hid-sp18-511/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-511/blob/master//technology/abstract-apttus.tex}{abstract-apttus.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-511/blob/master//technology/abstract-linode.tex}{abstract-linode.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-511/blob/master//technology/abstract-iics.tex}{abstract-iics.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-511/blob/master//technology/abstract-rapidminer.tex}{abstract-rapidminer.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-511/blob/master//technology/abstract-treasuredata.tex}{abstract-treasuredata.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-511/blob/master//technology/hid-sp18-511.bib}{../hid-sp18-511/technology/hid-sp18-511.bib}

\end{IU}
\input{../hid-sp18-512/bio-kikaya-felix.tex}

\begin{IU}

hid-sp18-512

\url{https://github.com/cloudmesh-community/hid-sp18-512}

\url{https://github.com/cloudmesh-community/hid-sp18-512/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-aurora.tex}{abstract-amazon-aurora.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazonml.tex}{abstract-amazonml.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-dynamodb.tex}{abstract-amazon-dynamodb.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-cloudfront.tex}{abstract-amazon-cloudfront.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-vpc.tex}{abstract-amazon-vpc.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/hid-sp18-512.bib}{../hid-sp18-512/technology/hid-sp18-512.bib}

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-512-amazon-dynamodb_faq |

\end{IU}
\input{../hid-sp18-512/bio-vonLaszewski-gregor.tex}

\begin{IU}

hid-sp18-512

\url{https://github.com/cloudmesh-community/hid-sp18-512}

\url{https://github.com/cloudmesh-community/hid-sp18-512/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-aurora.tex}{abstract-amazon-aurora.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazonml.tex}{abstract-amazonml.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-dynamodb.tex}{abstract-amazon-dynamodb.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-cloudfront.tex}{abstract-amazon-cloudfront.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/abstract-amazon-vpc.tex}{abstract-amazon-vpc.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-512/blob/master//technology/hid-sp18-512.bib}{../hid-sp18-512/technology/hid-sp18-512.bib}

 ERROR: No underscore allowed in citation lables: \verb| hid-sp18-512-amazon-dynamodb_faq |

\end{IU}
\input{../hid-sp18-513/bio-kugan-uma.tex}

\begin{IU}

hid-sp18-513

\url{https://github.com/cloudmesh-community/hid-sp18-513}

\url{https://github.com/cloudmesh-community/hid-sp18-513/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-513/blob/master//technology/abstract-docker.tex}{abstract-docker.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-513/blob/master//technology/abstract-datameer.tex}{abstract-datameer.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-513/blob/master//technology/abstract-tableau.tex}{abstract-tableau.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-513/blob/master//technology/abstract-neo4j.tex}{abstract-neo4j.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-513/blob/master//technology/abstract-oozie.tex}{abstract-oozie.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-513/blob/master//technology/hid-sp18-513.bib}{../hid-sp18-513/technology/hid-sp18-513.bib}

\end{IU}
\input{../hid-sp18-514/bio-lambadi-ravinder.tex}

\begin{IU}

hid-sp18-514

\url{https://github.com/cloudmesh-community/hid-sp18-514}

\url{https://github.com/cloudmesh-community/hid-sp18-514/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-514/blob/master//technology/abstract-oraclecoherence.tex}{abstract-oraclecoherence.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-514/blob/master//technology/abstract-apachegeode.tex}{abstract-apachegeode.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-514/blob/master//technology/abstract-apachekaraf.tex}{abstract-apachekaraf.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-514/blob/master//technology/abstract-apachecurator.tex}{abstract-apachecurator.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-514/blob/master//technology/abstract-tibcodatasynapsegridserver.tex}{abstract-tibcodatasynapsegridserver.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-514/blob/master//technology/hid-sp18-514.bib}{../hid-sp18-514/technology/hid-sp18-514.bib}

\end{IU}
\input{../hid-sp18-515/bio-Lin-Qingyun.tex}

\begin{IU}

hid-sp18-515

\url{https://github.com/cloudmesh-community/hid-sp18-515}

\url{https://github.com/cloudmesh-community/hid-sp18-515/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-515/blob/master//technology/abstract-HBase.tex}{abstract-HBase.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-515/blob/master//technology/abstract-CouchDB.tex}{abstract-CouchDB.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-515/blob/master//technology/abstract-Ranger.tex}{abstract-Ranger.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-515/blob/master//technology/abstract-Hadoop.tex}{abstract-Hadoop.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-515/blob/master//technology/abstract-OracleNosqlDB.tex}{abstract-OracleNosqlDB.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-515/blob/master//technology/hid-sp18-515.bib}{../hid-sp18-515/technology/hid-sp18-515.bib}

\end{IU}
\input{../hid-sp18-516/bio-pathan-shagufta.tex}

\begin{IU}

hid-sp18-516

\url{https://github.com/cloudmesh-community/hid-sp18-516}

\url{https://github.com/cloudmesh-community/hid-sp18-516/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-516/blob/master//technology/abstract-solr.tex}{abstract-solr.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-516/blob/master//technology/abstract-lingual.tex}{abstract-lingual.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-516/blob/master//technology/abstract-mariadb.tex}{abstract-mariadb.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-516/blob/master//technology/abstract-jaspersoft.tex}{abstract-jaspersoft.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-516/blob/master//technology/abstract-tokudb.tex}{abstract-tokudb.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-516/blob/master//technology/hid-sp18-516.bib}{../hid-sp18-516/technology/hid-sp18-516.bib}

\end{IU}
\input{../hid-sp18-517/bio-pitkar-harshad.tex}

\begin{IU}

hid-sp18-517

\url{https://github.com/cloudmesh-community/hid-sp18-517}

\url{https://github.com/cloudmesh-community/hid-sp18-517/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-sqoop.tex}{abstract-sqoop.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-hue.tex}{abstract-hue.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-mongodb.tex}{abstract-mongodb.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-kylin.tex}{abstract-kylin.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-kafka.tex}{abstract-kafka.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-kudu.tex}{abstract-kudu.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-pulsar.tex}{abstract-pulsar.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/abstract-metron.tex}{abstract-metron.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-517/blob/master//technology/hid-sp18-517.bib}{../hid-sp18-517/technology/hid-sp18-517.bib}

\end{IU}
\input{../hid-sp18-518/bio-robinson-michael.tex}

\begin{IU}

hid-sp18-518

\url{https://github.com/cloudmesh-community/hid-sp18-518}

\url{https://github.com/cloudmesh-community/hid-sp18-518/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-518/blob/master//technology/abstract-Cognito.tex}{abstract-Cognito.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-518/blob/master//technology/abstract-AWS-APIGateway.tex}{abstract-AWS-APIGateway.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-518/blob/master//technology/abstract-FoundationBenchmarks.tex}{abstract-FoundationBenchmarks.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-518/blob/master//technology/abstract-CloudWatch.tex}{abstract-CloudWatch.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-518/blob/master//technology/abstract-CloudTrail.tex}{abstract-CloudTrail.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-518/blob/master//technology/hid-sp18-518.bib}{../hid-sp18-518/technology/hid-sp18-518.bib}

\end{IU}
\input{../hid-sp18-519/bio-shukla-saurabh.tex}

\begin{IU}

hid-sp18-519

\url{https://github.com/cloudmesh-community/hid-sp18-519}

\url{https://github.com/cloudmesh-community/hid-sp18-519/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-519/blob/master//technology/abstract-denodo.tex}{abstract-denodo.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-519/blob/master//technology/abstract-spagobi.tex}{abstract-spagobi.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-519/blob/master//technology/abstract-alluxio.tex}{abstract-alluxio.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-519/blob/master//technology/abstract-systemml.tex}{abstract-systemml.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-519/blob/master//technology/abstract-connectthedots.tex}{abstract-connectthedots.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-519/blob/master//technology/hid-sp18-519.bib}{../hid-sp18-519/technology/hid-sp18-519.bib}

 ERROR: Citation Label wrong: \verb| denodo |

 ERROR: Citation Label wrong: \verb| spagobi |

 ERROR: Citation Label wrong: \verb| stratebi-bigdata |

 ERROR: Citation Label wrong: \verb| alluxio |

 ERROR: Citation Label wrong: \verb| systemml |

 ERROR: Citation Label wrong: \verb| systemml_blog |

 ERROR: No underscore allowed in citation lables: \verb| systemml_blog |

 ERROR: Citation Label wrong: \verb| connectthedots |

 ERROR: Citation Label wrong: \verb| Azure-IoT |

\end{IU}
\input{../hid-sp18-520/bio-Sinha-Arijit.tex}

\begin{IU}

hid-sp18-520

\url{https://github.com/cloudmesh-community/hid-sp18-520}

\url{https://github.com/cloudmesh-community/hid-sp18-520/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-TensorFlow.tex}{abstract-TensorFlow.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-PyTorch.tex}{abstract-PyTorch.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-OrientDB.tex}{abstract-OrientDB.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-RabbitMQ.tex}{abstract-RabbitMQ.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-OpenNN.tex}{abstract-OpenNN.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/abstract-Redis.tex}{abstract-Redis.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-520/blob/master//technology/hid-sp18-520.bib}{../hid-sp18-520/technology/hid-sp18-520.bib}

\end{IU}
\input{../hid-sp18-521/bio-steinbruegge-scott.tex}

\begin{IU}

hid-sp18-521

\url{https://github.com/cloudmesh-community/hid-sp18-521}

\url{https://github.com/cloudmesh-community/hid-sp18-521/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-521/blob/master//technology/abstract-athena.tex}{abstract-athena.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-521/blob/master//technology/abstract-greengrass.tex}{abstract-greengrass.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-521/blob/master//technology/abstract-sagemaker.tex}{abstract-sagemaker.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-521/blob/master//technology/abstract-fission.tex}{abstract-fission.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-521/blob/master//technology/abstract-emr.tex}{abstract-emr.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-521/blob/master//technology/hid-sp18-521.bib}{../hid-sp18-521/technology/hid-sp18-521.bib}

\end{IU}
\input{../hid-sp18-523/bio-tandon-ritesh.tex}

\begin{IU}

hid-sp18-523

\url{https://github.com/cloudmesh-community/hid-sp18-523}

\url{https://github.com/cloudmesh-community/hid-sp18-523/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-523/blob/master//technology/abstract-genomics.tex}{abstract-genomics.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-523/blob/master//technology/abstract-vision.tex}{abstract-vision.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-523/blob/master//technology/abstract-h2O.tex}{abstract-h2O.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-523/blob/master//technology/abstract-spanner.tex}{abstract-spanner.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-523/blob/master//technology/abstract-dremel.tex}{abstract-dremel.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-523/blob/master//technology/hid-sp18-523.bib}{../hid-sp18-523/technology/hid-sp18-523.bib}

\end{IU}
\input{../hid-sp18-524/bio-tian-hao.tex}

\begin{IU}

hid-sp18-524

\url{https://github.com/cloudmesh-community/hid-sp18-524}

\url{https://github.com/cloudmesh-community/hid-sp18-524/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-524/blob/master//technology/abstract-HPCC.tex}{abstract-HPCC.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-524/blob/master//technology/abstract-CenOS.tex}{abstract-CenOS.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-524/blob/master//technology/abstract-QDS.tex}{abstract-QDS.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-524/blob/master//technology/abstract-GCP-BD.tex}{abstract-GCP-BD.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-524/blob/master//technology/abstract-GCP-CD.tex}{abstract-GCP-CD.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-524/blob/master//technology/hid-sp18-524.bib}{../hid-sp18-524/technology/hid-sp18-524.bib}

 ERROR: Citation Label wrong: \verb| QDS |

 ERROR: Citation Label wrong: \verb| centOS |

 ERROR: Citation Label wrong: \verb| HPCC |

 ERROR: Citation Label wrong: \verb| GoogleCP_BDS |

 ERROR: No underscore allowed in citation lables: \verb| GoogleCP_BDS |

 ERROR: Citation Label wrong: \verb| GoogleCP_CD |

 ERROR: No underscore allowed in citation lables: \verb| GoogleCP_CD |

\end{IU}
\input{../hid-sp18-525/bio-walker-bruce.tex}

\begin{IU}

hid-sp18-525

\url{https://github.com/cloudmesh-community/hid-sp18-525}

\url{https://github.com/cloudmesh-community/hid-sp18-525/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-525/blob/master//technology/abstract-javascript.tex}{abstract-javascript.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-525/blob/master//technology/abstract-dsx.tex}{abstract-dsx.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-525/blob/master//technology/abstract-node.js.tex}{abstract-node.js.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-525/blob/master//technology/abstract-ibmcloud.tex}{abstract-ibmcloud.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-525/blob/master//technology/abstract-watson.tex}{abstract-watson.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-525/blob/master//technology/hid-sp18-525.bib}{../hid-sp18-525/technology/hid-sp18-525.bib}

\end{IU}
\input{../hid-sp18-526/bio-whitson-timothy.tex}

\begin{IU}

hid-sp18-526

\url{https://github.com/cloudmesh-community/hid-sp18-526}

\url{https://github.com/cloudmesh-community/hid-sp18-526/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-526/blob/master//technology/abstract-synthea.tex}{abstract-synthea.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-526/blob/master//technology/abstract-apache-accumulo.tex}{abstract-apache-accumulo.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-526/blob/master//technology/abstract-fluentd.tex}{abstract-fluentd.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-526/blob/master//technology/abstract-synthetic-data-vault.tex}{abstract-synthetic-data-vault.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-526/blob/master//technology/abstract-apache-phoenix.tex}{abstract-apache-phoenix.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-526/blob/master//technology/hid-sp18-526.bib}{../hid-sp18-526/technology/hid-sp18-526.bib}

\end{IU}
\input{../hid-sp18-601/bio-gianlupi-juliano.tex}

\begin{IU}

hid-sp18-601

\url{https://github.com/cloudmesh-community/hid-sp18-601}

\url{https://github.com/cloudmesh-community/hid-sp18-601/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/abstract-sharepoint.tex}{abstract-sharepoint.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/abstract-seti-at-home.tex}{abstract-seti-at-home.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/abstract-sharelatex.tex}{abstract-sharelatex.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/abstract-backblaze.tex}{abstract-backblaze.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/abstract-onedrive.tex}{abstract-onedrive.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/abstract-google-docs.tex}{abstract-google-docs.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-601/blob/master//technology/hid-sp18-601.bib}{../hid-sp18-601/technology/hid-sp18-601.bib}

\end{IU}
\input{../hid-sp18-602/bio-naredla-keerthi.tex}

\begin{IU}

hid-sp18-602

\url{https://github.com/cloudmesh-community/hid-sp18-602}

\url{https://github.com/cloudmesh-community/hid-sp18-602/issues}

\href{https://github.com/cloudmesh-community/hid-sp18-602/blob/master//technology/abstract-googlestackdriver.tex}{abstract-googlestackdriver.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-602/blob/master//technology/abstract-spinnaker.tex}{abstract-spinnaker.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-602/blob/master//technology/abstract-googleloadbalancing.tex}{abstract-googleloadbalancing.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-602/blob/master//technology/abstract-awscodestar.tex}{abstract-awscodestar.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-602/blob/master//technology/abstract-cloudautoml.tex}{abstract-cloudautoml.tex}

\href{https://github.com/cloudmesh-community/hid-sp18-602/blob/master//technology/hid-sp18-602.bib}{../hid-sp18-602/technology/hid-sp18-602.bib}

\end{IU}
